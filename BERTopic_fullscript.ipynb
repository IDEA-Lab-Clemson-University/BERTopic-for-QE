{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import itertools\n",
    "import json\n",
    "import openai\n",
    "import tiktoken\n",
    "import spacy\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import OpenAI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotly Functions for IDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from umap import UMAP\n",
    "# from typing import List, Union\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# def visualize_topics(topic_model,\n",
    "#                      topics: List[int] = None,\n",
    "#                      top_n_topics: int = None,\n",
    "#                      custom_labels: Union[bool, str] = False,\n",
    "#                      title: str = \"<b>Intertopic Distance Map</b>\",\n",
    "#                      width: int = 650,\n",
    "#                      height: int = 650) -> go.Figure:\n",
    "#     \"\"\" Visualize topics, their sizes, and their corresponding words\n",
    "\n",
    "#     This visualization is highly inspired by LDAvis, a great visualization\n",
    "#     technique typically reserved for LDA.\n",
    "\n",
    "#     Arguments:\n",
    "#         topic_model: A fitted BERTopic instance.\n",
    "#         topics: A selection of topics to visualize\n",
    "#         top_n_topics: Only select the top n most frequent topics\n",
    "#         custom_labels: If bool, whether to use custom topic labels that were defined using \n",
    "#                        `topic_model.set_topic_labels`.\n",
    "#                        If `str`, it uses labels from other aspects, e.g., \"Aspect1\".\n",
    "#         title: Title of the plot.\n",
    "#         width: The width of the figure.\n",
    "#         height: The height of the figure.\n",
    "\n",
    "#     Examples:\n",
    "\n",
    "#     To visualize the topics simply run:\n",
    "\n",
    "#     ```python\n",
    "#     topic_model.visualize_topics()\n",
    "#     ```\n",
    "\n",
    "#     Or if you want to save the resulting figure:\n",
    "\n",
    "#     ```python\n",
    "#     fig = topic_model.visualize_topics()\n",
    "#     fig.write_html(\"path/to/file.html\")\n",
    "#     ```\n",
    "#     <iframe src=\"../../getting_started/visualization/viz.html\"\n",
    "#     style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe>\n",
    "#     \"\"\"\n",
    "#     # Select topics based on top_n and topics args\n",
    "#     freq_df = topic_model.get_topic_freq()\n",
    "#     freq_df = freq_df.loc[freq_df.Topic != -1, :]\n",
    "#     if topics is not None:\n",
    "#         topics = list(topics)\n",
    "#     elif top_n_topics is not None:\n",
    "#         topics = sorted(freq_df.Topic.to_list()[:top_n_topics])\n",
    "#     else:\n",
    "#         topics = sorted(freq_df.Topic.to_list())\n",
    "\n",
    "#     # Extract topic words and their frequencies\n",
    "#     topic_list = sorted(topics)\n",
    "#     frequencies = [topic_model.topic_sizes_[topic] for topic in topic_list]\n",
    "#     if isinstance(custom_labels, str):\n",
    "#         words = [[[str(topic), None]] + topic_model.topic_aspects_[custom_labels][topic] for topic in topic_list]\n",
    "#         words = [\"_\".join([label[0] for label in labels[:4]]) for labels in words]\n",
    "#         words = [label if len(label) < 30 else label[:27] + \"...\" for label in words]\n",
    "#     elif custom_labels and topic_model.custom_labels_ is not None:\n",
    "#         words = [topic_model.custom_labels_[topic + topic_model._outliers] for topic in topic_list]\n",
    "#     else:\n",
    "#         words = [\" | \".join([word[0] for word in topic_model.get_topic(topic)[:5]]) for topic in topic_list]\n",
    "\n",
    "#     # Embed c-TF-IDF into 2D\n",
    "#     all_topics = sorted(list(topic_model.get_topics().keys()))\n",
    "#     indices = np.array([all_topics.index(topic) for topic in topics])\n",
    "\n",
    "#     if topic_model.topic_embeddings_ is not None:\n",
    "#         embeddings = topic_model.topic_embeddings_[indices]\n",
    "#         embeddings = UMAP(n_neighbors=2, n_components=2, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "#     else:\n",
    "#         embeddings = topic_model.c_tf_idf_.toarray()[indices]\n",
    "#         embeddings = MinMaxScaler().fit_transform(embeddings)\n",
    "#         embeddings = UMAP(n_neighbors=2, n_components=2, metric='hellinger', random_state=42).fit_transform(embeddings)\n",
    "\n",
    "#     # Visualize with plotly\n",
    "#     df = pd.DataFrame({\"x\": embeddings[:, 0], \"y\": embeddings[:, 1],\n",
    "#                        \"Topic\": topic_list, \"Words\": words, \"Size\": frequencies})\n",
    "#     return _plotly_topic_visualization(df, topic_list, title, width, height)\n",
    "\n",
    "\n",
    "# def _plotly_topic_visualization(df: pd.DataFrame,\n",
    "#                                 topic_list: List[str],\n",
    "#                                 title: str,\n",
    "#                                 width: int,\n",
    "#                                 height: int):\n",
    "#     \"\"\" Create plotly-based visualization of topics with a slider for topic selection \"\"\"\n",
    "\n",
    "#     def get_color(topic_selected):\n",
    "#         if topic_selected == -1:\n",
    "#             marker_color = [\"#B0BEC5\" for _ in topic_list]\n",
    "#         else:\n",
    "#             marker_color = [\"red\" if topic == topic_selected else \"#B0BEC5\" for topic in topic_list]\n",
    "#         return [{'marker.color': [marker_color]}]\n",
    "\n",
    "#     # Prepare figure range\n",
    "#     x_range = (df.x.min() - abs((df.x.min()) * .15), df.x.max() + abs((df.x.max()) * .15))\n",
    "#     y_range = (df.y.min() - abs((df.y.min()) * .15), df.y.max() + abs((df.y.max()) * .15))\n",
    "\n",
    "#     # Plot topics\n",
    "#     fig = px.scatter(df, x=\"x\", y=\"y\", size=\"Size\", text = \"Topic\", size_max=40, template=\"simple_white\", labels={\"x\": \"\", \"y\": \"\"} #,\n",
    "#                      #hover_data={\"Topic\": True, \"Words\": True, \"Size\": True, \"x\": False, \"y\": False}\n",
    "#                      )\n",
    "#     #fig.update_traces(marker=dict(color=\"#B0BEC5\", line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "#     # Update hover order\n",
    "#     # fig.update_traces(hovertemplate=\"<br>\".join([\"<b>Topic %{customdata[0]}</b>\",\n",
    "#     #                                              \"%{customdata[1]}\",\n",
    "#     #                                              \"Size: %{customdata[2]}\"]))\n",
    "\n",
    "#     # Create a slider for topic selection\n",
    "#     #steps = [dict(label=f\"Topic {topic}\", method=\"update\", args=get_color(topic)) for topic in topic_list]\n",
    "#     #sliders = [dict(active=0, pad={\"t\": 50}, steps=steps)]\n",
    "\n",
    "#     # Stylize layout\n",
    "#     # fig.update_layout(\n",
    "#     #     title={\n",
    "#     #         'text': f\"{title}\",\n",
    "#     #         'y': .95,\n",
    "#     #         'x': 0.5,\n",
    "#     #         'xanchor': 'center',\n",
    "#     #         'yanchor': 'top',\n",
    "#     #         'font': dict(\n",
    "#     #             size=22,\n",
    "#     #             color=\"Black\")\n",
    "#     #     },\n",
    "#     #     width=width,\n",
    "#     #     height=height,\n",
    "#     #     #hoverlabel=dict(\n",
    "#     #         #bgcolor=\"white\",\n",
    "#     #         #font_size=16,\n",
    "#     #         #font_family=\"Rockwell\"\n",
    "#     #    # ),\n",
    "#     #     xaxis={\"visible\": False},\n",
    "#     #     yaxis={\"visible\": False},\n",
    "#     #    #sliders=sliders\n",
    "#     # )\n",
    "\n",
    "#     # # Update axes ranges\n",
    "#     # fig.update_xaxes(range=x_range)\n",
    "#     # fig.update_yaxes(range=y_range)\n",
    "\n",
    "#     # Add grid in a 'plus' shape\n",
    "#     # fig.add_shape(type=\"line\",\n",
    "#     #               x0=sum(x_range) / 2, y0=y_range[0], x1=sum(x_range) / 2, y1=y_range[1],\n",
    "#     #               line=dict(color=\"#CFD8DC\", width=2))\n",
    "#     # fig.add_shape(type=\"line\",\n",
    "#     #               x0=x_range[0], y0=sum(y_range) / 2, x1=x_range[1], y1=sum(y_range) / 2,\n",
    "#     #               line=dict(color=\"#9E9E9E\", width=2))\n",
    "#     # fig.add_annotation(x=x_range[0], y=sum(y_range) / 2, text=\"D1\", showarrow=False, yshift=10)\n",
    "#     # fig.add_annotation(y=y_range[1], x=sum(x_range) / 2, text=\"D2\", showarrow=False, xshift=10)\n",
    "#     # fig.data = fig.data[::-1]\n",
    "\n",
    "#     return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Code Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   ID Interview.ID          Role           Speaker  \\\n",
       " 0   1           AH    Researcher      Researcher 1   \n",
       " 1   2           AH  Grad Student  Allyson Hauptman   \n",
       " 2   3           AH    Researcher      Researcher 1   \n",
       " 3   4           AH  Grad Student  Allyson Hauptman   \n",
       " 4   5           AH    Researcher      Researcher 1   \n",
       " \n",
       "                                            Utterance  Line  Stanza  \\\n",
       " 0  Okay, here we go. All right. So I'm recording ...     1       1   \n",
       " 1                                               Yes.     2       1   \n",
       " 2  Okay, great. And what is the date today? Today...     3       1   \n",
       " 3  Sure. So I am an active duty army officer. I w...     4       1   \n",
       " 4  Okay. Can you tell us more about, you said the...     5       1   \n",
       " \n",
       "    ML.AI.Applications  Limitations.of.ML.AI  Privacy  \\\n",
       " 0                   0                     0        0   \n",
       " 1                   0                     0        0   \n",
       " 2                   0                     0        0   \n",
       " 3                   0                     0        0   \n",
       " 4                   0                     0        0   \n",
       " \n",
       "    Human.involvemnt.interaction  Bias..Prejudice..and.Harm  \\\n",
       " 0                             0                          0   \n",
       " 1                             0                          0   \n",
       " 2                             0                          0   \n",
       " 3                             1                          0   \n",
       " 4                             0                          0   \n",
       " \n",
       "    Minimizing.Bias.and.Harm  Assessing.Underlying.Motivations  Trust.Building  \\\n",
       " 0                         0                                 0               0   \n",
       " 1                         0                                 0               0   \n",
       " 2                         0                                 0               0   \n",
       " 3                         0                                 0               0   \n",
       " 4                         0                                 0               0   \n",
       " \n",
       "    Algorithms.and.computational.thinking  \\\n",
       " 0                                      0   \n",
       " 1                                      0   \n",
       " 2                                      0   \n",
       " 3                                      0   \n",
       " 4                                      0   \n",
       " \n",
       "    Training.and.Testing.Machines.Using.Data  \n",
       " 0                                         0  \n",
       " 1                                         0  \n",
       " 2                                         0  \n",
       " 3                                         0  \n",
       " 4                                         0  ,\n",
       " Index(['ID', 'Interview.ID', 'Role', 'Speaker', 'Utterance', 'Line', 'Stanza',\n",
       "        'ML.AI.Applications', 'Limitations.of.ML.AI', 'Privacy',\n",
       "        'Human.involvemnt.interaction', 'Bias..Prejudice..and.Harm',\n",
       "        'Minimizing.Bias.and.Harm', 'Assessing.Underlying.Motivations',\n",
       "        'Trust.Building', 'Algorithms.and.computational.thinking',\n",
       "        'Training.and.Testing.Machines.Using.Data'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset to check its structure\n",
    "file_path = 'AI_interview_data.csv'\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset and the column names\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45 possible pairs of codes or links/connections to be examined in the coded data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45,\n",
       " Index(['ML.AI.Applications', 'Limitations.of.ML.AI', 'Privacy',\n",
       "        'Human.involvemnt.interaction', 'Bias..Prejudice..and.Harm',\n",
       "        'Minimizing.Bias.and.Harm', 'Assessing.Underlying.Motivations',\n",
       "        'Trust.Building', 'Algorithms.and.computational.thinking',\n",
       "        'Training.and.Testing.Machines.Using.Data'],\n",
       "       dtype='object'),\n",
       " [('ML.AI.Applications', 'Limitations.of.ML.AI'),\n",
       "  ('ML.AI.Applications', 'Privacy'),\n",
       "  ('ML.AI.Applications', 'Human.involvemnt.interaction'),\n",
       "  ('ML.AI.Applications', 'Bias..Prejudice..and.Harm'),\n",
       "  ('ML.AI.Applications', 'Minimizing.Bias.and.Harm')],\n",
       " None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pairing Codes for Connection\n",
    "# I assume the codes columns will ALWAYS begin immediately after the Stanza col\n",
    "code_columns = df.columns[df.columns.get_loc('Stanza') + 1:]\n",
    "\n",
    "# Generating all combinations of code pairs\n",
    "code_pairs = list(itertools.combinations(code_columns, 2))\n",
    "\n",
    "# Display the code columns and the first few pairs to verify\n",
    "len(code_pairs), code_columns, code_pairs[:5], print(f\"There are {len(code_pairs)} possible pairs of codes or links/connections to be examined in the coded data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEGMENTATION 1A: WHOLE WINDOW (without referent line logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm for whole data\n",
    "def find_cooccurrences_in_windows(df, code_pairs, window_size):\n",
    "    pair_dfs = {}\n",
    "    \n",
    "    # Iterate through each code pair\n",
    "    for pair in code_pairs:\n",
    "        code1, code2 = pair\n",
    "        pair_key = f\"{code1}_{code2}\"\n",
    "        pair_dfs[pair_key] = []\n",
    "\n",
    "        # Slide through the dataframe using the specified window size\n",
    "        for start in range(len(df) - window_size + 1):\n",
    "            window = df.iloc[start:start + window_size]\n",
    "            \n",
    "            # Check if there are any lines where both codes are coded as 1\n",
    "            co_occurrences = window[(window[code1] == 1) & (window[code2] == 1)]\n",
    "            \n",
    "            if not co_occurrences.empty:\n",
    "                # Concatenate all the Utterances in the window\n",
    "                concatenated_utterances = window['Utterance'].tolist()\n",
    "                pair_dfs[pair_key].append(concatenated_utterances)\n",
    "\n",
    "    return pair_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying\n",
    "window_size = 4  # Adjust the window size as needed\n",
    "pair_occurrences_in_windows = find_cooccurrences_in_windows(df, code_pairs, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if it works\n",
    "#for key, value in pair_occurrences_in_windows.items():\n",
    "    #print(f\"{key}: {value[:7]}\")  # Printing only first 7 windows for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding & Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for saving paired files for the list of list ones\n",
    "def save_pairs_to_csv(pair_dfs, output_file):\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    max_len = max(len(v) for v in pair_dfs.values())  # Find the maximum length of lists in the dictionary\n",
    "    data = {k: v + [None] * (max_len - len(v)) for k, v in pair_dfs.items()}  # Pad lists with None\n",
    "    df_output = pd.DataFrame(data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df_output.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to whole_paired_code_occurrences.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving\n",
    "whole_output_file = 'whole_paired_code_occurrences.csv'\n",
    "save_pairs_to_csv(pair_occurrences_in_windows, whole_output_file)\n",
    "\n",
    "print(f\"Data saved to {whole_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEGMENTATION 1B: WHOLE WINDOW (WITH REFERENT LINE LOGIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function traverses the data using a sliding window. It uses the last line of each window as the referent line. If there is a co-occurrence in the referent line, it concatenates all the `Utterances` in the window (including the referent line) and save them to a list specific to that code pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_win_cooccurrences_with_referent(df, code_pairs, window_size):\n",
    "    pair_dfs = {}\n",
    "    \n",
    "    # Iterate through each code pair\n",
    "    for pair in code_pairs:\n",
    "        code1, code2 = pair\n",
    "        pair_key = f\"{code1}_{code2}\"\n",
    "        pair_dfs[pair_key] = []\n",
    "\n",
    "        # Slide through the dataframe using the specified window size\n",
    "        for start in range(len(df) - window_size + 1):\n",
    "            window = df.iloc[start:start + window_size]\n",
    "            referent_line = window.iloc[-1]\n",
    "            \n",
    "            # Check if there is a co-occurrence in the referent line\n",
    "            if referent_line[code1] == 1 and referent_line[code2] == 1:\n",
    "                # Concatenate all the Utterances in the window\n",
    "                concatenated_utterances = window['Utterance'].tolist()\n",
    "                pair_dfs[pair_key].append(concatenated_utterances)\n",
    "\n",
    "    return pair_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying\n",
    "window_size = 4  # Adjust the window size as needed\n",
    "pair_win_occurrences_with_referent = find_win_cooccurrences_with_referent(df, code_pairs, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if it works\n",
    "#for key, value in pair_win_occurrences_with_referent.items():\n",
    "    #print(f\"{key}: {value[:7]}\")  # Printing only first 7 windows for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "whole_ref_output_file = 'paired_whole_occurrences_window_referent.csv'\n",
    "save_pairs_to_csv(pair_win_occurrences_with_referent, whole_ref_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEGMENTATION 2: ONLY CODED LINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below performs the following actions:\n",
    "1. For each window, set the referent line as the last line in the window.\n",
    "2. Check if any of the coded pairs is coded as 1 in the referent line.\n",
    "3. If a code is coded as 1 in the referent line, then\n",
    "\n",
    "\n",
    "a. It check if there's a co-occurrence with the other code in the same line. If true, save the referent line's utterance as a list.\n",
    "\n",
    "\n",
    "b. It check if any other line in the window has the second code coded as 1. If true, concatenate the referent line with this line and save them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_code_occurrences_with_referent(df, code_pairs, window_size):\n",
    "    pair_dfs = {}\n",
    "    \n",
    "    # Iterate through each code pair\n",
    "    for pair in code_pairs:\n",
    "        code1, code2 = pair\n",
    "        pair_key = f\"{code1}_{code2}\"\n",
    "        pair_dfs[pair_key] = []\n",
    "\n",
    "        # Slide through the dataframe using the specified window size\n",
    "        for start in range(len(df) - window_size + 1):\n",
    "            window = df.iloc[start:start + window_size]\n",
    "            referent_line = window.iloc[-1]\n",
    "            \n",
    "            # Check if any of the code pairs is coded as 1 in the referent line\n",
    "            if referent_line[code1] == 1 or referent_line[code2] == 1:\n",
    "                # Check for co-occurrence of both codes in the referent line\n",
    "                if referent_line[code1] == 1 and referent_line[code2] == 1:\n",
    "                    pair_dfs[pair_key].append([referent_line['Utterance']])\n",
    "                \n",
    "                # Check for co-occurrence with any other line in the window, excluding the referent line itself\n",
    "                for i in range(len(window) - 1):\n",
    "                    row = window.iloc[i]\n",
    "                    if row[code1] == 1 and referent_line[code2] == 1:\n",
    "                        concatenated_lines = f\"{row['Utterance']} | {referent_line['Utterance']}\"\n",
    "                        # if you prefer that each utterance be within its own quote in the list, uncomment code below\n",
    "                        # concatenated_lines = [f'\"{row[\"Utterance\"]}\"', f'\"{referent_line[\"Utterance\"]}\"']\n",
    "                        pair_dfs[pair_key].append([concatenated_lines])\n",
    "                    elif row[code2] == 1 and referent_line[code1] == 1:\n",
    "                        concatenated_lines = f\"{row['Utterance']} | {referent_line['Utterance']}\"\n",
    "                        # if you prefer that each utterance be within its own quote in the list, uncomment code below\n",
    "                        # concatenated_lines = [f'\"{row[\"Utterance\"]}\"', f'\"{referent_line[\"Utterance\"]}\"']\n",
    "                        pair_dfs[pair_key].append([concatenated_lines])\n",
    "\n",
    "    return pair_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying function to data\n",
    "window_size = 4  # You can adjust the window size as needed\n",
    "pair_occurrences_with_referent = find_code_occurrences_with_referent(df, code_pairs, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if it works\n",
    "#for key, value in pair_occurrences_with_referent.items():\n",
    "    #print(f\"{key}: {value[:7]}\")  # Printing only first 7 windows for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "output_file = 'ref_pipe_paired_code_occurrences.csv'\n",
    "save_pairs_to_csv(pair_occurrences_with_referent, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Speaker Column For Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------------------This function includes the Speaker In The Segmentation ---------------------------##\n",
    "\n",
    "def find_occurrences_with_referent(df, code_pairs, window_size):\n",
    "    pair_dfs = {}\n",
    "    \n",
    "    # Iterate through each code pair\n",
    "    for pair in code_pairs:\n",
    "        code1, code2 = pair\n",
    "        pair_key = f\"{code1}_{code2}\"\n",
    "        pair_dfs[pair_key] = []\n",
    "\n",
    "        # Slide through the dataframe using the specified window size\n",
    "        for start in range(len(df) - window_size + 1):\n",
    "            window = df.iloc[start:start + window_size]\n",
    "            referent_line = window.iloc[-1]\n",
    "            \n",
    "            # Check if any of the code pairs is coded as 1 in the referent line\n",
    "            if referent_line[code1] == 1 or referent_line[code2] == 1:\n",
    "                # Check for co-occurrence of both codes in the referent line\n",
    "                if referent_line[code1] == 1 and referent_line[code2] == 1:\n",
    "                    pair_dfs[pair_key].append([f\"{referent_line['Speaker']}: {referent_line['Utterance']}\"])\n",
    "                \n",
    "                # Check for co-occurrence with any other line in the window, excluding the referent line itself\n",
    "                for i in range(len(window) - 1):\n",
    "                    row = window.iloc[i]\n",
    "                    if row[code1] == 1 and referent_line[code2] == 1:\n",
    "                        concatenated_lines = f\"{row['Speaker']}: {row['Utterance']} | {referent_line['Speaker']}: {referent_line['Utterance']}\"\n",
    "                        pair_dfs[pair_key].append([concatenated_lines])\n",
    "                    elif row[code2] == 1 and referent_line[code1] == 1:\n",
    "                        concatenated_lines = f\"{row['Speaker']}: {row['Utterance']} | {referent_line['Speaker']}: {referent_line['Utterance']}\"\n",
    "                        pair_dfs[pair_key].append([concatenated_lines])\n",
    "\n",
    "    return pair_dfs\n",
    "\n",
    "\n",
    "##-----------------------------------------------------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the algorithm\n",
    "window_size = 4  # You can adjust the window size as needed\n",
    "code_occurrences_with_referent = find_occurrences_with_referent(df, code_pairs, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if it works\n",
    "#for key, value in code_occurrences_with_referent.items():\n",
    "    #print(f\"{key}: {value[:7]}\")  # Printing only first 7 windows for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert csv to list function because I can't figure out to index the dictionary!!!! :o\n",
    "import csv\n",
    "def column_to_list(csv_file, column_index):\n",
    "    data = []\n",
    "    with open(csv_file, 'r', newline='', encoding=\"cp1252\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if len(row) > column_index:  # Check if the row has the desired column\n",
    "                data.append(row[column_index])\n",
    "   # del data[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list of documents for topic modeling from csv file\n",
    "\n",
    "# #coded lines only \n",
    "# csv_file = \"ref_pipe_paired_code_occurrences.csv\" \n",
    "# column_index = 33 #select column/code pair \n",
    "# docs_clo = column_to_list(csv_file, column_index)\n",
    "# print(\"code pair for clo is \", docs_clo[0])\n",
    "# del docs_clo[0]\n",
    "# docs_clo = list(filter(None, docs_clo))\n",
    "\n",
    "\n",
    "# #whole window\n",
    "# csv_file = \"paired_whole_occurrences_window_referent.csv\" \n",
    "# column_index = 2 #select column/code pair \n",
    "# docs_ww = column_to_list(csv_file, column_index)\n",
    "# print(\"code pair for ww is \", docs_ww[0])\n",
    "# del docs_ww[0]\n",
    "# docs_ww = list(filter(None, docs_ww))\n",
    "\n",
    "# # check output\n",
    "# #docs_ww\n",
    "# #docs_clo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA instead of UMAP (to remove stochastic behavior/randomness in dim red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code pair for clo is  Human.involvemnt.interaction_Algorithms.and.computational.thinking\n"
     ]
    }
   ],
   "source": [
    "#select column/code pair for coded lines only list\n",
    "column_index =  28\n",
    "\n",
    "#create list for BERTopic\n",
    "csv_file = \"ref_pipe_paired_code_occurrences.csv\" \n",
    "docs_clo = column_to_list(csv_file, column_index)\n",
    "print(\"code pair for clo is \", docs_clo[0])\n",
    "del docs_clo[0]\n",
    "docs_clo = list(filter(None, docs_clo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract Embeddings\n",
    "#embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\") \n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n",
    "\n",
    "# 2. Reduce Dimensionality\n",
    "pca_model = PCA(n_components=30)\n",
    "\n",
    "# 3. Cluster Reduced Embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=4, metric='euclidean', prediction_data=True)\n",
    "\n",
    "# 4. Tokenize Topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# 5. Create Topic Reprensetations\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# 6. Fine-tune representations\n",
    "#key_representation_model = KeyBERTInspired()'\n",
    "key_representation_model = MaximalMarginalRelevance()\n",
    "\n",
    "# Create model\n",
    "alt_topic_model = BERTopic(\n",
    "    embedding_model = embedding_model,\n",
    "    umap_model=pca_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    representation_model=key_representation_model, \n",
    "    calculate_probabilities=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_topics, alt_probs = alt_topic_model.fit_transform(docs_clo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "these_docs = alt_topic_model.get_document_info(docs_clo)\n",
    "these_docs.to_csv(\"results/docs_pair3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>152</td>\n",
       "      <td>-1_like_ai_data_learning</td>\n",
       "      <td>[like, ai, data, learning, things, machine, pe...</td>\n",
       "      <td>[['Yeah. I love that, Joe. And I can see that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0_computer_ai_school_tech</td>\n",
       "      <td>[computer, ai, school, tech, degree, career, g...</td>\n",
       "      <td>[[\"Yeah. So pretty much been an academic in tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1_ai_teammates_collaboration_humans</td>\n",
       "      <td>[ai, teammates, collaboration, humans, team, t...</td>\n",
       "      <td>[[\"So a lot of times human... we would call it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2_robot_text_speech_work</td>\n",
       "      <td>[robot, text, speech, work, techniques, needs,...</td>\n",
       "      <td>[['A bit. Yeah. So the primary ones that I\\'d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3_ai_ethics_students_case</td>\n",
       "      <td>[ai, ethics, students, case, focus, promises, ...</td>\n",
       "      <td>[['I know that there\\'s a K12 curriculum that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4_logical_teaching_language_kids</td>\n",
       "      <td>[logical, teaching, language, kids, logic, cod...</td>\n",
       "      <td>[[\"I think it's super important because it tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5_game_kids_information_hacked</td>\n",
       "      <td>[game, kids, information, hacked, risk, unethi...</td>\n",
       "      <td>[[\"I would say one way is to maybe if they hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6_implemented_mimic_capabilities_ai</td>\n",
       "      <td>[implemented, mimic, capabilities, ai, ability...</td>\n",
       "      <td>[[\"That's interesting how that seems very appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7_models_reinforcement_training_kids</td>\n",
       "      <td>[models, reinforcement, training, kids, learni...</td>\n",
       "      <td>[['Hmm. I think kids would probably always lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>8_security_relationship_tools_capabilities</td>\n",
       "      <td>[security, relationship, tools, capabilities, ...</td>\n",
       "      <td>[[\"Sure. So the big differentiator for what wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9_robot_space_robots_lab</td>\n",
       "      <td>[robot, space, robots, lab, roam, navigates, l...</td>\n",
       "      <td>[['Oh yeah. Sorry. Simultaneous Location and M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>10_understand_thing_crosstalk_ears</td>\n",
       "      <td>[understand, thing, crosstalk, ears, robots, b...</td>\n",
       "      <td>[['Yeah. Absolutely. You answered everything. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>11_tiktok_youth_learning_global</td>\n",
       "      <td>[tiktok, youth, learning, global, things, youn...</td>\n",
       "      <td>[['I tell people all the time, \"Robots are not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>12_gaze_task_robot_methodology</td>\n",
       "      <td>[gaze, task, robot, methodology, behavior, stu...</td>\n",
       "      <td>[[\"Yeah. Yeah. It's cool. And what's interesti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>13_environments_gaming_virtual_ai</td>\n",
       "      <td>[environments, gaming, virtual, ai, learning, ...</td>\n",
       "      <td>[[\"And the best thing about being able to util...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>14_alexa_experiences_words_kids</td>\n",
       "      <td>[alexa, experiences, words, kids, pilots, swip...</td>\n",
       "      <td>[[\"Yeah, I learned that a little bit the hard ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                        Name  \\\n",
       "0      -1    152                    -1_like_ai_data_learning   \n",
       "1       0     13                   0_computer_ai_school_tech   \n",
       "2       1      8         1_ai_teammates_collaboration_humans   \n",
       "3       2      8                    2_robot_text_speech_work   \n",
       "4       3      8                   3_ai_ethics_students_case   \n",
       "5       4      7            4_logical_teaching_language_kids   \n",
       "6       5      7              5_game_kids_information_hacked   \n",
       "7       6      6         6_implemented_mimic_capabilities_ai   \n",
       "8       7      6        7_models_reinforcement_training_kids   \n",
       "9       8      5  8_security_relationship_tools_capabilities   \n",
       "10      9      5                    9_robot_space_robots_lab   \n",
       "11     10      5          10_understand_thing_crosstalk_ears   \n",
       "12     11      5             11_tiktok_youth_learning_global   \n",
       "13     12      5              12_gaze_task_robot_methodology   \n",
       "14     13      4           13_environments_gaming_virtual_ai   \n",
       "15     14      4             14_alexa_experiences_words_kids   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [like, ai, data, learning, things, machine, pe...   \n",
       "1   [computer, ai, school, tech, degree, career, g...   \n",
       "2   [ai, teammates, collaboration, humans, team, t...   \n",
       "3   [robot, text, speech, work, techniques, needs,...   \n",
       "4   [ai, ethics, students, case, focus, promises, ...   \n",
       "5   [logical, teaching, language, kids, logic, cod...   \n",
       "6   [game, kids, information, hacked, risk, unethi...   \n",
       "7   [implemented, mimic, capabilities, ai, ability...   \n",
       "8   [models, reinforcement, training, kids, learni...   \n",
       "9   [security, relationship, tools, capabilities, ...   \n",
       "10  [robot, space, robots, lab, roam, navigates, l...   \n",
       "11  [understand, thing, crosstalk, ears, robots, b...   \n",
       "12  [tiktok, youth, learning, global, things, youn...   \n",
       "13  [gaze, task, robot, methodology, behavior, stu...   \n",
       "14  [environments, gaming, virtual, ai, learning, ...   \n",
       "15  [alexa, experiences, words, kids, pilots, swip...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [['Yeah. I love that, Joe. And I can see that ...  \n",
       "1   [[\"Yeah. So pretty much been an academic in tr...  \n",
       "2   [[\"So a lot of times human... we would call it...  \n",
       "3   [['A bit. Yeah. So the primary ones that I\\'d ...  \n",
       "4   [['I know that there\\'s a K12 curriculum that ...  \n",
       "5   [[\"I think it's super important because it tea...  \n",
       "6   [[\"I would say one way is to maybe if they hav...  \n",
       "7   [[\"That's interesting how that seems very appl...  \n",
       "8   [['Hmm. I think kids would probably always lik...  \n",
       "9   [[\"Sure. So the big differentiator for what wo...  \n",
       "10  [['Oh yeah. Sorry. Simultaneous Location and M...  \n",
       "11  [['Yeah. Absolutely. You answered everything. ...  \n",
       "12  [['I tell people all the time, \"Robots are not...  \n",
       "13  [[\"Yeah. Yeah. It's cool. And what's interesti...  \n",
       "14  [[\"And the best thing about being able to util...  \n",
       "15  [[\"Yeah, I learned that a little bit the hard ...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "these_topics = alt_topic_model.get_topic_info()\n",
    "these_topics.to_csv(\"results/topics_pair1.csv\", index=False)\n",
    "\n",
    "these_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#C8D2D7",
          "line": {
           "color": "#6E8484",
           "width": 1
          }
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          1
         ],
         "y": [
          "<b>Topic 0</b>: computer_ai_school_tech_..."
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Topic Probability Distribution</b>",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 800,
        "xaxis": {
         "title": {
          "text": "Probability"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#topic_distr, _ = alt_topic_model.approximate_distribution(docs_clo, min_similarity=0)\n",
    "#alt_topic_model.visualize_distribution(topic_distr[131])\n",
    "alt_topic_model.visualize_distribution(alt_probs[131])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.038113593395703096,
          0.03868723932336755,
          0.04278030333546133,
          0.042808378586742885,
          0.07108991626316805
         ],
         "xaxis": "x",
         "y": [
          "degree  ",
          "tech  ",
          "school  ",
          "ai  ",
          "computer  "
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#0072B2"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.06805666806809268,
          0.07589174943821679,
          0.08193811423376883,
          0.08351239071604077,
          0.12388147564793599
         ],
         "xaxis": "x2",
         "y": [
          "team  ",
          "humans  ",
          "collaboration  ",
          "teammates  ",
          "ai  "
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "color": "#CC79A7"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.025347612295110834,
          0.041237249609199694,
          0.0501984064624772,
          0.05022945140046452,
          0.06582273596309374
         ],
         "xaxis": "x3",
         "y": [
          "techniques  ",
          "work  ",
          "speech  ",
          "text  ",
          "robot  "
         ],
         "yaxis": "y3"
        },
        {
         "marker": {
          "color": "#E69F00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.02303561653485656,
          0.02793790927610407,
          0.029657016455490962,
          0.03336414351242733,
          0.039753648416076606
         ],
         "xaxis": "x4",
         "y": [
          "focus  ",
          "case  ",
          "students  ",
          "ethics  ",
          "ai  "
         ],
         "yaxis": "y4"
        },
        {
         "marker": {
          "color": "#56B4E9"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.061754245722558804,
          0.06684250978035584,
          0.07436890870650048,
          0.08890647381136631,
          0.08898805494146303
         ],
         "xaxis": "x5",
         "y": [
          "logic  ",
          "kids  ",
          "language  ",
          "teaching  ",
          "logical  "
         ],
         "yaxis": "y5"
        },
        {
         "marker": {
          "color": "#009E73"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.07630132775349102,
          0.07795709354687855,
          0.09272466317006339,
          0.09864865606317132,
          0.10068572252302158
         ],
         "xaxis": "x6",
         "y": [
          "risk  ",
          "hacked  ",
          "information  ",
          "kids  ",
          "game  "
         ],
         "yaxis": "y6"
        },
        {
         "marker": {
          "color": "#F0E442"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.07333403956167393,
          0.08752651015531843,
          0.08860126856621224,
          0.10185421266058893,
          0.1052248329646332
         ],
         "xaxis": "x7",
         "y": [
          "ability  ",
          "ai  ",
          "capabilities  ",
          "mimic  ",
          "implemented  "
         ],
         "yaxis": "y7"
        },
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.04081119540305937,
          0.04517006806340171,
          0.05159851836219541,
          0.062214491090533564,
          0.06358444325679938
         ],
         "xaxis": "x8",
         "y": [
          "learning  ",
          "kids  ",
          "training  ",
          "reinforcement  ",
          "models  "
         ],
         "yaxis": "y8"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 0",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 1",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 2",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 3",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 4",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 5",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 6",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 7",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 500,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "Topic Word Scores",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis7": {
         "anchor": "y7",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis8": {
         "anchor": "y8",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis8": {
         "anchor": "x8",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alt_topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "skip",
         "legendgroup": "0_computer_ai_school_tech",
         "marker": {
          "color": "#636efa",
          "size": [
           13
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "0_computer_ai_school_tech",
         "orientation": "v",
         "showlegend": true,
         "text": [
          0
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          7.337852478027344
         ],
         "xaxis": "x",
         "y": [
          2.109586477279663
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "1_ai_teammates_collaboration_humans",
         "marker": {
          "color": "#EF553B",
          "size": [
           8
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "1_ai_teammates_collaboration_humans",
         "orientation": "v",
         "showlegend": true,
         "text": [
          1
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          -1.246701717376709
         ],
         "xaxis": "x",
         "y": [
          5.4196038246154785
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "2_robot_text_speech_work",
         "marker": {
          "color": "#00cc96",
          "size": [
           8
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "2_robot_text_speech_work",
         "orientation": "v",
         "showlegend": true,
         "text": [
          2
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          8.628267288208008
         ],
         "xaxis": "x",
         "y": [
          4.440662384033203
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "3_ai_ethics_students_case",
         "marker": {
          "color": "#ab63fa",
          "size": [
           8
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "3_ai_ethics_students_case",
         "orientation": "v",
         "showlegend": true,
         "text": [
          3
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          7.134211540222168
         ],
         "xaxis": "x",
         "y": [
          2.508531332015991
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "4_logical_teaching_language_kids",
         "marker": {
          "color": "#FFA15A",
          "size": [
           7
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "4_logical_teaching_language_kids",
         "orientation": "v",
         "showlegend": true,
         "text": [
          4
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          6.719105243682861
         ],
         "xaxis": "x",
         "y": [
          5.094915866851807
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "5_game_kids_information_hacked",
         "marker": {
          "color": "#19d3f3",
          "size": [
           7
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "5_game_kids_information_hacked",
         "orientation": "v",
         "showlegend": true,
         "text": [
          5
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          7.227514266967773
         ],
         "xaxis": "x",
         "y": [
          2.9742774963378906
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "6_implemented_mimic_capabilities_ai",
         "marker": {
          "color": "#FF6692",
          "size": [
           6
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "6_implemented_mimic_capabilities_ai",
         "orientation": "v",
         "showlegend": true,
         "text": [
          6
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          7.97305154800415
         ],
         "xaxis": "x",
         "y": [
          4.399386405944824
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "7_models_reinforcement_training_kids",
         "marker": {
          "color": "#B6E880",
          "size": [
           6
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "7_models_reinforcement_training_kids",
         "orientation": "v",
         "showlegend": true,
         "text": [
          7
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          6.195333480834961
         ],
         "xaxis": "x",
         "y": [
          5.32420539855957
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "8_security_relationship_tools_capabilities",
         "marker": {
          "color": "#FF97FF",
          "size": [
           5
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "8_security_relationship_tools_capabilities",
         "orientation": "v",
         "showlegend": true,
         "text": [
          8
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          -1.4233726263046265
         ],
         "xaxis": "x",
         "y": [
          5.766209602355957
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "9_robot_space_robots_lab",
         "marker": {
          "color": "#FECB52",
          "size": [
           5
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "9_robot_space_robots_lab",
         "orientation": "v",
         "showlegend": true,
         "text": [
          9
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          -1.321771264076233
         ],
         "xaxis": "x",
         "y": [
          6.431273937225342
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "10_understand_thing_crosstalk_ears",
         "marker": {
          "color": "#636efa",
          "size": [
           5
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "10_understand_thing_crosstalk_ears",
         "orientation": "v",
         "showlegend": true,
         "text": [
          10
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          6.452142238616943
         ],
         "xaxis": "x",
         "y": [
          3.8424715995788574
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "11_tiktok_youth_learning_global",
         "marker": {
          "color": "#EF553B",
          "size": [
           5
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "11_tiktok_youth_learning_global",
         "orientation": "v",
         "showlegend": true,
         "text": [
          11
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          7.334064960479736
         ],
         "xaxis": "x",
         "y": [
          4.325087070465088
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "12_gaze_task_robot_methodology",
         "marker": {
          "color": "#00cc96",
          "size": [
           5
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "12_gaze_task_robot_methodology",
         "orientation": "v",
         "showlegend": true,
         "text": [
          12
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          8.894229888916016
         ],
         "xaxis": "x",
         "y": [
          4.151827335357666
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "13_environments_gaming_virtual_ai",
         "marker": {
          "color": "#ab63fa",
          "size": [
           4
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "13_environments_gaming_virtual_ai",
         "orientation": "v",
         "showlegend": true,
         "text": [
          13
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          -1.5244910717010498
         ],
         "xaxis": "x",
         "y": [
          6.236063480377197
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "skip",
         "legendgroup": "14_alexa_experiences_words_kids",
         "marker": {
          "color": "#FFA15A",
          "size": [
           4
          ],
          "sizemode": "area",
          "sizeref": 0.0325,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "14_alexa_experiences_words_kids",
         "orientation": "v",
         "showlegend": true,
         "text": [
          14
         ],
         "textposition": [
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left",
          "middle right",
          "middle left"
         ],
         "type": "scatter",
         "x": [
          6.7956318855285645
         ],
         "xaxis": "x",
         "y": [
          4.0788421630859375
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 700,
        "legend": {
         "itemsizing": "constant",
         "title": {
          "text": "topic"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Intertopic Distance Map"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "range": [
          -1.7531647324562072,
          10.228364372253418
         ],
         "title": {
          "text": "D1"
         },
         "visible": false
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          1.7931485056877137,
          7.395965027809143
         ],
         "title": {
          "text": "D2"
         },
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get idp coordinates from plot\n",
    "\n",
    "idp = alt_topic_model.visualize_topics()\n",
    "#print(idp.show)\n",
    "topic_data = idp.data\n",
    "\n",
    "dots = topic_data[0][\"marker\"]\n",
    "dots = np.array(dots.size)\n",
    "\n",
    "x_coord = np.array((topic_data[0]['x']))\n",
    "y_coord = np.array((topic_data[0]['y']))\n",
    "\n",
    "topic_nums = these_topics.Topic\n",
    "topic_nums = topic_nums.drop(topic_nums.index[[0]])\n",
    "\n",
    "topic_labels = these_topics.Name\n",
    "topic_labels = topic_labels.drop(topic_labels.index[[0]])\n",
    "\n",
    "idp_coordinates = pd.DataFrame({\"x\":x_coord, \"y\":y_coord, \"topic\":topic_labels, \"dot_size\":dots, \"topics_nums\":topic_nums})\n",
    "#print(idp_coordinates)\n",
    "#idp_coordinates.to_csv(\"idpCoords.csv\")\n",
    "\n",
    "# plot IDP on my own\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "def improve_text_position(x):\n",
    "    # fix indentation \n",
    "    positions = [ \"middle left\" , \"middle right\" ] \n",
    "    return [positions[i % len(positions)] for i in range(len(x))]\n",
    "\n",
    "df = idp_coordinates\n",
    "\n",
    "fig = px.scatter(df, \n",
    "                 x=\"x\", \n",
    "                 y=\"y\", \n",
    "                 text=\"topics_nums\", \n",
    "                 color = \"topic\",\n",
    "                 size =\"dot_size\",\n",
    "                 labels=dict(x=\"D1\", y=\"D2\"),\n",
    "                 width = 1000,\n",
    "                 height = 700,\n",
    "                 title= \"Intertopic Distance Map\"\n",
    "                 )\n",
    "fig.update_traces(\n",
    "   hovertemplate=None,\n",
    "   hoverinfo='skip',\n",
    "   textposition=improve_text_position(df['x']\n",
    "   )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "     xaxis={\"visible\": False},\n",
    "    yaxis={\"visible\": False}\n",
    ")\n",
    "   \n",
    "# Prepare figure range\n",
    "x_range = (df.x.min() - abs((df.x.min()) * .15), df.x.max() + abs((df.x.max()) * .15))\n",
    "y_range = (df.y.min() - abs((df.y.min()) * .15), df.y.max() + abs((df.y.max()) * .15))\n",
    "\n",
    "# Update axes ranges\n",
    "fig.update_xaxes(range=x_range)\n",
    "fig.update_yaxes(range=y_range)\n",
    "\n",
    "#fig.write_image(\"results/idp_pair1.png\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"For kids, especially younger, the more tactile they can get on anything the better and the more able they are to really pay attention. So just if we can make physical representations of those trees and the logic and things that move to the better. So one of my sons teach preschool teacher, they have were teaching them shapes and they have like shaped man who gets on the floor and they put them all the pieces together to make shapes and patterns and things. You can do the same thing with logic trees and decision making and then even understanding things like privacy, where you know how far something spreads or what you're taking. So just the more tactile you can really make something for kids at that age the better. | I think it's going to be a lot easier than we think just because they're so ingrained in the technology itself. My daughter is just about to turn one and she can already swipe on a tablet because she's watches her brother swipe on a tablet. And I think just those simple movement to think, they're going to be so more used to it than we are getting used to it. I don't think it'll be that much of a stretch just relating it to the devices they already use or have , I mean, even Alexa is a type of AI, because it does learn things about you, it learns to pick up your voice and your inflections and specific words and what you like. So, I mean just being able to talk about the everyday devices that they use is going to be easy for them.\"]",
          "['I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI. ']",
          "[\"I think it's wildly important. I don't think that programming should be something you go to college for. I think this is something that everybody should learn. It literally should just be like typing class. And I think AI should be... I don't know. Because some people, I don't know, I don't want to make people go that deep into something that they might not be that interested in. You know, I think everybody should learn programming because I think it'll become somewhat of a basic skill too. But actually, you know what, we got AI that are starting to program now and you got the discussion that manual programming is going to become obsolete because AI can program everything. So, that just kind of speaks to the complexity of what AI represents. I think it should be included more in the curriculum. I'll meet in the middle. I think it should be included more in the curriculum. I might not put it as an entire class. However, I do think that children should be given many more opportunities to learn about these technical things than they currently are. I think we should give them more to challenge them with. I think kids can do a lot more than we think they can.\"]",
          "[\"It's definitely different than undergrad. I got my undergrad at Clemson too, in genetics. So, I understand. I'm doing a similar thing as you. Would you tell me a little bit more about your AI teaming and cognition, your research, what you're studying? | Sure. All the research I've done up to this point has been looking at basically what does team cognition currently look in human-AI teams? I did a study on the role of spatial awareness or the availability of spatial information. And then I did another study that was essentially just looking at, it was pretty exploratory in the fact that I just studied or I just collected measures of shared mental models, along with trust and a bunch of qualitative data. So, for the objective measure, I say objective, but it's hard to actually measure a construct like that objectively. We didn't get anything on the objective measures, but the qualitative data that we got was really interesting. The perceived team cognition too is lower when you're working with AI, which I thought was pretty interesting. And then, there was lower trust as well. And it gets worse when you work with two AIS and you're the only human. When you become a minority member of the team, versus when it's two humans and one AI. So, that's where my research has been. And basically what I want to move into is how to develop, or figuring out what qualities an AI can have that is going to best support shared understanding between humans in a human-AI team. And then going that extra step and creating basically a shared mental model of the AI teammate and its operation. And that sounds a lot like AI explainability and transparency, but I think you take it a step further in making sure that both human teammates have a shared understanding of that. You know, I can go deep into the weeds in terms of the theory behind it, because you got task mental models and team mental models, and I'm kind proposing that there should be a AI teammate mental model that the humans can share, but basically developing AI that are going to support the more traditional aspects of shared understanding for humans. And then also developing them in a way that makes it easy for the humans to develop a mental model of the AI teammate, because you know, working with AI is very different. They are very smart and very stupid at the same time.\"]",
          "[\"Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight. | It depends on the context. So if it's a Google image search engine, many kids say, yeah, I'm not represented here, but that's how the world is. The consequence isn't huge. I search for computer science professor and I don't see myself as a African American woman. Okay, that hurts. But I don't think Google should mess with that. Then when it comes to hiring algorithms, yeah, they see that as problematic. So the context really matters and the effect of the consequence as they see it matters to them.\"]",
          "[\"I would say my machine learning to me, my understanding is it's kind of the algorithm behind the scene. I consider artificial intelligence as kind of like with, how to say, a subject being there. It could be virtual, it could be a robot being there, but machine learning is more like the algorithm behind it. Kind of like its mind or core. That kind of feeling. It's kind of like if I use human as an example, it's just like artificial intelligence is a human and machine learning is kind of his brain to think, to help it to learn and make predictions.\"]",
          "['Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick.']",
          "[\"Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess.\"]",
          "[\"Yeah. And even just the tendency, like one thing I was discussing in a research project I'm working on right now is that a lot of people didn't trust an AI to work in a risky decision be because they thought it'd be too logical. Like it's going to make a decision based off what the exact probability of something happening is versus what a human in this scenario is probably going to start thinking of the worst case as possible and they may be tangentially probable, but it might be so bad that they're not going to make a decision. And that's part of a human emotional factor that's not going to be replicated by a machine.\"]",
          "['Unfortunately, it can kind of amplify some of the systemic issues that are already happening. | One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system.']",
          "['Oh yeah. I think that\\'s more important, but it\\'s mostly because I think it\\'s more important because you as an individual interact with society through those platforms and through that data and through that AI, even if you don\\'t realize you are, so me watching a video on YouTube, if I watch a video that has ethical issues, if I watch a video that it just is a very mean spirited video that can harm someone else by watching it I\\'m promoting it. And then while it might be, \"well, I didn\\'t tell anyone about it.\" It\\'s still me actually watching it needs an algorithm system that then promotes it to especially people in my geographic region or people with somewhat profiles to me. So it\\'s this idea that, I think it\\'s important because it also important and ultimately I think it\\'s the fault of the company that is the case, but I don\\'t trust them to fix that. So I think it\\'s more important to educate the individual on how that\\'s going to work. And then I think that\\'s just, once again your data literacy, understanding your digital footprint and also understanding your worth, humans are worth more as data to people than they are as humans. So I think understanding just how much your data is worth contextualize, you should just be giving it out for free, it is something fairly important and also the other adage of nothing is free in life where it\\'s, \"yeah, all these things are free.\" TikTok is free, but you\\'re getting a lot of data and you\\'re getting advertised to a lot.']",
          "['Technology in general.\\n | I met a guy in person, he used to help make movies. He\\'s like, \"yeah, my job used to take a team. And now it\\'s just one dude at one program.\" And so he\\'s changing careers into learning how to code, and so there\\'s that. Consumers might not be aware that they\\'re interacting with AI when they\\'re shopping online, except it might not be aware to the extent that they are and that can have harms to principles like consent, being aware enough of what\\'s going on to be able to fully make a decision and not doing that you\\'re interacting with AI or that visual. Invisible changes are happening on the screen behind the scenes. It might be bad depending on that context. And it\\'s fucking hard to regulate AI.']",
          "['I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI.  | It\\'s the same bucket. If it\\'s a random person on the street, one, why the fuck are they talking to you about it, machine learning. Usually when we have that discussion, the technical discussion of AI versus machine learning, machine learning might be seen as a sub-case of AI a broad category and machine learning being a set of particular techniques for how we optimize AI. So having the machine learn from data, again, a big metaphor about comparing the human brain to computers, which is a bad thing to do for your own wellbeing. We aren\\'t machines. Machines don\\'t think like us. We call it... this is from Ellen Ullman\\'s Close to the Machine, fantastic book. It\\'s a memoir. I have it with me. And she talks about how we call it the machine. We call it a memory, but it\\'s not right. So one, I don\\'t like the name machine learning when people use it too much to compare to how babies. I\\'m like no, fuck, that\\'s not it. But a way of optimizing around data, and so it\\'s just really cool statistics. And it\\'s really close to data science, where you\\'re like you have data, but what meaning can you get out of this for a social reason? What\\'s blind on this? I don\\'t really care about the difference between, but then AI a more general label. If I have a bunch of if/then statements. Is that AI? Well, yeah, it used to be. We were trying to solve like chess originally or checkers. We had some pretty simple sequence of steps because that\\'s how we thought the human brain worked or at least that area thought things worked. So I don\\'t know. I use them broadly because I don\\'t think the specifics matter. It\\'s when we get to the ethics side of things.']",
          "[\"I mean, I'm a cynic in this regard. I think everything in this country and this world is motivated by money and financial interest, and corporate entities maintain that at the highest level. And depending on the governments, they either promote that or try to curb that back. But at the end of the day, it's all about money. So I think when you talk about AI and dealing with the problems that are dealt with, with AI, you have to understand that people are making money off of AI. Even if it's terrible AI in terms of being this most biased, terrible agent in the world, someone's going to make money off of it potentially. And I'm a cynic in that regard. People will put money above much of their own ethics in some cases. | No. I think what you're doing is important. I think that AI, machine learning, reinforcement learning, neural networks aren't going anywhere. They're going to become a bigger entity and play a bigger role in society as time goes by. So the people that are really going to matter in terms of making sure that this works for other people, our youth, their kids, they're the ones that are going to actually have the big impact on being able to change this and understand it at a better level then people that are our age it. It starts with the education of the things that we're talking about, understanding how this can go wrong, understanding the errors so you're educated on that and you can avoid that. In many ways, I think what people like me and a lot of people that are similar to me, we just try to put bandaids on this because we're too far along on the road. And youth have the ability to not just put bandaids on things, they can actually fix things, I think.\"]",
          "[\"So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful.\"]",
          "[\"I would say it's a computer or an agent that can take information and learn from it and make prediction or decision based on their ease. I don't know, learning process. | I would say my machine learning to me, my understanding is it's kind of the algorithm behind the scene. I consider artificial intelligence as kind of like with, how to say, a subject being there. It could be virtual, it could be a robot being there, but machine learning is more like the algorithm behind it. Kind of like its mind or core. That kind of feeling. It's kind of like if I use human as an example, it's just like artificial intelligence is a human and machine learning is kind of his brain to think, to help it to learn and make predictions.\"]",
          "[\"Yeah. So there's code blue resuscitations in hospitals. I think you guys are probably familiar with that, when somebody goes into cardiac arrest. Whenever that happens, an alarm goes off in the hospital and everybody that is assigned to that code blue that day is supposed to, in theory, stop what they're doing and run to that room and help triage a patient in real time. And it's one of the best instances that I've ever studied of human-human teamwork. It's an absolute chaotic environment where you have extreme time pressures. People don't have, they have information disparities, inaccuracies, they don't know what's going on really so they're trying to understand at an individual level, what do I do? What do my team members do? Leadership is needed. One of the biggest problems that you see within those is what I was just saying, is information disparities and information needs not being able to access that very quickly. So in the communities I've been talking to regarding this, there's the development of utilizing robots and intelligent agents to aid in that collaborative decision making process, where the intelligent agent is providing, it's looking at all the data on the human, because as a human, we give a lot of biophysical data and can't possibly look at all of that in real time. It takes us a long time to go through all that. So an AI agent that has been trained on an algorithm to look at peaks and valleys of all of that, and then flag it can be trained to provide real time relevant information to that team when they need it. They don't have time to dig through the data, where the AI can dig through all that real time data and alert you of multiple things that are going wrong so you can better understand the medical operational environment. That's an example of where we're heading towards. | Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight.\"]",
          "[\"That's awesome. I love that. What are your thoughts about youth learning about AI or machine learning? | I think it's wildly important. I don't think that programming should be something you go to college for. I think this is something that everybody should learn. It literally should just be like typing class. And I think AI should be... I don't know. Because some people, I don't know, I don't want to make people go that deep into something that they might not be that interested in. You know, I think everybody should learn programming because I think it'll become somewhat of a basic skill too. But actually, you know what, we got AI that are starting to program now and you got the discussion that manual programming is going to become obsolete because AI can program everything. So, that just kind of speaks to the complexity of what AI represents. I think it should be included more in the curriculum. I'll meet in the middle. I think it should be included more in the curriculum. I might not put it as an entire class. However, I do think that children should be given many more opportunities to learn about these technical things than they currently are. I think we should give them more to challenge them with. I think kids can do a lot more than we think they can.\"]",
          "['I met a guy in person, he used to help make movies. He\\'s like, \"yeah, my job used to take a team. And now it\\'s just one dude at one program.\" And so he\\'s changing careers into learning how to code, and so there\\'s that. Consumers might not be aware that they\\'re interacting with AI when they\\'re shopping online, except it might not be aware to the extent that they are and that can have harms to principles like consent, being aware enough of what\\'s going on to be able to fully make a decision and not doing that you\\'re interacting with AI or that visual. Invisible changes are happening on the screen behind the scenes. It might be bad depending on that context. And it\\'s fucking hard to regulate AI. | Yes, exactly. There\\'s a lot of other things that go into it. So we\\'re going to pivot a little bit more. I know you don\\'t necessarily always work with youth, but what are your thoughts about youth learning about machine learning or learning about algorithms and AI?\\n']",
          "[\"I mean, I know privacy is always a big issue, especially when you're talking about collecting major data, in order for something to apply or use machine learning. It's got to be collecting a lot of data about environment and people it's working with. So you have people who are comfortable sharing different levels of information and different levels of being information collected about them. And then also if you have an agent like that constantly collecting data, wherever it's working or interacting, there's the concept of like, okay, at what point do you require people to be like, oh, where this is going on and happening and require some sort of consent versus like it's just, it's so ubiquitous that everybody just knows it's going on. There's probably a tipping point somewhere there, but I think that are long ways off from that. So I think the privacy concerns are going to be pretty, pretty important. | Definitely. Yeah. Okay. So just shifting a little bit. This is more directly related to some of the work that we do in my lab. What are your thoughts about youth learning about artificial intelligence or machine learning even as young as elementary or middle school?\"]",
          "[\"Yeah, so work that I've done has been outlining how there are different camps I've worked in. Number one is outlining how humans and AI interact with each other, specifically my dissertation work is on how humans and AI systems can impact and influence each other in a task. So how a human could be susceptible to letting a robot or an AI system tell it what to do and take commands from it or vice versa. So looking at what comprises that, how humans should lead AI or how an AI should lead a human, things like that. So that trade off and then I've also done work looking at how ethics in AI systems can be implemented, created and how it interacts with humans. So, it's fairly big part is the ethical implications of AI systems and how those ethical implications ultimately impact the utility of AI. And then the last, there are other smaller things that I've had to do. But then the last main one I worked on is AI. There's a grant that I work on fairly often that is a grant that looks at using machine intelligence and recommender systems to provide recommendations to teachers for professional development. So it's a lot of providing recommendations for their professional development and I work on the side of that where I work on building this system and outline in that aspect. | Yeah, I'm actually familiar with that one, because it's the CU-TLP program. We have some people in the learning sciences that are working on that. So I'm familiar with this one. Would you mind giving me a little bit more detail about your project that involves AI ethics? What were some of the results that came out of that?\"]",
          "[\"Yeah. And even just the tendency, like one thing I was discussing in a research project I'm working on right now is that a lot of people didn't trust an AI to work in a risky decision be because they thought it'd be too logical. Like it's going to make a decision based off what the exact probability of something happening is versus what a human in this scenario is probably going to start thinking of the worst case as possible and they may be tangentially probable, but it might be so bad that they're not going to make a decision. And that's part of a human emotional factor that's not going to be replicated by a machine. | Yeah. That's great. Like simulating that environment a little bit closer to what we had in person potentially.\"]",
          "[\"So one job I had, when I was on a team, we would go out to different sites of critical infrastructure and basically assess their security posture and detect if they had any breaches or things like that and that all I've also done instant response missions, where a corporation may detect that they've had a breach of their security defenses and we come in and figure out where it came from, how to fix it and get rid of all the traces that are still on their systems or the networks. | Okay. Got it. Got it. So then how did you become interested in joining the research lab that you're in now and exploring more AI applications?\"]",
          "['Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences. | Yeah, so from two perspectives, in terms of full formalized teaching, I\\'ve taught the second one is about stuff I do with my mother, but I\\'ve taught coding courses at the collegiate level, like lab courses at collegiate style. And so there is less of a gap there in terms of age. I was pretty much two years older than everyone I was teaching. And so yeah, you do get interesting. You do have to work down in terms of their understanding, but also you can still stay at a very high level, but even when I was doing that, the best instructors I\\'ve ever had are the ones that pulled up their code and did it with you. So, that\\'s just how I did it. Like I said, I think what changes is the modality you do it through. So for college kids, you don\\'t pull up a YouTube and talk about how their data is produced there. You just pull up data and you work with them through a data set. So, I think the teaching principle, the pedagogy principle, [inaudible 00:38:56] whatever the word is, principle still stays the same, which that demonstration aspect, the difference is how abstract that demonstration gets at the collegiate level. We don\\'t need that. But then my mother works with students who have gotten interested in stem and I\\'m like, \"yeah, I can talk to them about this.\" And it\\'s more about like once again, the education side for that younger side is, I don\\'t want to just sit up there and tell them about those things. I want them to go find what they find interesting. So I give them the website of, \"here\\'s is all, here\\'s scratch.\" Just play around with it for 10 minutes. I was like, \"I don\\'t want to touch it.\" I\\'m not going to talk to you about it. You just go play with it for 10 minutes because that\\'s like how I not only learned it, but I got interested in it. Because I think it\\'s less interesting for a student to be told what to do. And it\\'s way more interesting if they were, \"here\\'s a fun thing. Here\\'s 10 minutes.\" Instead of doing homework in class, go do this for 10 minutes. And that was always my favorite part of doing computer stuff when I was younger.']",
          "[\"I think one of the topics that keeps coming up is this issue of facial recognition, and a lot of research in that area where the accuracy of facial recognition is certainly in doubt, especially for people of color. There's a lot of research and certainly a lot of improvements being made in those areas, but still, wrongfully identifying somebody because of a failure in a AI system for facial and image recognition is devastating for the people. When you're not one of those people who are flagged, you're like, well, what's the deal? That doesn't affect me, but if you're that person, it's life changing in a bad way. It could be. | One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system.\"]",
          "[\"Yeah. That's great. Like simulating that environment a little bit closer to what we had in person potentially. | I mean, the aspect is so, AI is only as good as the data we give it, things like that. And I think there's a lot of discussion and research right now about a lot of AI being really biased to majorities because that's the data that they have access to. And so there might not be the ability to accurately assess or communicate with minority populations. And it could, I think there's some discussion about it kind of reinforcing biases and stereotypes because it's just operating off of the set of data that it's given.\"]",
          "[\"Starting with logic trees and then some state machine logic, things like that. | Yeah, absolutely. Kids are getting Chromebooks in second grade now. The schools are distributing. We just talked about our children, interacting with Alexa that's a intelligent system, so.\"]",
          "[\"Yeah. Yeah. I think so too. And they're so creative, the kids that ... All kids really, but the kids we're working with are just phenomenal. And once they get into it, they design such amazing things to help people. They're really interested in designing robots to help and robots for social good. So they're really understanding this stuff. And even if the goal is not for them to go be computer scientists or to go build these, but to have that fundamental understanding so they can be critical consumers, so they stop and say, wait, this is wrong. I need to say something. That's what we're hoping, obviously. I mean, we're not going to follow them, but we're hoping that what we do has a little bit of that impact anyway.\"]",
          "[\"Yeah. So there's code blue resuscitations in hospitals. I think you guys are probably familiar with that, when somebody goes into cardiac arrest. Whenever that happens, an alarm goes off in the hospital and everybody that is assigned to that code blue that day is supposed to, in theory, stop what they're doing and run to that room and help triage a patient in real time. And it's one of the best instances that I've ever studied of human-human teamwork. It's an absolute chaotic environment where you have extreme time pressures. People don't have, they have information disparities, inaccuracies, they don't know what's going on really so they're trying to understand at an individual level, what do I do? What do my team members do? Leadership is needed. One of the biggest problems that you see within those is what I was just saying, is information disparities and information needs not being able to access that very quickly. So in the communities I've been talking to regarding this, there's the development of utilizing robots and intelligent agents to aid in that collaborative decision making process, where the intelligent agent is providing, it's looking at all the data on the human, because as a human, we give a lot of biophysical data and can't possibly look at all of that in real time. It takes us a long time to go through all that. So an AI agent that has been trained on an algorithm to look at peaks and valleys of all of that, and then flag it can be trained to provide real time relevant information to that team when they need it. They don't have time to dig through the data, where the AI can dig through all that real time data and alert you of multiple things that are going wrong so you can better understand the medical operational environment. That's an example of where we're heading towards. | Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "['Technology in general.\\n']",
          "['Machine learning is an AI that essentially learns by itself and it improves by ingesting more data to perform a task with better efficiency and accuracy. That\\'s kind of what I think of it. That\\'s why when people start talking about training data, their eyes glass over, but it is about that. You\\'re just educating this AI, helping it to learn, like what a cat looks like. So that, when you say, \"Is this a cat?\", it knows what a cat looks like, and that\\'s what we see in image recognition and stuff like that. That\\'s why you can look for cats on the internet. It\\'s just that, more and more data, but the downside of it is, you\\'re assuming that what you\\'re feeding it is accurate and complete. | Yeah. It\\'ll definitely change the whole algorithm. Switching gears a little bit, we\\'re going to start talking about youth. What are your thoughts about youth learning about AI or machine learning?']",
          "[\"Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "['One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system.']",
          "['That\\'s awesome. I mean we need more people to understand coding in general. So I think it\\'s great that you do that work. Has any of the work you\\'ve done with those type of groups ever bridged into AI before or machine learning? | So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]-']",
          "['Well, I mean, so there\\'s definitely documentations of bias that are in there for essentially, right, the people who are designing and making these things cook in their own biases into there. And so if 98% of the data that we trained on are a bunch of white males, then anybody not in that category is going to be on the margins of the data set. And that means they have not been properly trained into that data. And so we\\'re going to be misclassified at a much higher rate. So I mean, I think there\\'s good efforts that are out there now to actually really think about representation and training sets. I don\\'t know how that gets reported. I haven\\'t seen anyone who has openly reported their training statistics that way, that, \"Here\\'s how we balance gender and all the other demographic things that would go into impacting these outcomes.\" So in a healthcare setting, I mean, it\\'s happened before machine learning, is doing these diagnostic things, the reports of people where there\\'s no black people represented in medical textbooks, right? That\\'s not about a machine learning thing. That\\'s about a human learning thing, where if you\\'ve never actually practiced or considered examining a person\\'s body who\\'s different than cliche US Western standards, then you\\'re going to make errors. And so I think we\\'re making the same mistake that we ignored before machine learning. We\\'re making the same mistake. And yeah, I think right now, the biggest thing for me would be transparency on these things. How are these systems trained? What do we know about those data sets? So yeah, they\\'re much more likely to impact marginalized communities because those are going to be the data that\\'s on the margins in the training sets. But I do think there\\'s repercussions for everyone as well. And that\\'s part of my worry too, that definitely some populations are going to be overrepresented in how they\\'re negatively impacted. But I think there\\'s negative impacts across the board. And so again, emotion recognition kind of things, I think those are problematic. Medical examination kind of things, I think are really problematic. So I\\'m skeptical across the board, but yeah. [crosstalk 00:28:37]-']",
          "[\"Sure. Yeah. Currently an assistant professor in computer science and learning sciences at University of Illinois, Chicago. The work I do is in social robots for educational purposes that are designed as learning companions, where they work with or around kids to help them learn in lots of different scenarios. My training with it is... My PhD's in learning sciences from University of Wisconsin, but I have a PhD minor in computer science where I did a lot of human robot interaction work. So I'm trained on the HR, the design of the robots and the interactions, and on learning and learning theory. And so the combination of those two is to design learning interactions for kids to really enhance their learning experiences. | And what sort of technology or programming do you use for these robots or have you been using? Do you use artificial intelligence or machine learning?\"]",
          "[\"I think you can introduce it pretty young depending on how you do it. I taught a fourth and fifth grade after school class on computer science and programming and we were just kind of learning about how to do programming and scratch and they were making little games, but it was more about kind of how do you... It's about computational thinking, right? It's how do you break a problem down and think of a solution step by step and those are pretty young kids. And I think the idea of extending that to how can you think about machines or computers that are intelligent and what does it mean to kind of have a system learned from examples rather than telling it instructions step by step? I think you can start to kind of explain those concepts at a pretty young age, at a high level. You don't need to get into the weeds of a deep learning algorithm or anything even at the high school level. I don't think it's important to kind of get into the details of how any particular ML algorithm is built. But I think it's important to think about just what does it even mean to have a machine learning model and what does it mean to learn from data and learning about the importance of knowing where the data comes from and blah, blah, blah. So I think it could be pretty early. | I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that.\"]",
          "['Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick. | Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "[\"Sure. Basically, I have not been officially in the working world yet. I did my undergrad in psychology in Clemson and graduated December of 2018. Then my current grad program, which is in human-centered computing, did not start until the fall. So, I was a substitute teacher at a middle school for that whole semester. And then over the summer, I just kind of take a break, went to Europe, all that good stuff. Then I started in the fall and I've been a research assistant ever since, working on human-AI teaming projects. Specifically, my research is on team cognition in human-AI teams.  I don't know, we can get more into the weeds with that, but the only other professional work experience I have is working at a light during internship my junior year of undergrad. And they did benefits management stuff. To be honest with you, I know this is a bad AI. Basically, their entire company could go away because they... I don't know, my job, it was just so much data management and it's crazy because they hire people with four-year degrees to do these jobs that you could basically do a Python script for.\"]",
          "['That\\'s great, that\\'s a good perspective. So what do you think about youth learning about the ethical issues of AI? | AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "['All good, there\\'s too many schools. Summer 2019, it was in-person. Summer 2020 it wasn\\'t held for the obvious reason. But because it wasn\\'t held in 2020, we\\'d started doing during the semester some three Saturdays in a row camps instead of three weeks continuous. So I didn\\'t make as much money that year, because I didn\\'t have the... it pays a thousand a week, which is good money when you\\'re trying to pay fucking rent over the summer and your [inaudible 00:03:34] doesn\\'t get paid over the summer. So I taught instead 2020, I did remote. We were trying out a few different ways of running it. Right. And we kind of did some experiments with it and it was a class on... shoot, what the fuck was it called? Analysis Skills for College Success: Research and Data Analysis, something like that. It was essentially a research methods course for high schoolers. So they know that the word research doesn\\'t mean Googling shit. It means thinking through things in a certain way, but it had to work no matter what major the student might go into. So I got to teach this really, really mixed methods, fun little three week camp thing to students drawing on the fact that I have a crazy, weird, mixed background and I continue doing weird mixed stuff. But all the offerings of that, every time I\\'ve done it, students the first time had fun. They were really attentive and stuff. Last time I had one student who actually did anything. And so students have just been increasingly burnt out from that. But summer \\'20, this most recent summer, we did it online. So I came into my office on Zoom and just was on Zoom for nine to three, but breaks, trying to mirror as much as we could of the in-person experience from the two years prior. It was better because students were a bit more captive. They are honors kids with rich parents, honestly. And they were excited. They wanted to go to this camp. They know it was canceled the previous year. They\\'re big nerds. I never went to those camps, but I\\'m glad to teach the kids who go to those camp. So we\\'ve adapted with the pandemic with that. But it\\'s all just various kinds of teaching. That\\'s my one-line bio that I teach kids and adults how to code which is the [crosstalk 00:05:38].\\n | So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]-']",
          "['I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic.']",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything? | Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "[\"If you sit around in a room and you hear a bunch of people just talking, you learn a lot. They're not teaching you, you're just learning a lot. You're just kind of catching it. I think by employing things like gaming and some of these thought workshops, like what would you do in this case? It's interesting. That's why I really like teaching the class I'm teaching this semester, which is computing, ethics, and global society. Because some things that seem simple, are not. They're tough decisions. They may benefit some people. They're going to hurt some people. Is it the right thing to do? Even though you can do it, should you do it? It's-\"]",
          "[\"Yeah, that's fascinating. And I think that's a really good example of the power of AI and how it can be so helpful, especially, like you're taking the person's strengths, you're taking the AI strengths and bringing them together to make this really functional system even more functional. So what are some examples then kind of on the opposite end of the spectrum of how potentially AI or machine learning can harm us? And who in particular do you think it harms? | Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"So in work, we mainly test actually how humans react to AI with different attributes, kind of like human, maybe talk a lot or maybe do a lot or different features. And sometimes actually we would use a wizard [inaudible 00:08:08] just because AI teammate means they need to coordinate at a higher level instead of just like, okay, I get the data and analyze it. I give you a prediction result. So it's more than that. And that's why a lot of times we would use wizard of OZ to try to make sure it's kind of controlled, all the variables, the confounding variables are controlled, especially in experiments. And so in addition to that, in terms of [inaudible 00:08:41] we have use. Normally it's the machine learning algorithm that's used to kind of give them an instruction in terms of how they react in different situations. And in terms of AI in personal life, I actually don't really trust AI that much. Siri on my phone is always off. I don't really use it and I don't have a Google home. I just don't feel very comfortable that it listens to my voice all the time. Although I'm not saying anything that cannot be heard, it's kind of a privacy issue, I guess. For instance, if my family member really wants to have one, I would be fine with it. Although I personally would not choose to have one, if that makes sense?\"]",
          "[\"Good. I mean that's fascinating. That's great. So you touched on the fact that you had some experience teaching young people about computer science. If you were going to teach, just think about what you could do if you were going to teach some young people about machine learning or AI, what type of activities or resources would you maybe use in order to do that? | I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that.\"]",
          "['Yeah. That makes sense. I am still learning how to code. So I\\'ll get there one day. | Oh yeah. I think that\\'s more important, but it\\'s mostly because I think it\\'s more important because you as an individual interact with society through those platforms and through that data and through that AI, even if you don\\'t realize you are, so me watching a video on YouTube, if I watch a video that has ethical issues, if I watch a video that it just is a very mean spirited video that can harm someone else by watching it I\\'m promoting it. And then while it might be, \"well, I didn\\'t tell anyone about it.\" It\\'s still me actually watching it needs an algorithm system that then promotes it to especially people in my geographic region or people with somewhat profiles to me. So it\\'s this idea that, I think it\\'s important because it also important and ultimately I think it\\'s the fault of the company that is the case, but I don\\'t trust them to fix that. So I think it\\'s more important to educate the individual on how that\\'s going to work. And then I think that\\'s just, once again your data literacy, understanding your digital footprint and also understanding your worth, humans are worth more as data to people than they are as humans. So I think understanding just how much your data is worth contextualize, you should just be giving it out for free, it is something fairly important and also the other adage of nothing is free in life where it\\'s, \"yeah, all these things are free.\" TikTok is free, but you\\'re getting a lot of data and you\\'re getting advertised to a lot.']",
          "['Yeah. We have thought of that, integrating Snapchat filters or something like that. Something that they would be interested in. | Are you talking about just learning AI and ML in general or the ethical issues?']",
          "[\"Yeah. And even just the tendency, like one thing I was discussing in a research project I'm working on right now is that a lot of people didn't trust an AI to work in a risky decision be because they thought it'd be too logical. Like it's going to make a decision based off what the exact probability of something happening is versus what a human in this scenario is probably going to start thinking of the worst case as possible and they may be tangentially probable, but it might be so bad that they're not going to make a decision. And that's part of a human emotional factor that's not going to be replicated by a machine. | And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there.\"]",
          "['Yeah. I love that, Joe. And I can see that in your work, the way you\\'ve described it. That\\'s great. Okay. So let\\'s switch. So talking a little bit about why we\\'re doing this interview, right? We\\'re taking what people are saying and trying to apply it for learning experiences for young people. So what are your thoughts just generally about elementary school, middle school-age kids learning about either AI machine learning and the social and ethical impacts or both? [crosstalk 00:29:56] ideas around that, what they should learn, what\\'s important for them to know? Can they [crosstalk 00:30:00] in those issues | Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick.']",
          "[\"Yeah. I mean, so like my young son, if we're doing something like virtual learning, he's got to be on virtual learning for that day or that week. It's very difficult for him to pay attention to just a screen for longer than like 10 to 15 minutes. But maybe if it was like a very immersive environment where you had some AI students that were almost collaborating with you, it might be something he could engage in and get a lot more from that environment. | I mean, the aspect is so, AI is only as good as the data we give it, things like that. And I think there's a lot of discussion and research right now about a lot of AI being really biased to majorities because that's the data that they have access to. And so there might not be the ability to accurately assess or communicate with minority populations. And it could, I think there's some discussion about it kind of reinforcing biases and stereotypes because it's just operating off of the set of data that it's given.\"]",
          "[\"I mean, I know privacy is always a big issue, especially when you're talking about collecting major data, in order for something to apply or use machine learning. It's got to be collecting a lot of data about environment and people it's working with. So you have people who are comfortable sharing different levels of information and different levels of being information collected about them. And then also if you have an agent like that constantly collecting data, wherever it's working or interacting, there's the concept of like, okay, at what point do you require people to be like, oh, where this is going on and happening and require some sort of consent versus like it's just, it's so ubiquitous that everybody just knows it's going on. There's probably a tipping point somewhere there, but I think that are long ways off from that. So I think the privacy concerns are going to be pretty, pretty important. | Yeah. As, so as someone who works so closely with AI, can you imagine an elementary school student or a middle schooler or even younger than that, what sort of, how could you break that down? Like AI or machine learning to get them kind of exposed to that?\"]",
          "[\"Yeah, there's a really thought provoking... Hang on, just let me see. It's a great thought provoking website. What is it called? Oh yeah. moralmachine.net. If you go to moralmachine.net and then click the judge link, what this is, is this research that's done by MIT and a handful of other universities, and it asks you some interesting questions because everybody's talking about self-driving cars, and how they're going to happen. They're already happening. It's going to happen, and the reason behind it is, humans are not good at driving. People always think they're good at driving, but they're not. So, if we get the self-driving stuff done correctly, even if it's not entirely self-driving, even if you're sitting in the driver's seat and have to take over, there are these interesting moral decisions to be made by the technology in the event of something unexpected. This website poses all these interesting questions about, if you were programming this self-driving car, what would you do in the situation? Your brakes fail. If you continue on your path, you're going to destroy three children in the pedestrian path. If you swerve, you're going to kill everybody in your car because you're going to crash. It's an interesting reveal of how we think and they, MIT and these other universities collect this data. I think for young people, gaming, just taking advantage of gaming technology to educate in a way that's maybe not right in your face, I think is a good way to learn. So much of learning is caught rather than taught. You just never know. | If you sit around in a room and you hear a bunch of people just talking, you learn a lot. They're not teaching you, you're just learning a lot. You're just kind of catching it. I think by employing things like gaming and some of these thought workshops, like what would you do in this case? It's interesting. That's why I really like teaching the class I'm teaching this semester, which is computing, ethics, and global society. Because some things that seem simple, are not. They're tough decisions. They may benefit some people. They're going to hurt some people. Is it the right thing to do? Even though you can do it, should you do it? It's-\"]",
          "[\"Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important.\"]",
          "[\"So I'm currently a fourth year PhD student in human centered computing, where we basically just study how humans interact with different technologies. And in our lab, our research focus is mainly human AI teams, human AI teaming. So we basically study how humans interact with AI teammates in a given environment like gaming, where AI is pretty common to see. And before that I did my bachelor and master in engineering. So basically it's kind of like the algorithm behind the thing. That's basically my background.\"]",
          null,
          "[\"Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "['Yeah. Absolutely. You answered everything. I\\'m looking at my follow-ups. I\\'m like, \"Oh, you got that. You got that.\" Yeah. I mean, if you want to expand a little bit, if you were to take Jules for example, right, your daughter, and she\\'s six, and what would you want her to know about machine learning or about at that age, anything or about how harmful it can be? Would you talk to her about privacy? Would you talk to her about misrepresentation or discrimination? Where would you go? | Yeah. One that just comes to mind that didn\\'t come up is self-driving car technology. And that is dangerous in a much more salient way, right? And I think getting people to understand, when I see a car and I look over, I evaluate what the person behind that wheel is attending to, right? Before I cross the street, I like to make sure that I see that person look at me so that I know that they\\'re sensing me, right? So if and when it comes to be that I look over and there\\'s nobody at that wheel, there really needs to be an acclimation to understanding, \"Okay, the situation is that there\\'s a machine driving that car. How does it make mistakes?\" I know how people make mistakes, right? I know what it is. If they didn\\'t look at me and I start to walk out and they just roll even at a red light, that could catch me. So I pay attention to that. What are the types of errors that machines make in that scenario are important, that we know that sometimes, it might not recognize a stop sign at all. People are typically better at it. Maybe not typically, but we learn to understand the types of errors that can be common in these systems, or even uncommon, if they\\'re going to be catastrophic, and are aware enough in a way that we can respond in reasonable actions to it. I mean, right now, if I saw a driverless car, I would go nowhere near the street because they\\'re prone to weird accidents that I don\\'t understand right now. So I can\\'t figure them out well enough to do it. But eventually, if they become commonplace, we\\'ll have to know how that works.']",
          "[\"No. I think what you're doing is important. I think that AI, machine learning, reinforcement learning, neural networks aren't going anywhere. They're going to become a bigger entity and play a bigger role in society as time goes by. So the people that are really going to matter in terms of making sure that this works for other people, our youth, their kids, they're the ones that are going to actually have the big impact on being able to change this and understand it at a better level then people that are our age it. It starts with the education of the things that we're talking about, understanding how this can go wrong, understanding the errors so you're educated on that and you can avoid that. In many ways, I think what people like me and a lot of people that are similar to me, we just try to put bandaids on this because we're too far along on the road. And youth have the ability to not just put bandaids on things, they can actually fix things, I think. | Yeah. Yeah. I think so too. And they're so creative, the kids that ... All kids really, but the kids we're working with are just phenomenal. And once they get into it, they design such amazing things to help people. They're really interested in designing robots to help and robots for social good. So they're really understanding this stuff. And even if the goal is not for them to go be computer scientists or to go build these, but to have that fundamental understanding so they can be critical consumers, so they stop and say, wait, this is wrong. I need to say something. That's what we're hoping, obviously. I mean, we're not going to follow them, but we're hoping that what we do has a little bit of that impact anyway.\"]",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything? | Yeah. In my work, not really. Not in an AI/ML way, one of the things we do is we'll set up auto video recording when kids are reading with a robot in the home. And so that was when we spent a lot of time thinking about and working with families and what's going to make them comfortable. And most of them were pretty fine with it. They just wanted it to be really, really clear when recording was happening. So it's actually impacts the choice of robots that I use. So the Misty robot I have has a little LED on its advisor that we can make really bright. And so it's there because when it's video recording, and it actually has a nice scene, but it's also really good indicator so that we train families to know that if you see that light, it's video recording. And we also train the families to know exactly how to shut that off in a one-touch thing. There's a spot on the robot that if you touch it there, the video recording will shut down.We also give them free license if they don't want to do that and just want to snap the whole thing off, that's okay too. So it's not in an AI/ML way, but in a privacy ethics issue, that's probably the number one thing I think we run into with my work.\"]",
          "[\"I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that. | Oh, I think it's super important. And I think that should be right with lesson number one. I mean, especially with machine learning, because again machine learning is algorithms that are trained on data and the data has to come from somewhere. And so there's the ethical questions around where does the data come from? And there are ethical questions of when someone's going to use this algorithm, this model. And so where are they going to use it? Who's going to use it? What are they going to use it for? So I think those are really important just to talk about early, I would say.\"]",
          "[\"I think it's going to be a lot easier than we think just because they're so ingrained in the technology itself. My daughter is just about to turn one and she can already swipe on a tablet because she's watches her brother swipe on a tablet. And I think just those simple movement to think, they're going to be so more used to it than we are getting used to it. I don't think it'll be that much of a stretch just relating it to the devices they already use or have , I mean, even Alexa is a type of AI, because it does learn things about you, it learns to pick up your voice and your inflections and specific words and what you like. So, I mean just being able to talk about the everyday devices that they use is going to be easy for them. | Yeah, I mean, I just have to be broad as close to them as they can like little things like, hey, you can't turn on this device in your sister's room and listen, because there's a privacy implication there. Just don't make it about huge asylum impact. It's got to be close ecosystems to themselves.\"]",
          "['Yeah. I think at a high level, my generation, it was always, for your education, you really need to understand reading and writing and arithmetic, math. I think to add to that, now you have to understand computational thinking. You have to understand just some basics of how computers work, things like that, just basic stuff, because you just have to. You probably grew up in a world that already had the internet, that already had things like GPS, that already had smartphone, or something close to it. I didn\\'t. My kids, they think I [inaudible 00:19:33] prehistoric times or something. It\\'s like cave man, how did you get anything done? But now, everybody that\\'s being born is born into a world where computing is everywhere. It\\'s not just reading and writing, and arithmetic. It\\'s reading, writing, arithmetic, and computational thinking, understanding how stuff works. In the spirit of that, you have to understand, I think at least conceptually, what AI is, and what it is not presently. We\\'re in no danger of robots taking over the world anytime soon. | I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic.']",
          "[\"Yeah. We have thought of that, integrating Snapchat filters or something like that. Something that they would be interested in. | Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess.\"]",
          "[\"I mean, just pull anything out. I mean, a phone. This Google Home I have here, AI is everywhere. There is not a thing... I'm sure there's an AI processing algorithm in my camera right now that is doing something with my face. Any browser, your email. I mean, it's everywhere. Getting them to connect that training data and all this data that AI uses to continually teach itself and learn is coming from you and it's coming from everywhere. That might be a little scary for them to think about. But I think it's important to know.\"]",
          "[\"Oh, demographically. I think I did actually, the first thought was people who are pretty good at programming and math probably, and who are interested in creating really advanced technology that's very smart. I can think about humans and demographic. I don't actually really have a thought of a specific group who design those. I would say just people who are good at programming and math or either one of those. | Actually, I was surprised. I remember when I learned programming, I think I started learning programming after I went to college. The first year freshman, I feel like currently kids, they learn. They actually have the channels to interact with different technologies really early. They probably have a Google home or Siri on their mom or dad's phone or like their iPad or so. So I feel they're exposed to those advanced technologies, including machine learning or artificial intelligence a lot. And I feel like, I'm not sure, I know high school kids definitely learn programming. I'm not totally sure about middle school kids, but I would say even if it's not in school, they still have a lot of chance to interact with technologies. Not necessarily to learn how it works, but more just to get to know it. And I currently don't see, at least I haven't really thought about the harm of they learning machine learning at a really young age. There might be [inaudible 00:17:12] issues. I'm not sure, but I think they might trust more than we do. I don't really trust the Google, not trust, it's more like, I just don't feel very comfortable having Google home listening to my voice, but they probably got the young generation because they interacted with technologies much earlier than we did. So that might have built the trust in machine learning or artificial intelligence.\"]",
          "['Oh yeah. Well, so for the most part, I have a relatively Wisconsin accent. And so the models that we used were... we tried to do was out of Carnegie Mellon. I forget the name of the program that did it, but it was one that could run on board. And that did relatively standard US dialect, was generally really successful. The student who was running that was from India, though, and her accent was completely inaccessible to the robot. And then she worked with another student who was Korean and also really struggled with her accent as well. So between the two of them, they were just really frustrated trying to get... And we were just trying to set up simple keyword kind of things like, \"Hey, Alexa,\" kind of stuff. We were just trying to do simple keywords with the robot, and it got nowhere. So that was the Carnegie Mellon one. And then we\\'ve been a little bit more successful. I haven\\'t used it, but the folks I worked with at Wisconsin who followed up with some of this, they did add some keywords to our last field-based one over the summer. And I don\\'t know if they tested it much as far as accents go. I know one of the researchers on the team is Turkish, and she didn\\'t seem to have much trouble. But again, we really constrained the word list for that. And the cloud-based ones I think are a little bit better now than the ones we were... Sync, that\\'s the CMU one that we were using. So I think they\\'re a little bit better now. And so that\\'s one that has been starting to get cooked into the robots I have. I just haven\\'t sent them out in the field doing that yet, but just simple, yes, no kind of stuff seems to work, and that hasn\\'t... We haven\\'t seen any evidence that that level has been impacted by any accent or dialects.  | Yeah. I mean, it\\'s essentially... So it has to appear intelligent, I think, right? So I don\\'t know if it has to actually be intelligent. And so that\\'s where I\\'d classify some of the things that I do that are... If you really dig down into it, it would probably be classified in algorithms, right? It\\'s an algorithm. So I did a book selection algorithm. But I think it\\'s important to think about the user\\'s perspective on these things. And so I have a pretty broad idea about AI in that if it appears intelligent, if it comes across as intelligent... I mean, maybe if it even wasn\\'t the intent that it comes across as intelligent, but it appears that way, I think we have to treat it as AI because of the impact it would have on the person that it\\'s interacting with, right? So my book selection algorithm is not complicated at all. It\\'s essentially like a sorting algorithm and we add in a couple of inputs about what the kids\\' book preferences are like, their reading skill level, the amount of time they read, that kind of stuff. And then we tag books for all those features, and we just make a priority queue out of it. Really simple, early CS stuff. But to the kids, it came across as intelligent. And I think that\\'s the key factor, that when we interviewed them afterwards, kids felt that the robot was paying attention to them and that the suggestions the robot made for books were personal, that they were about them. And so that to me is now where you\\'re in artificial intelligence and you really have to then take that seriously, because if the person believes that they\\'re working with an intelligent machine, then you have to treat that carefully.']",
          "['I can only steal ideas for that because I usually don\\'t work with kids that young. I try to avoid compulsory schooling age stuff because I\\'m just like there\\'s so much more to consider and so many more standards you have to meet, which is why summer camps are good because you can do whatever you want within reason. But there is this book. I wish this had the fun cover version of it. It\\'s Living in data by Jer Thorp. And he\\'s an installation artist. It\\'s fascinating fucking reading. You should really find a recording of it if he has any book talks. It\\'s the same stuff in the book. But he says it and it has pictures.He\\'s an installation artist. Him and his group of his grad students, one thing they did is they did this installation in New York, right on a city street. And it\\'s this heart. You look at it, it\\'s a heart. But it\\'s all these pipes that are taller or shorter or whatever. It makes this heart shape. And people are like, \"Cool, we\\'ll take selfies in front of it.\" And they go look at it closer. And it\\'s a bar chart. And it labels on there where the population in New York has come from, from around the world.So then he sees people taking selfies with the bar, finding where their family is from. And then they\\'ve caught two people having weddings in front of it. And he\\'s like, \"This is the world\\'s only bar chart that has also been a wedding venue.\" And it\\'s just fascinating. He has all these different ways of getting people to experience data differently as a way to get out of their head and think about things in a really cool way. So I asked what his views on data ethics were. He said, \"Well, I have a 300 page answer to it. It\\'s called the book.\" So I had to get the book and fuck, there was another one. There is this fun activity that I saw someone do and post results of where they had real young kids. I can\\'t remember what age he said, but it had them draw what they thought Alexa looks like. Just take this disembodied voice that we as adults might take for granted and say, \"Well, kids, how do you think this looks?\" Something else that Jer Thorp did is working with kids, I just remembered this, is he had the students draw on big printouts of maps of their city, things that were important to them or things that they had noticed, like where are broken sidewalks? Where is the good place to get food? All these things that have meaning to the kids as they\\'ve experienced their own city. And they take this giant kid map and overlay it on old voting things. And you can see, well, here\\'s the red lining that happened. Here\\'s the history that you can see in the voting record aligning from forever ago with what you could see in your day to day life. And it was just this fascinating moment of overlaying and you go, \"Oh fuck.\" You don\\'t know that that\\'s what you\\'re drawing, but it\\'s what you\\'re drawing. And then they overlay it. It\\'s like, well, there you go. This data has all a long history and you can see it kids. So that I thought was really cool. ']",
          "[\"That's awesome. I mean we need more people to understand coding in general. So I think it's great that you do that work. Has any of the work you've done with those type of groups ever bridged into AI before or machine learning? | Yes, it's a data ethics course. We talk about AI a fuck ton. So besides camps, teaching people how to code, I'm a PA right now with YJ Kim here at the Wisconsin Center for Ed Research and I know David Shaffer. And through him I've heard about goal like everyone else in this community. So I hear about Clemson all the time. But where was I going? So I have a PA ship here. I've TAed here, TA for code and power a lot with Dr. Royston and that's kind of your classic critical theory approach to AI and a lot of... but like meritocracy, Google image search results for black women showing images of gorillas, those kinds of cases. Yeah, it's kind of fucked up. So talk about how code and power get gets embedded. And it's very critical around institutions and getting students to think about that stuff and learn about implicit biases and things like that. We actually have them take the implicit bias test and then think about the limitations of that test and try and reason about what data says. But-\"]",
          "['No, I actually feel that because I recently joined Golnaz\\'s lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations? | Yeah, I think there\\'s a couple things with that in terms of, if I\\'m thinking about helping humans, I think it\\'s more about something that\\'s really cool about how humans work is that a lot of times they find easy small problems and they figure out how to fix it. And then it turns out that problem is a really big problem because it affects a lot of people. And so AI systems create this interesting idea where in my opinion, AI systems have a very low barrier of entry or at least including machine learning, you can use machine learning stuff if you can take two weekend classes and I think get pretty up to speed with it if you have the right education and teaching resources behind it. And the key to that is that, you can start solving really small problems in your life really quickly with really simple systems. And eventually what that does is it enables people on a micro level to make their life a bit easier, but also understand technology from this smaller perspective. And then I have a second way as well, but in my head there\\'s a small piece there where it\\'s the toolbox for people to get involved in computing stuff, I think actually lowers with that. So it helps people make things easier. Because I would say back in the day, coding on C and Fortran and stuff was awful and computers were millions of dollars and now it\\'s, \"oh you can do very easy things with very little.\" So that\\'s that The second one I have is, more of on the large scale meta-perspective is I think one of the reasons I also got into AI and find it really interesting is I like to see it from a perspective like aiding humans in reducing workload and replacing a human systems within a group and not having humans pick up that work. So more of the idea of freeing up humans to do things that matter to them, but also freeing up humans to work on stuff that they find important. I think that\\'s the 10 years from now potential is, I think in terms of workload and work future it has the most potential to impact and improve people.']",
          "[\"But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam. | Yeah I mean that is you just gave us a lot of good information Thank you so, can you think of any ways to help us connect these topics like thinking about data machine learning AI to their everyday lives and make it meaningful for them.\"]",
          "[\"No, I actually feel that because I recently joined Golnaz's lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations? | Yeah, so I think piggybacking off of my last answer, I think the most optimistic thing is work future and enabling a work future. But that is also simultaneously the worst aspect of it if done incorrectly. So generally speaking, I like the idea of relieving human workload, but I think the larger concern is in actual phasing out of them from a work perspective. So in other words, an over-reliance on a machine system for the sake of using a machine system as opposed to a human I think is a main issue. And I think ultimately it affects lower level, blue collar workers more and it affects a larger population that has received less expertise, education, throughout their lifestyle, throughout their entire life. So for instance, a PhD individual would probably not have a larger issue with that, given that the specialized area of knowledge that they received, but someone who has a more general work area where the knowledge application is not as deep, but it's a task based knowledge. I think that's the person that has the greatest ability to impact. And I think ultimately it comes down to who we let use that system because there's easy ways to mitigate that, creating systems where for instance, there's the idea of creating head to access for robots, whereas the utilization of a robotic employee doesn't allow you to just skirt the idea of paying for that employee. It still should be something that CRE, it is something you use in generating revenues, therefore it's something that needs to also be accounted for. So I think that's the main issue, the work future I want is also a good and a bad thing, depending on who's in charge of it.\"]",
          "[\"Sure. Yeah. Currently an assistant professor in computer science and learning sciences at University of Illinois, Chicago. The work I do is in social robots for educational purposes that are designed as learning companions, where they work with or around kids to help them learn in lots of different scenarios. My training with it is... My PhD's in learning sciences from University of Wisconsin, but I have a PhD minor in computer science where I did a lot of human robot interaction work. So I'm trained on the HR, the design of the robots and the interactions, and on learning and learning theory. And so the combination of those two is to design learning interactions for kids to really enhance their learning experiences.\"]",
          "[\"But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam.\"]",
          "[\"I think it's wildly important. I don't think that programming should be something you go to college for. I think this is something that everybody should learn. It literally should just be like typing class. And I think AI should be... I don't know. Because some people, I don't know, I don't want to make people go that deep into something that they might not be that interested in. You know, I think everybody should learn programming because I think it'll become somewhat of a basic skill too. But actually, you know what, we got AI that are starting to program now and you got the discussion that manual programming is going to become obsolete because AI can program everything. So, that just kind of speaks to the complexity of what AI represents. I think it should be included more in the curriculum. I'll meet in the middle. I think it should be included more in the curriculum. I might not put it as an entire class. However, I do think that children should be given many more opportunities to learn about these technical things than they currently are. I think we should give them more to challenge them with. I think kids can do a lot more than we think they can. | You know there's different ways to like introduce these things like you can introduce you know the concepts of AI to elementary school or elementary aged children, and then you know, just as they grow in the middle school and high school.  Those concepts can be expanded upon and elaborated, and you can get into those details.  And the specifics like when I substitute teacher who there I can't remember her name, I think it was very she has shared like technology class and she was a you know it's like physical programming, where you have the little robot that you guys along the line you know so like you know, an elementary school, you can-Yeah, you can talk to him about what.  You know what these.  Like how basically how these devices function, you know, probably not get super in depth with it, but like you know, once you get a middle school, you can start talking to them about  more advanced aspects, like the you know physical programming that my name is Barry was trying to show them, and you know, in the in the high school, you can really kind of get down to the nitty gritty.  So I think I think it should be taught at all ages, but obviously it should just follow the curve of development\"]",
          "[\"Yeah, absolutely. I'm a researcher at Microsoft Research in Redmond. I've been here for about five years. And before that I was a PhD student at University of Wisconsin studying computer science, probably, but I was working kind of on human robot interaction and doing a lot that was kind of based on understanding social gaze behavior of people and how to design social gaze mechanisms for robots. And so at Microsoft Research, I'm a research scientist and I'm kind of in a group that focuses on broad problems in AI. My focus is more on still on interaction. So I work in an area that my group calls situated interaction or situated intelligence, where we're very interested in understanding how to design AI systems that can interact with people in kind of everyday settings. So it encompasses human robot interaction. I've done stuff with human virtual agent interaction, stuff in mixed reality. But the idea is how do you combine multiple AI technologies and actually build real systems that can interact with people? So there's an element of studying interaction and studying people and studying social science and then studying the AI and how to actually build systems that can interact with people effectively.\"]",
          "['I met a guy in person, he used to help make movies. He\\'s like, \"yeah, my job used to take a team. And now it\\'s just one dude at one program.\" And so he\\'s changing careers into learning how to code, and so there\\'s that. Consumers might not be aware that they\\'re interacting with AI when they\\'re shopping online, except it might not be aware to the extent that they are and that can have harms to principles like consent, being aware enough of what\\'s going on to be able to fully make a decision and not doing that you\\'re interacting with AI or that visual. Invisible changes are happening on the screen behind the scenes. It might be bad depending on that context. And it\\'s fucking hard to regulate AI.']",
          "[\"No, I actually feel that because I recently joined Golnaz's lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations? | Yeah. I mean, we've actually already seen that with some of like there's AI robots that Ford uses in their factories and stuff. So we've already seen that happening and I'm sure it's going to grow from here. So same kind of question, but in reverse. So how do advances in AI or machine learning harm us and are there any particular populations that it harms? Oh, oh no. Okay. All right. I'm going to go ahead and ask the question again just in case. So how do advances in AI or machine learning harm us, harm humans and who in particular is it harming?\"]",
          "[\"Sure. Basically, I have not been officially in the working world yet. I did my undergrad in psychology in Clemson and graduated December of 2018. Then my current grad program, which is in human-centered computing, did not start until the fall. So, I was a substitute teacher at a middle school for that whole semester. And then over the summer, I just kind of take a break, went to Europe, all that good stuff. Then I started in the fall and I've been a research assistant ever since, working on human-AI teaming projects. Specifically, my research is on team cognition in human-AI teams.  I don't know, we can get more into the weeds with that, but the only other professional work experience I have is working at a light during internship my junior year of undergrad. And they did benefits management stuff. To be honest with you, I know this is a bad AI. Basically, their entire company could go away because they... I don't know, my job, it was just so much data management and it's crazy because they hire people with four-year degrees to do these jobs that you could basically do a Python script for. | That's interesting. How did you actually... I know you said you had a psychology degree from Clemson and then you kind of moved into the human-centered computing program. What made you interested in that? How did you become interested in working with AI and machine learning?\"]",
          "['Well, I mean, so there\\'s definitely documentations of bias that are in there for essentially, right, the people who are designing and making these things cook in their own biases into there. And so if 98% of the data that we trained on are a bunch of white males, then anybody not in that category is going to be on the margins of the data set. And that means they have not been properly trained into that data. And so we\\'re going to be misclassified at a much higher rate. So I mean, I think there\\'s good efforts that are out there now to actually really think about representation and training sets. I don\\'t know how that gets reported. I haven\\'t seen anyone who has openly reported their training statistics that way, that, \"Here\\'s how we balance gender and all the other demographic things that would go into impacting these outcomes.\" So in a healthcare setting, I mean, it\\'s happened before machine learning, is doing these diagnostic things, the reports of people where there\\'s no black people represented in medical textbooks, right? That\\'s not about a machine learning thing. That\\'s about a human learning thing, where if you\\'ve never actually practiced or considered examining a person\\'s body who\\'s different than cliche US Western standards, then you\\'re going to make errors. And so I think we\\'re making the same mistake that we ignored before machine learning. We\\'re making the same mistake. And yeah, I think right now, the biggest thing for me would be transparency on these things. How are these systems trained? What do we know about those data sets? So yeah, they\\'re much more likely to impact marginalized communities because those are going to be the data that\\'s on the margins in the training sets. But I do think there\\'s repercussions for everyone as well. And that\\'s part of my worry too, that definitely some populations are going to be overrepresented in how they\\'re negatively impacted. But I think there\\'s negative impacts across the board. And so again, emotion recognition kind of things, I think those are problematic. Medical examination kind of things, I think are really problematic. So I\\'m skeptical across the board, but yeah. [crosstalk 00:28:37]- | Yeah. I love that, Joe. And I can see that in your work, the way you\\'ve described it. That\\'s great. Okay. So let\\'s switch. So talking a little bit about why we\\'re doing this interview, right? We\\'re taking what people are saying and trying to apply it for learning experiences for young people. So what are your thoughts just generally about elementary school, middle school-age kids learning about either AI machine learning and the social and ethical impacts or both? [crosstalk 00:29:56] ideas around that, what they should learn, what\\'s important for them to know? Can they [crosstalk 00:30:00] in those issues']",
          "['Yeah, a weird one. I was looking at job applications recently. I was just on LinkedIn, saw a few and this is a very interesting one. There\\'s a weird and this is where my dissertation\\'s about acceptance and over-reliance and things like that. And there\\'s a weird over-reliance for some of these things that creates someone missing the point almost. So I was looking at data science positions and they were, \"oh yeah, so data science work, you need to be able to do machine learning stuff for data science and predictive analysis.\" And so for that, they\\'re something called random force generation, which is a machine learning technique where you just put a bunch of data in and then say what you want to predict. And then it predicts the stuff with a bunch of complicated math and that\\'s all fine and dandy and it works out great. But you can also do that with basic statistical analysis. So it\\'s almost one of the concerns I run into is, people use it when they don\\'t need to like, don\\'t use a tool that isn\\'t needed or don\\'t use a tool that\\'s too much for the issue almost, which is an issue I\\'ve seen. And something I run into as a larger issue is over designing and over implementing some of that tech. | No, I actually feel that because I recently joined Golnaz\\'s lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations?']",
          "['I think it\\'s important, but I think it\\'s important from two perspectives. The thing I think it\\'s more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it\\'s very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society\\'s going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there\\'s it\\'s good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it\\'s a two front benefit from it. | Yeah. I think my full pathâ€¦ My mother\\'s an educational therapist who works at a high school/middle school. So I\\'ve talked about this a fair bit. I always think if I start with general coding, it starts in second to third grade and I think you can start introducing levels of machine intelligence as early as fifth grade, because machine intelligence doesn\\'t have to be demonstrated from a coding perspective. It can be demonstrated from just what are these things? What do they do? What do they look like? And the benefit is it\\'s an interesting part about it. There\\'s so many really good abstractions of coding principles. I said earlier, I said, \"oh a big coding thing for machine intelligence is something called a random forest. And a random forest is called that because it\\'s a bunch of binary decision trees grouped together and so they call it a forest.\" Computing and AI and all those things are already filled with a very large amount of abstract representations of what they are. And so translating that and it\\'s because a lot of us aren\\'t very smart and so it makes it easier to teach us, but there are also things that make it easier to lower the barrier of entry for a lot of these concepts, like talking about a decision tree from the perspective of an actual tree makes it easier for people to understand. So I think you can at least get that high level interest in there at fifth grade, or maybe even slightly earlier with fourth grade, just from a concept perspective under the assumption that there\\'s already some computing knowledge before that. And then, I think from a coding perspective, getting in there at late middle school is always my ideal because then it\\'s, \"oh this is something that you can pursue further in high school, but also you are smart enough right now to understand that.\" And that\\'s because I also think that as someone who did a lot of coding, a big issue with learning to code complex things is, for me it was a lot tied to my math maturity. So I found that as I gained greater math maturity, I also gained a lot better ability just to understand coding quickly. And so in terms of hitting the hardcore coding concepts of it, I wouldn\\'t want to do it before that math maturity, is at least somewhere. I felt like I started to get that in late middle school. So I think that\\'s the time to do that.']",
          "[\"It depends on the context. So if it's a Google image search engine, many kids say, yeah, I'm not represented here, but that's how the world is. The consequence isn't huge. I search for computer science professor and I don't see myself as a African American woman. Okay, that hurts. But I don't think Google should mess with that. Then when it comes to hiring algorithms, yeah, they see that as problematic. So the context really matters and the effect of the consequence as they see it matters to them.\"]",
          "[\"Yeah, that's fascinating. And I think that's a really good example of the power of AI and how it can be so helpful, especially, like you're taking the person's strengths, you're taking the AI strengths and bringing them together to make this really functional system even more functional. So what are some examples then kind of on the opposite end of the spectrum of how potentially AI or machine learning can harm us? And who in particular do you think it harms? | Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight.\"]",
          "[\"Yeah. I mean, it's essentially... So it has to appear intelligent, I think, right? So I don't know if it has to actually be intelligent. And so that's where I'd classify some of the things that I do that are... If you really dig down into it, it would probably be classified in algorithms, right? It's an algorithm. So I did a book selection algorithm. But I think it's important to think about the user's perspective on these things. And so I have a pretty broad idea about AI in that if it appears intelligent, if it comes across as intelligent... I mean, maybe if it even wasn't the intent that it comes across as intelligent, but it appears that way, I think we have to treat it as AI because of the impact it would have on the person that it's interacting with, right? So my book selection algorithm is not complicated at all. It's essentially like a sorting algorithm and we add in a couple of inputs about what the kids' book preferences are like, their reading skill level, the amount of time they read, that kind of stuff. And then we tag books for all those features, and we just make a priority queue out of it. Really simple, early CS stuff. But to the kids, it came across as intelligent. And I think that's the key factor, that when we interviewed them afterwards, kids felt that the robot was paying attention to them and that the suggestions the robot made for books were personal, that they were about them. And so that to me is now where you're in artificial intelligence and you really have to then take that seriously, because if the person believes that they're working with an intelligent machine, then you have to treat that carefully.\"]",
          "[\"It's the same bucket. If it's a random person on the street, one, why the fuck are they talking to you about it, machine learning. Usually when we have that discussion, the technical discussion of AI versus machine learning, machine learning might be seen as a sub-case of AI a broad category and machine learning being a set of particular techniques for how we optimize AI. So having the machine learn from data, again, a big metaphor about comparing the human brain to computers, which is a bad thing to do for your own wellbeing. We aren't machines. Machines don't think like us. We call it... this is from Ellen Ullman's Close to the Machine, fantastic book. It's a memoir. I have it with me. And she talks about how we call it the machine. We call it a memory, but it's not right. So one, I don't like the name machine learning when people use it too much to compare to how babies. I'm like no, fuck, that's not it. But a way of optimizing around data, and so it's just really cool statistics. And it's really close to data science, where you're like you have data, but what meaning can you get out of this for a social reason? What's blind on this? I don't really care about the difference between, but then AI a more general label. If I have a bunch of if/then statements. Is that AI? Well, yeah, it used to be. We were trying to solve like chess originally or checkers. We had some pretty simple sequence of steps because that's how we thought the human brain worked or at least that area thought things worked. So I don't know. I use them broadly because I don't think the specifics matter. It's when we get to the ethics side of things. | No that's great, thank you. So this might go more into some of your ethics background, but how do some advances in AI or machine learning help humans?\"]",
          "[\"Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas? | So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful.\"]",
          "[\"I think it's important, but I think it's important from two perspectives. The thing I think it's more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it's very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society's going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there's it's good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it's a two front benefit from it. | Yeah. That makes sense. I am still learning how to code. So I'll get there one day.\"]",
          "[\"Are you talking about just learning AI and ML in general or the ethical issues? | Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess.\"]",
          "['AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "[\"Yeah, I don't. It's been a while. I don't know...do something with tick tock? | Are you talking about just learning AI and ML in general or the ethical issues?\"]",
          "['Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences.']",
          "[\"I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that. | Yeah. Just teach the basics. That makes a lot of sense. So what are your thoughts about youth learning or youth being introduced to some of the ethical or social issues around AI and machine learning?\"]",
          "[\"That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning? | Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "[\"I think it's important, but I think it's important from two perspectives. The thing I think it's more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it's very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society's going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there's it's good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it's a two front benefit from it.\"]",
          "[\"Okay, great. And then, so related to what we were talking about, what about ethical or social issues that we discussed or even beyond that maybe we haven't discussed, how would you, what are your thoughts about youth learning about that in conjunction with AI and ML? | Yeah, absolutely. Kids are getting Chromebooks in second grade now. The schools are distributing. We just talked about our children, interacting with Alexa that's a intelligent system, so.\"]",
          "['Sure. Yeah. Currently an assistant professor in computer science and learning sciences at University of Illinois, Chicago. The work I do is in social robots for educational purposes that are designed as learning companions, where they work with or around kids to help them learn in lots of different scenarios. My training with it is... My PhD\\'s in learning sciences from University of Wisconsin, but I have a PhD minor in computer science where I did a lot of human robot interaction work. So I\\'m trained on the HR, the design of the robots and the interactions, and on learning and learning theory. And so the combination of those two is to design learning interactions for kids to really enhance their learning experiences. | A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on.']",
          "[\"I mean, the aspect is so, AI is only as good as the data we give it, things like that. And I think there's a lot of discussion and research right now about a lot of AI being really biased to majorities because that's the data that they have access to. And so there might not be the ability to accurately assess or communicate with minority populations. And it could, I think there's some discussion about it kind of reinforcing biases and stereotypes because it's just operating off of the set of data that it's given. | We were talking about it a lot in recognition software and things like that, where the data sets that it's usually using to recognize and communicate with people is generally very Western white male and wrongly classifies people and communicates with them as if they were that group, which makes it very difficult for people to not only work but connect to it and get the benefits as so much to some other populations.\"]",
          "[\"I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "['Those are even still around. Yeah. Those are really fun. And that\\'s-by high school coding class we did Java programming for Mindstorms and it was really interesting, but there\\'s so much more you could do with it after that. So if you had kids start with it and grow up with it, the potential they have there is pretty cool I think. | Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences.']",
          "[\"I think to not be fearful of it, but to understand at its core what it is and how it works, so that there's no fear, for one thing. And then, when they're learning that, that they can also, I think, open up their imaginations to what it could be used for, for the benefit of global society because I think what's been happening with technology is, it used to be inaccessible to people who had great ideas, and now with things like cloud computing and tons of open source software, some very specialized around AI, if you have a good idea, you can try some things out. If you get everybody's brain in the game, we have a lot of serious problems that we need to address, and I think young people need to understand that no one's going to solve those problems, and you need to think about... I should say, other people are not always going to solve the problem. You have the ability to solve some of these problems or to contribute to the knowledge. We have to start addressing these difficult global challenges, and we have to stop, I think, focusing so much on things like TikTok. Just think about all the technology that went into building TikTok. It's like, I'm sure there are valid and great use cases for TikTok, but I would say by and large, from my small sample, this is kind of ridiculous. It's entertaining, and you get that dopamine hit. I understand that, but I just think, of these technologists that develop things like TikTok or Facebook, I'm like, we probably could have ended food distribution problems or just poverty, environmental concerns, but instead, we have TikTok. It's kind of depressing a little bit. I'm hoping that you get to the youth and have them understand this. It can only be a good thing, I think.\"]",
          "['So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]-']",
          "['So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]- | Yes, it\\'s a data ethics course. We talk about AI a fuck ton. So besides camps, teaching people how to code, I\\'m a PA right now with YJ Kim here at the Wisconsin Center for Ed Research and I know David Shaffer. And through him I\\'ve heard about goal like everyone else in this community. So I hear about Clemson all the time. But where was I going? So I have a PA ship here. I\\'ve TAed here, TA for code and power a lot with Dr. Royston and that\\'s kind of your classic critical theory approach to AI and a lot of... but like meritocracy, Google image search results for black women showing images of gorillas, those kinds of cases. Yeah, it\\'s kind of fucked up. So talk about how code and power get gets embedded. And it\\'s very critical around institutions and getting students to think about that stuff and learn about implicit biases and things like that. We actually have them take the implicit bias test and then think about the limitations of that test and try and reason about what data says. But-']",
          "[\"Yeah, I mean, I just have to be broad as close to them as they can like little things like, hey, you can't turn on this device in your sister's room and listen, because there's a privacy implication there. Just don't make it about huge asylum impact. It's got to be close ecosystems to themselves.\"]",
          "['I think ultimately AI systems themselves are tools. So, an ethical dilemma existing in AI is simply the reflection of that ethical dilemma existing in a person. What I think AI makes it easier is AI is a tool that makes a lot of things easier and I think that also means AI could be a tool that makes being unethical a lot easier. There\\'s a lot of things you can do with AI systems, with them being black boxes and completely hidden. And also then just being a machine system, if I think of a really modern example, Google is extremely adamant that they will never share the AI algorithm side that determines whether or not a video on YouTube should be monetized or demonetized and the reason they say they don\\'t want to do that is because of bad actors. They\\'re like, \"oh, if we tell you that, then they\\'re a bad actor come in and do something,\" but the issue with this, of them coming in and so saying like, \"oh, we won\\'t do this,\" is there\\'s no oversight now to know if that\\'s discriminatory or not, we can\\'t even tell. And then we say like, \"well shouldn\\'t you provide something?\" No, we can\\'t even provide the oversight because that\\'s too much information, and you be like, \"you just have to trust us.\" And I think the challenge right now is that with AI systems, people are way too willing to give them that trust. If a company comes out and says, \"oh, we can\\'t divulge the AI secrets,\" because they\\'ll be, \"oh they\\'re just trade secrets, you can\\'t,\" it\\'s like a normal trade secret, but it\\'s a trade secret that heavily impacts a lot of other people. And it doesn\\'t have that oversight right now. So, I think that\\'s where the complication of ethics comes in is that it makes it easier especially with the current state of it to do something unethical because there\\'s that lack of oversight coming from both a computational side and a social side where people are just like, \"oh, it\\'s an AI system. We can just trust it,\" when in reality, it allows the bad motivations of individuals to almost be hidden because they\\'re exercising those bad motivations through a system. | I think it\\'s important, but I think it\\'s important from two perspectives. The thing I think it\\'s more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it\\'s very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society\\'s going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there\\'s it\\'s good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it\\'s a two front benefit from it.']",
          "[\"Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right? | Oh yeah. My son figured out, I guess about a month ago, how to actually tell Alexa how to do things. And now it's hilarious because I have the app on my phone where I can see all the devices and I can see what Alexa, hears him saying.\"]",
          "[\"I have to think. And I can maybe send you some links, but I know there's research going on, for example, because Microsoft owns Minecraft. There's research going on. I think they've released it as... But there's really research going on like how do you do reinforcement learning in Minecraft? How do you use Minecraft to create intelligent agents? And I think they've released tools and things that people can explore that on their own with Minecraft. So that's one thing. There's another project that comes to mind. I think it's called Make Code. I don't know. Is that a thing? Let me look really quick. I thought that has some element of... Oh this also has something to do with Minecraft, I guess. Microsoft free online learn to code platform. Yeah. So let me send you this link. | So this is another kind of project that I remember seeing out of Microsoft Research. And it's about coding in general, I think and computer science, but I think it gets into some things that have some AI. So yeah, I think that those are the places I'd start.\"]",
          "['Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.']",
          "['Yes, venturing indicates well is a different story. So what age do you think students or young people should start learning about AI and machine learning? | AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "['Yeah. In that sense, that I think, I would fall in line with a little bit more traditional perspective, because machine learning doesn\\'t necessarily always have to... It\\'s not is a user-facing part of what\\'s happening. So it\\'s essentially how to take a bunch of inputs, teaching a machine how to interpret those inputs, and to organize, categorize, or plan actions based on those inputs. So it\\'s different levels of black boxiness that go along with it. But yeah, it\\'s essentially the training machines to have a space in between input and output that is nonlinear, I guess. So I mean, the traditional perspective is you have this set of data that\\'s coded with these sets. And so you train that way. And then a new set of data that isn\\'t coded, the machine should take what it learned from this first one, apply it there, and come up with the same codes. Those codes could then be actions to do. Those codes could be categories. Those codes could be things like emotions, right? So, \"Here\\'s 10,000 pictures of people who look angry. Here\\'s 10,000 more. Which of these are angry?\" kind of thing. So that, I think. And now that I\\'m thinking about it, that\\'s almost sneaky or more problematic sometimes because you don\\'t necessarily always have the user interacting with it while you\\'re developing these things and testing them. And it can be to such a scale sometimes that the errors and the problems in there are easy to miss, right? That, \"Hey, we got 99% accuracy,\" but that means if there\\'s 100,000 images in that set that you\\'re classifying on, 1% is actually a lot. And if that 1% impacts me and you\\'re just going to take this thing off the shelf that\\'s 99% accurate, and you\\'re going to take it off the shelf, and it\\'s going to make a medical diagnosis, and I get the 1% problem, that\\'s pretty impactful. So again, that\\'s one of the things I talk a lot with graduate students who are like, \"Oh, I\\'ll just grab the thing and we\\'ll just figure it out. It\\'ll tell us what to do.\" No, that\\'s not safe in a lot of the things that we\\'re doing. So, yeah. So I guess, I don\\'t know. I mean, that\\'s a too-long explanation of what machine learning is. ']",
          "[\"So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful. | Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.\"]",
          "['I can only steal ideas for that because I usually don\\'t work with kids that young. I try to avoid compulsory schooling age stuff because I\\'m just like there\\'s so much more to consider and so many more standards you have to meet, which is why summer camps are good because you can do whatever you want within reason. But there is this book. I wish this had the fun cover version of it. It\\'s Living in data by Jer Thorp. And he\\'s an installation artist. It\\'s fascinating fucking reading. You should really find a recording of it if he has any book talks. It\\'s the same stuff in the book. But he says it and it has pictures.He\\'s an installation artist. Him and his group of his grad students, one thing they did is they did this installation in New York, right on a city street. And it\\'s this heart. You look at it, it\\'s a heart. But it\\'s all these pipes that are taller or shorter or whatever. It makes this heart shape. And people are like, \"Cool, we\\'ll take selfies in front of it.\" And they go look at it closer. And it\\'s a bar chart. And it labels on there where the population in New York has come from, from around the world.So then he sees people taking selfies with the bar, finding where their family is from. And then they\\'ve caught two people having weddings in front of it. And he\\'s like, \"This is the world\\'s only bar chart that has also been a wedding venue.\" And it\\'s just fascinating. He has all these different ways of getting people to experience data differently as a way to get out of their head and think about things in a really cool way. So I asked what his views on data ethics were. He said, \"Well, I have a 300 page answer to it. It\\'s called the book.\" So I had to get the book and fuck, there was another one. There is this fun activity that I saw someone do and post results of where they had real young kids. I can\\'t remember what age he said, but it had them draw what they thought Alexa looks like. Just take this disembodied voice that we as adults might take for granted and say, \"Well, kids, how do you think this looks?\" Something else that Jer Thorp did is working with kids, I just remembered this, is he had the students draw on big printouts of maps of their city, things that were important to them or things that they had noticed, like where are broken sidewalks? Where is the good place to get food? All these things that have meaning to the kids as they\\'ve experienced their own city. And they take this giant kid map and overlay it on old voting things. And you can see, well, here\\'s the red lining that happened. Here\\'s the history that you can see in the voting record aligning from forever ago with what you could see in your day to day life. And it was just this fascinating moment of overlaying and you go, \"Oh fuck.\" You don\\'t know that that\\'s what you\\'re drawing, but it\\'s what you\\'re drawing. And then they overlay it. It\\'s like, well, there you go. This data has all a long history and you can see it kids. So that I thought was really cool.  | I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI. ']",
          "['I\\'m going to bring up... I had this already. Nope, that\\'s my writing sample. I have just been writing all this down for applications to places. And I\\'m at the point where if I write something down, I\\'m going to forget it immediately, which is the opposite problem because now I\\'m not constantly thinking about it. So AI or AI ethics is a bundle of ethical issues. And so thinking about populations depends on your entry point to thinking what AI ethics is. And so I\\'m just reading my notes, honestly. We can\\'t violate public trust, is kind of one thing. That comes up a lot in the discussions around autonomous vehicles, that we want to have autonomous vehicles for X, Y, Z reason. But in order for that to happen, the public kind of has to trust in the system. We have to agree. We have to know that it\\'s not going to run over black people more than white people. If it gets down to that moment, we have to know that it isn\\'t going to confuse us, not know that a cyclist is there or the actual case of where Uber hit a woman. I don\\'t know if it was Uber or not, but there was an autonomous vehicle that hit a woman because she jaywalked. Well, jaywalking should result in a fine, if that, not in death from a car that didn\\'t see you. The idea that the road is owned by cars is a relatively new one in human history. Roads were owned by the people walking on them. And so that one, public needs trust in it and that we don\\'t want to violate that trust. So in a way we can harm everyone when we release things too early in that sense or have things that create harms. Also, the people who are literally hit by the car, I think get harmed the most. Let\\'s see, there\\'s a lot of talk around misinformation being amplified on social media. And there\\'s also talk on certain things when you have newsfeeds, like Facebook, that put the things at the top that they think you want to see or that they want you to see. That\\'s in their benefit for you to see. Or TikTok, as fun as it is to find your own extremely niche set of friends on TikTok, and ridiculously fast, what\\'s getting left out? Who\\'s getting pushed down? And so there\\'s a lot of harms that come up when misinformation gets brought up to the top and there\\'s a lot of harms that get caused when certain communities are just completely pushed down and systematically given lower scores in the algorithm. And so there\\'s a lot of talk on TikTok around trans communities and people of color, people who don\\'t look pretty getting rated lower by the algorithm, so they\\'re not going to have as much of a viewership. And so when your money is tied to being a content creator, it sucks. There\\'s also the whole thing that if you\\'re queer and online, you are subject to harassment. You always have to have a few backup accounts because one of them is going to get blocked out because people are going to mass... people who don\\'t like you are going to mass report everything you do until you get flagged by the algorithm as bad and systematically have your account removed, even though you didn\\'t do anything wrong. That happens all the fucking times with trans content creators. There\\'s like all these, \"Hey, my thing got deleted again.\" And it\\'s just happens. So there\\'s a lot of algorithms that play a part in that. But also the algorithm isn\\'t separate from the system of humans interacting with it. So fuck, I talked a long list, when it\\'s applied in places like policing, compulsory education, medicine, et cetera. Those already have a lot of scrutiny on them, legal and public scrutiny on them. So it\\'s very important that we get AI right in those cases because we\\'re constantly looking at them for whatever reason. Reason might be that we have a lot of public scrutiny on policing and education because we want a fair and just society. And we see those institutions that are very important to the function of a fair and just society. So if we fuck up there at all, everyone pays attention. Work, AI changes the nature of work, and when the nature of work changes, some people are benefited and some people are completely displaced.']",
          "['Yeah I mean that is you just gave us a lot of good information Thank you so, can you think of any ways to help us connect these topics like thinking about data machine learning AI to their everyday lives and make it meaningful for them. | Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences.']",
          "[\"But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam. | Those are even still around. Yeah. Those are really fun. And that's-by high school coding class we did Java programming for Mindstorms and it was really interesting, but there's so much more you could do with it after that. So if you had kids start with it and grow up with it, the potential they have there is pretty cool I think.\"]",
          "['Yeah. And that also makes it to where it creates a mythos around it, that makes it really intimidating for some people like, \"is this new fancy amazing,\". Like that job application, I was like, \"yeah, it\\'s not hard to use, machine learning though.\" My brother learned it in a week and does it for his job, and he goes to his job and he tells his job, \"oh, I implement machine learning for this.\" And they\\'re just amazed by it. He learned that in two days. So it creates this weird thing where it is simultaneous, over-engineering the problem, but also intimidating new people from getting involved in it because it is considered magic. | No, I actually feel that because I recently joined Golnaz\\'s lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations?']",
          "[\"Geez. Hopefully they help everyone. I don't know. I mean, I just see it as, it's like any technology, I mean it has the potential to increase human productivity. It has the potential to increase human collaboration, creativity. I don't know. It's hard to answer because I think there's so many different kinds of technology that you could lump under AI and it's almost like who's helped by computers? | Awesome.  I'm kind of switching gears a little bit, what are your thoughts about youth learning about AI and machine learning?\"]",
          "[\"Yeah, there's a really thought provoking... Hang on, just let me see. It's a great thought provoking website. What is it called? Oh yeah. moralmachine.net. If you go to moralmachine.net and then click the judge link, what this is, is this research that's done by MIT and a handful of other universities, and it asks you some interesting questions because everybody's talking about self-driving cars, and how they're going to happen. They're already happening. It's going to happen, and the reason behind it is, humans are not good at driving. People always think they're good at driving, but they're not. So, if we get the self-driving stuff done correctly, even if it's not entirely self-driving, even if you're sitting in the driver's seat and have to take over, there are these interesting moral decisions to be made by the technology in the event of something unexpected. This website poses all these interesting questions about, if you were programming this self-driving car, what would you do in the situation? Your brakes fail. If you continue on your path, you're going to destroy three children in the pedestrian path. If you swerve, you're going to kill everybody in your car because you're going to crash. It's an interesting reveal of how we think and they, MIT and these other universities collect this data. I think for young people, gaming, just taking advantage of gaming technology to educate in a way that's maybe not right in your face, I think is a good way to learn. So much of learning is caught rather than taught. You just never know.\"]",
          "[\"Yeah. In my work, not really. Not in an AI/ML way, one of the things we do is we'll set up auto video recording when kids are reading with a robot in the home. And so that was when we spent a lot of time thinking about and working with families and what's going to make them comfortable. And most of them were pretty fine with it. They just wanted it to be really, really clear when recording was happening. So it's actually impacts the choice of robots that I use. So the Misty robot I have has a little LED on its advisor that we can make really bright. And so it's there because when it's video recording, and it actually has a nice scene, but it's also really good indicator so that we train families to know that if you see that light, it's video recording. And we also train the families to know exactly how to shut that off in a one-touch thing. There's a spot on the robot that if you touch it there, the video recording will shut down.We also give them free license if they don't want to do that and just want to snap the whole thing off, that's okay too. So it's not in an AI/ML way, but in a privacy ethics issue, that's probably the number one thing I think we run into with my work. | Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "['Well, as we said before we started recording I\\'m a tired, stressed grad student. I do a lot. I have a bajillion hats. I\\'m from Kentucky, before here and before that I lived in Georgia. In Kentucky, I got a Master\\'s in Computer Science and a Master\\'s in English. I worked at Fruit of the Loom, whose headquarters was in Bowling Green, Kentucky, where I was. And I did database stuff for them for six months. And that team, it was kind of rite of passage to accidentally lose the company a million dollars because they make enough that that\\'s laughable. And we\\'re just like, \"Holy shit, I\\'m going to get fired.\" And they\\'re like, \"No, no, no, we\\'ve all done it. Let\\'s go fix this.\" So I quit that job because I wanted to teach, and a full-time teaching position opened at the community college where I was an adjunct. So I taught for three years while I did my English masters and I moved here. So I do summer camps with kids and stuff, something I started doing in Kentucky that I still do here with [Wickedy 00:01:53] here. It was called VAMPY in Kentucky. I don\\'t know what it is about making fun five letter acronym names for these things. So I work with Wickedy here and [Bedra 00:02:03] pre-college, doing stuff with high school students. The youngest I\\'ve done is fourth graders all the way up to 60 year old guy changing careers for the 12th time in the community college, teaching them how to program. Or last summer I was just fucking bored from the pandemics. Well, I get to pick the topics. I\\'m going to have fun with this. We made art in class, but we wrote code to generate art for us. And these kids from Korea are fucking phenomenal, just putting that out there. They blew us all away with the stuff they did. Let\\'s see...']",
          "[\"I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"How can we engage youth in learning about AI and machine learning and ethics? Have you ever used any activities? Do you know of any resources out there of [inaudible 00:24:53] or any ideas of how we can? | Yeah, there's a really thought provoking... Hang on, just let me see. It's a great thought provoking website. What is it called? Oh yeah. moralmachine.net. If you go to moralmachine.net and then click the judge link, what this is, is this research that's done by MIT and a handful of other universities, and it asks you some interesting questions because everybody's talking about self-driving cars, and how they're going to happen. They're already happening. It's going to happen, and the reason behind it is, humans are not good at driving. People always think they're good at driving, but they're not. So, if we get the self-driving stuff done correctly, even if it's not entirely self-driving, even if you're sitting in the driver's seat and have to take over, there are these interesting moral decisions to be made by the technology in the event of something unexpected. This website poses all these interesting questions about, if you were programming this self-driving car, what would you do in the situation? Your brakes fail. If you continue on your path, you're going to destroy three children in the pedestrian path. If you swerve, you're going to kill everybody in your car because you're going to crash. It's an interesting reveal of how we think and they, MIT and these other universities collect this data. I think for young people, gaming, just taking advantage of gaming technology to educate in a way that's maybe not right in your face, I think is a good way to learn. So much of learning is caught rather than taught. You just never know.\"]",
          "[\"Yeah. I mean, that's definitely related because a lot of AI and ML technologies surveil us, right, or take that data and then do something with it. So in that this case, it's about the data collection and how comfortable we are. | Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "[\"I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that.\"]",
          "[\"We were talking about it a lot in recognition software and things like that, where the data sets that it's usually using to recognize and communicate with people is generally very Western white male and wrongly classifies people and communicates with them as if they were that group, which makes it very difficult for people to not only work but connect to it and get the benefits as so much to some other populations. | Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right?\"]",
          "[\"Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these- | It depends on the context. So if it's a Google image search engine, many kids say, yeah, I'm not represented here, but that's how the world is. The consequence isn't huge. I search for computer science professor and I don't see myself as a African American woman. Okay, that hurts. But I don't think Google should mess with that. Then when it comes to hiring algorithms, yeah, they see that as problematic. So the context really matters and the effect of the consequence as they see it matters to them.\"]",
          "[\"No, I actually feel that because I recently joined Golnaz's lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations?\"]",
          "['Oh yeah. I think that\\'s more important, but it\\'s mostly because I think it\\'s more important because you as an individual interact with society through those platforms and through that data and through that AI, even if you don\\'t realize you are, so me watching a video on YouTube, if I watch a video that has ethical issues, if I watch a video that it just is a very mean spirited video that can harm someone else by watching it I\\'m promoting it. And then while it might be, \"well, I didn\\'t tell anyone about it.\" It\\'s still me actually watching it needs an algorithm system that then promotes it to especially people in my geographic region or people with somewhat profiles to me. So it\\'s this idea that, I think it\\'s important because it also important and ultimately I think it\\'s the fault of the company that is the case, but I don\\'t trust them to fix that. So I think it\\'s more important to educate the individual on how that\\'s going to work. And then I think that\\'s just, once again your data literacy, understanding your digital footprint and also understanding your worth, humans are worth more as data to people than they are as humans. So I think understanding just how much your data is worth contextualize, you should just be giving it out for free, it is something fairly important and also the other adage of nothing is free in life where it\\'s, \"yeah, all these things are free.\" TikTok is free, but you\\'re getting a lot of data and you\\'re getting advertised to a lot. | Yeah. So, I said what happens to your data is the most important thing to me. And also what do you produce as a person in terms of data? So what do you produce and then what happens to it? That\\'s the biggest thing for me, because I think that also goes hand in hand with that literacy angle. But I also think the most important thing is how easy it is. Because I think reducing a mythos around a technology, there\\'s a quote about \"tech big magic hammer,\" but reducing the mythos around it, I think is also an extremely important aspect of it because computing has been plagued with people who brag about how hard it is and try to make it seem extra hard. And it isn\\'t. Especially if you don\\'t go into it with respective of it being extra hard. If you go into it with an encouragement angle and with an interest in learning something, it\\'s a lot easier. So I think the second most important thing is you need to learn just how easy it is to get started. There are so many things out there just to get started and they\\'re really easy to get started. They\\'re really easy to get just a little bit interested in it and then that\\'s enough to know if you\\'re going to like it or not. So I think that\\'s the two things. It\\'s from the computing side, what happens to your data? And then from that side, what are the resources you can do to get started and how easy is it to get started?']",
          "[\"Actually, I was surprised. I remember when I learned programming, I think I started learning programming after I went to college. The first year freshman, I feel like currently kids, they learn. They actually have the channels to interact with different technologies really early. They probably have a Google home or Siri on their mom or dad's phone or like their iPad or so. So I feel they're exposed to those advanced technologies, including machine learning or artificial intelligence a lot. And I feel like, I'm not sure, I know high school kids definitely learn programming. I'm not totally sure about middle school kids, but I would say even if it's not in school, they still have a lot of chance to interact with technologies. Not necessarily to learn how it works, but more just to get to know it. And I currently don't see, at least I haven't really thought about the harm of they learning machine learning at a really young age. There might be [inaudible 00:17:12] issues. I'm not sure, but I think they might trust more than we do. I don't really trust the Google, not trust, it's more like, I just don't feel very comfortable having Google home listening to my voice, but they probably got the young generation because they interacted with technologies much earlier than we did. So that might have built the trust in machine learning or artificial intelligence.\"]",
          "[\"It's definitely different than undergrad. I got my undergrad at Clemson too, in genetics. So, I understand. I'm doing a similar thing as you. Would you tell me a little bit more about your AI teaming and cognition, your research, what you're studying?\"]",
          "[\"That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way? | Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "[\"I mean, I'm a cynic in this regard. I think everything in this country and this world is motivated by money and financial interest, and corporate entities maintain that at the highest level. And depending on the governments, they either promote that or try to curb that back. But at the end of the day, it's all about money. So I think when you talk about AI and dealing with the problems that are dealt with, with AI, you have to understand that people are making money off of AI. Even if it's terrible AI in terms of being this most biased, terrible agent in the world, someone's going to make money off of it potentially. And I'm a cynic in that regard. People will put money above much of their own ethics in some cases. | Yeah. Yeah. I think so too. And they're so creative, the kids that ... All kids really, but the kids we're working with are just phenomenal. And once they get into it, they design such amazing things to help people. They're really interested in designing robots to help and robots for social good. So they're really understanding this stuff. And even if the goal is not for them to go be computer scientists or to go build these, but to have that fundamental understanding so they can be critical consumers, so they stop and say, wait, this is wrong. I need to say something. That's what we're hoping, obviously. I mean, we're not going to follow them, but we're hoping that what we do has a little bit of that impact anyway.\"]",
          "[\"I'm trying to remember. I sent it to my mom a while ago. If I ever find it, I will send you again. There was a really good group I found a while ago, that did coding education for individuals, but they split it up by grade and it was research group I think I have Kentucky or something, but they do second grade should learn this and this. And second grade is when they start them on scratch | But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam.\"]",
          "[\"Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n | Yeah. And you haven't had any problems recognizing faces. It recognizes kids, all different kinds of kids?\"]",
          "['Yeah. With bias, the there\\'s a famous Facebook, one where they\\'re serving ads for higher paying jobs to more men than women. There\\'s other ones where they\\'re predictive policing. The algorithm AI will tell officers to target more communities of color and not the communities of... it\\'s the inverse of minority and majority. And then there\\'s another one. Oh. That they have AI algorithms that try and identify candidates for parole. And it\\'ll say that African American individuals are 40% more likely to repeat offend, even though the human will look at the data and be like, \"That\\'s not my experience. That\\'s not the case. That\\'s not true.\" Things like that.']",
          "[\"How can we engage youth in learning about AI and machine learning and ethics? Have you ever used any activities? Do you know of any resources out there of [inaudible 00:24:53] or any ideas of how we can? | If you sit around in a room and you hear a bunch of people just talking, you learn a lot. They're not teaching you, you're just learning a lot. You're just kind of catching it. I think by employing things like gaming and some of these thought workshops, like what would you do in this case? It's interesting. That's why I really like teaching the class I'm teaching this semester, which is computing, ethics, and global society. Because some things that seem simple, are not. They're tough decisions. They may benefit some people. They're going to hurt some people. Is it the right thing to do? Even though you can do it, should you do it? It's-\"]",
          "['Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick. | Yeah. Absolutely. You answered everything. I\\'m looking at my follow-ups. I\\'m like, \"Oh, you got that. You got that.\" Yeah. I mean, if you want to expand a little bit, if you were to take Jules for example, right, your daughter, and she\\'s six, and what would you want her to know about machine learning or about at that age, anything or about how harmful it can be? Would you talk to her about privacy? Would you talk to her about misrepresentation or discrimination? Where would you go?']",
          "[\"So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful. | Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important.\"]",
          "['Yeah. I mean, it\\'s essentially... So it has to appear intelligent, I think, right? So I don\\'t know if it has to actually be intelligent. And so that\\'s where I\\'d classify some of the things that I do that are... If you really dig down into it, it would probably be classified in algorithms, right? It\\'s an algorithm. So I did a book selection algorithm. But I think it\\'s important to think about the user\\'s perspective on these things. And so I have a pretty broad idea about AI in that if it appears intelligent, if it comes across as intelligent... I mean, maybe if it even wasn\\'t the intent that it comes across as intelligent, but it appears that way, I think we have to treat it as AI because of the impact it would have on the person that it\\'s interacting with, right? So my book selection algorithm is not complicated at all. It\\'s essentially like a sorting algorithm and we add in a couple of inputs about what the kids\\' book preferences are like, their reading skill level, the amount of time they read, that kind of stuff. And then we tag books for all those features, and we just make a priority queue out of it. Really simple, early CS stuff. But to the kids, it came across as intelligent. And I think that\\'s the key factor, that when we interviewed them afterwards, kids felt that the robot was paying attention to them and that the suggestions the robot made for books were personal, that they were about them. And so that to me is now where you\\'re in artificial intelligence and you really have to then take that seriously, because if the person believes that they\\'re working with an intelligent machine, then you have to treat that carefully. | Yeah. In that sense, that I think, I would fall in line with a little bit more traditional perspective, because machine learning doesn\\'t necessarily always have to... It\\'s not is a user-facing part of what\\'s happening. So it\\'s essentially how to take a bunch of inputs, teaching a machine how to interpret those inputs, and to organize, categorize, or plan actions based on those inputs. So it\\'s different levels of black boxiness that go along with it. But yeah, it\\'s essentially the training machines to have a space in between input and output that is nonlinear, I guess. So I mean, the traditional perspective is you have this set of data that\\'s coded with these sets. And so you train that way. And then a new set of data that isn\\'t coded, the machine should take what it learned from this first one, apply it there, and come up with the same codes. Those codes could then be actions to do. Those codes could be categories. Those codes could be things like emotions, right? So, \"Here\\'s 10,000 pictures of people who look angry. Here\\'s 10,000 more. Which of these are angry?\" kind of thing. So that, I think. And now that I\\'m thinking about it, that\\'s almost sneaky or more problematic sometimes because you don\\'t necessarily always have the user interacting with it while you\\'re developing these things and testing them. And it can be to such a scale sometimes that the errors and the problems in there are easy to miss, right? That, \"Hey, we got 99% accuracy,\" but that means if there\\'s 100,000 images in that set that you\\'re classifying on, 1% is actually a lot. And if that 1% impacts me and you\\'re just going to take this thing off the shelf that\\'s 99% accurate, and you\\'re going to take it off the shelf, and it\\'s going to make a medical diagnosis, and I get the 1% problem, that\\'s pretty impactful. So again, that\\'s one of the things I talk a lot with graduate students who are like, \"Oh, I\\'ll just grab the thing and we\\'ll just figure it out. It\\'ll tell us what to do.\" No, that\\'s not safe in a lot of the things that we\\'re doing. So, yeah. So I guess, I don\\'t know. I mean, that\\'s a too-long explanation of what machine learning is. ']",
          "[\"Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas? | Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.\"]",
          "[\"Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight. | Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight.\"]",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything? | Yeah. I mean, that's definitely related because a lot of AI and ML technologies surveil us, right, or take that data and then do something with it. So in that this case, it's about the data collection and how comfortable we are.\"]",
          "[\"Yeah. It's still, I think, very early for AI, but at small companies, if done properly, small companies can leverage AI to be bigger, and for a big company, you can definitely leverage AI to keep growing and to be more competitive. So, certainly at places like Morningstar, where there is so much data to be processed, we applied AI in a wide variety of fashions. We really focus on providing independent investment data and research. Mostly, that analysis is done by human experts, but there are times where there's so many possible investments to cover that you have to consider something like AI to look at all the information about a particular investment that may not be that well known or very popular, but for completeness, a company like Morningstar wants to provide some insights. Oftentimes, we will experiment with AI to consume all that available information and generate some information that a human analyst could also generate, but maybe doesn't have the time to. There's lots of applications for AI all over the place. | Yeah, I'm part of the teaching faculty, so most of my focus is on teaching, but another responsibility I have is I serve as the executive director for Clemson's AI Research Institute for Science and Engineering, which is a new institute that came online in the summer of 2020, so right in the middle of COVID, and is only now sort of getting going. It was founded by Dr. Feng Luo, who is a professor in the school of computing. I'm helping him realize his vision for AI RISE, is what we call it, and it really is a combination of providing educational opportunities across Clemson to train faculty and researchers and students. It's to bring in the community, the upstate and the entire state, to help educate everyone really, on AI, and what AI is because it's such an overloaded, overused term and everybody maybe thinks they know what it is, but I think everybody has probably a different idea of what it is. AI always comes up in whatever subject I'm teaching. One of the classes I'm teaching this semester is on computing, ethics, and society. So obviously, talk a lot about AI and the moral issues associated with the use of AI and the bias that is proven to be in a lot of systems that employ AI today, and it has certainly lots of positive impacts, but also a lot of negative impacts. We talked a lot about that in that class. It always comes up because it's everywhere, honestly.\"]",
          "[\"I have to think. And I can maybe send you some links, but I know there's research going on, for example, because Microsoft owns Minecraft. There's research going on. I think they've released it as... But there's really research going on like how do you do reinforcement learning in Minecraft? How do you use Minecraft to create intelligent agents? And I think they've released tools and things that people can explore that on their own with Minecraft. So that's one thing. There's another project that comes to mind. I think it's called Make Code. I don't know. Is that a thing? Let me look really quick. I thought that has some element of... Oh this also has something to do with Minecraft, I guess. Microsoft free online learn to code platform. Yeah. So let me send you this link.\"]",
          "[\"Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right? | Yeah. Yeah, absolutely. It's getting a little better though. I have a young son too and Alexa is starting to understand him.\"]",
          "[\"Yeah, so work that I've done has been outlining how there are different camps I've worked in. Number one is outlining how humans and AI interact with each other, specifically my dissertation work is on how humans and AI systems can impact and influence each other in a task. So how a human could be susceptible to letting a robot or an AI system tell it what to do and take commands from it or vice versa. So looking at what comprises that, how humans should lead AI or how an AI should lead a human, things like that. So that trade off and then I've also done work looking at how ethics in AI systems can be implemented, created and how it interacts with humans. So, it's fairly big part is the ethical implications of AI systems and how those ethical implications ultimately impact the utility of AI. And then the last, there are other smaller things that I've had to do. But then the last main one I worked on is AI. There's a grant that I work on fairly often that is a grant that looks at using machine intelligence and recommender systems to provide recommendations to teachers for professional development. So it's a lot of providing recommendations for their professional development and I work on the side of that where I work on building this system and outline in that aspect.\"]",
          "[\"Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "[\"Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right? | Yeah. It's like everyone's got an Alexa or something like that in their house. And it is vocal recognition software, but if it can't understand you, it can't understand you. So it already has some of that bias in it or it can't understand certain dialects and accents and things like that.\"]",
          "['I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI.  | No that\\'s great, thank you. So this might go more into some of your ethics background, but how do some advances in AI or machine learning help humans?']",
          "[\"Yeah. It's still, I think, very early for AI, but at small companies, if done properly, small companies can leverage AI to be bigger, and for a big company, you can definitely leverage AI to keep growing and to be more competitive. So, certainly at places like Morningstar, where there is so much data to be processed, we applied AI in a wide variety of fashions. We really focus on providing independent investment data and research. Mostly, that analysis is done by human experts, but there are times where there's so many possible investments to cover that you have to consider something like AI to look at all the information about a particular investment that may not be that well known or very popular, but for completeness, a company like Morningstar wants to provide some insights. Oftentimes, we will experiment with AI to consume all that available information and generate some information that a human analyst could also generate, but maybe doesn't have the time to. There's lots of applications for AI all over the place.\"]"
         ],
         "marker": {
          "color": "#CFD8DC",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "other",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          11.38722038269043,
          9.561930656433105,
          9.808817863464355,
          6.1012420654296875,
          10.57840347290039,
          8.286977767944336,
          10.84753131866455,
          11.85797119140625,
          7.881875514984131,
          7.902233600616455,
          9.558833122253418,
          9.5651273727417,
          9.605277061462402,
          10.018453598022461,
          11.338351249694824,
          8.2659330368042,
          6.193267345428467,
          9.844965934753418,
          9.562457084655762,
          11.161344528198242,
          10.06132698059082,
          7.740703105926514,
          6.2443766593933105,
          12.759666442871094,
          8.034488677978516,
          10.77718734741211,
          11.62857723236084,
          11.893750190734863,
          6.186286926269531,
          8.13193130493164,
          9.167240142822266,
          7.067263126373291,
          8.067205429077148,
          12.094244003295898,
          10.949677467346191,
          5.948583126068115,
          10.106847763061523,
          10.862460136413574,
          5.593217849731445,
          9.300582885742188,
          11.601151466369629,
          9.20700740814209,
          13.042436599731445,
          9.799583435058594,
          12.604119300842285,
          7.235065460205078,
          9.315840721130371,
          9.486730575561523,
          11.887472152709961,
          7.21204948425293,
          10.894654273986816,
          10.803726196289062,
          11.124765396118164,
          9.967775344848633,
          11.914761543273926,
          6.130407810211182,
          10.164441108703613,
          12.588354110717773,
          11.999635696411133,
          9.6761474609375,
          12.897804260253906,
          9.297103881835938,
          12.180770874023438,
          9.47177791595459,
          11.923580169677734,
          11.606590270996094,
          10.962738990783691,
          12.055109024047852,
          12.092558860778809,
          9.382301330566406,
          8.647886276245117,
          13.150760650634766,
          8.53052806854248,
          5.9324235916137695,
          13.155166625976562,
          9.828329086303711,
          5.722314834594727,
          9.560892105102539,
          8.644782066345215,
          5.562239170074463,
          10.950823783874512,
          10.982025146484375,
          9.523815155029297,
          12.635882377624512,
          10.554224014282227,
          8.646514892578125,
          9.145666122436523,
          11.523588180541992,
          9.540789604187012,
          11.923492431640625,
          9.236117362976074,
          8.813636779785156,
          12.741194725036621,
          9.287994384765625,
          7.045257568359375,
          9.529842376708984,
          11.228217124938965,
          5.874740123748779,
          10.867643356323242,
          10.462228775024414,
          12.750529289245605,
          9.415671348571777,
          12.08303451538086,
          12.071253776550293,
          12.183683395385742,
          9.891891479492188,
          11.622149467468262,
          13.11120319366455,
          11.69202709197998,
          9.283979415893555,
          10.800416946411133,
          11.390931129455566,
          12.091562271118164,
          9.784276008605957,
          12.736499786376953,
          13.15076732635498,
          8.688785552978516,
          9.185197830200195,
          9.937573432922363,
          12.547213554382324,
          11.789803504943848,
          11.194515228271484,
          9.994346618652344,
          13.180315971374512,
          9.281343460083008,
          11.38668441772461,
          12.626029014587402,
          8.744852066040039,
          9.559976577758789,
          11.052268028259277,
          6.102085113525391,
          7.056909561157227,
          10.029860496520996,
          13.161208152770996,
          13.175039291381836,
          10.88953685760498,
          9.690808296203613,
          10.773694038391113,
          11.480561256408691,
          8.694488525390625,
          11.949301719665527,
          10.594230651855469,
          10.570072174072266,
          12.92568302154541,
          6.666338920593262,
          13.133326530456543,
          11.523812294006348,
          10.039254188537598,
          13.185888290405273,
          11.489029884338379,
          9.549604415893555,
          6.632709980010986
         ],
         "y": [
          0.9457551836967468,
          3.431974172592163,
          0.675367534160614,
          0.735724151134491,
          -1.1301698684692383,
          -0.5965385437011719,
          6.034865379333496,
          2.0786352157592773,
          -0.08402404934167862,
          -0.27872660756111145,
          2.859564781188965,
          2.878153085708618,
          3.429063320159912,
          2.4288747310638428,
          0.8104275465011597,
          -0.5876800417900085,
          0.04751889035105705,
          0.7056319713592529,
          2.8565919399261475,
          2.6999425888061523,
          2.3760552406311035,
          -0.04162639379501343,
          0.4302785098552704,
          5.3985395431518555,
          -0.3993339240550995,
          -0.6485549807548523,
          1.0516469478607178,
          0.7742501497268677,
          0.007868466898798943,
          -0.2492682933807373,
          1.1162029504776,
          -2.6330347061157227,
          -0.34308576583862305,
          5.7470173835754395,
          -0.7019976377487183,
          2.439009189605713,
          1.1950078010559082,
          5.978687763214111,
          0.7818479537963867,
          1.508654236793518,
          5.776379585266113,
          0.5016707181930542,
          0.8731279373168945,
          4.272860050201416,
          1.9180132150650024,
          -0.0973486453294754,
          6.441981315612793,
          2.7994070053100586,
          2.1322336196899414,
          -0.08252722024917603,
          6.0697855949401855,
          -0.6091964244842529,
          2.6840083599090576,
          3.4592361450195312,
          0.7817853689193726,
          0.5881544351577759,
          1.943772554397583,
          1.952129602432251,
          1.4152206182479858,
          0.8193439245223999,
          0.9261385798454285,
          6.423438549041748,
          0.9015491604804993,
          0.7674670815467834,
          2.1105785369873047,
          2.1475300788879395,
          1.1594874858856201,
          0.5767742395401001,
          5.751255989074707,
          4.518009185791016,
          0.3629535436630249,
          7.644092082977295,
          0.26385998725891113,
          2.4360830783843994,
          7.642175197601318,
          0.6641350388526917,
          2.5237855911254883,
          2.8751327991485596,
          0.2718982696533203,
          0.7706331610679626,
          -0.7063409090042114,
          5.658602237701416,
          1.1272697448730469,
          1.8849424123764038,
          -1.1289936304092407,
          -0.5189886689186096,
          1.1185346841812134,
          0.8808465003967285,
          1.1444005966186523,
          2.1219229698181152,
          1.4050841331481934,
          -0.1318112015724182,
          5.419429302215576,
          6.41367244720459,
          -2.6559157371520996,
          1.1112327575683594,
          1.7427016496658325,
          2.407318115234375,
          -0.7029477953910828,
          1.0273219347000122,
          5.418896198272705,
          0.8890349268913269,
          5.748955726623535,
          5.711997985839844,
          1.1016950607299805,
          2.6269257068634033,
          -0.19526031613349915,
          7.597463130950928,
          0.6672068238258362,
          1.4492515325546265,
          5.819816589355469,
          0.8929587006568909,
          5.7484822273254395,
          3.0909335613250732,
          5.418581008911133,
          7.653262615203857,
          0.3776063323020935,
          0.37438011169433594,
          3.4306812286376953,
          0.8840312957763672,
          5.669846534729004,
          2.146550416946411,
          3.4570271968841553,
          0.7356030344963074,
          6.3976311683654785,
          -0.39978641271591187,
          1.982964038848877,
          -0.0058957370929419994,
          2.860539674758911,
          1.206363320350647,
          0.7503526210784912,
          -2.6423537731170654,
          2.3549904823303223,
          7.649858474731445,
          0.7299148440361023,
          -0.7738473415374756,
          4.3486199378967285,
          6.107155799865723,
          0.8320263624191284,
          -0.5117589235305786,
          0.6483072638511658,
          -1.1420918703079224,
          -1.1441620588302612,
          0.9686279892921448,
          0.036227475851774216,
          7.618134498596191,
          -0.2704489231109619,
          2.3944778442382812,
          0.7305585741996765,
          -0.3187599182128906,
          3.45729923248291,
          0.06794680655002594
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess. | I mean, I got into computer science generally. I was interested in computer games, video games. I was kind of interested in the AI that goes into all the games and the enemies that you fight against or whatever. But I started out more interested in graphics and animation and my undergrad was more, I took more classes that are about computer graphics and animation with a little bit of AI, just kind of out of interest. But then in grad school, it was really just in talking with some of the potential professors that were there that I could work with. So my main advisor [inaudible 00:23:30] he kind of really started to introduce me to some of the ideas of, HCI and just thinking about interaction and thinking about how to... Because I was still interested in animation and animated characters, but thinking about how could you actually design animated characters that interact with people? And so I just got super interested in thinking about how does interaction work and then more and more interested in how does humans' social interaction work in the first place? It's so complex. So my passion has become understanding as much as I can about human social interaction and how to computationalize it enough that you can start to develop intelligence systems that participate in that.\"]",
          "[\"Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at.\"]",
          "[\"Yeah. So pretty much been an academic in training for a very long time. My dad was a dean at Penn State, so I grew up in academia. I knew about academia. I never wanted to be in academia, this is the funny part of the story. I remember telling both of my parents, why would I ever, ever want to do what my dad does? It sounds like the most boring thing in the world. And then I have ended up mimicking his career in many ways. So my professional career is very interdisciplinary. I have a degree in psychology, I have a degree in information science, and then a postdoc basically on computer science. And then I'm a professor of computer science, essentially. So I expand the spectrum from social and hard computational perspectives, and I think that's really important when you talk about AI. It can't just be one or the other. It can't just be a bunch of psychologists ruminating about what they think is important or sociologists thinking what they think is important about AI. You need real computer scientists in the room as well to understand the feasibility of how these things are going to happen. So both of these entities need to be talking to each other. I kind of planned my career that way, that I knew from a very early age that I was interested in the intersection of just people and technology. So I started off learning, I built my foundation with people, with the psychology, understanding that. And throughout my career, I've kept that foundation. It's what grounds me as a researcher and from my worldview of things. I've kept that humanistic foundation, but I built on top of that through additional degrees, more of a technology flare from information sciences and computer science. And now you kind of get whatever the heck I am nowadays. It's like a morphous blob of social and computer science. But I think it's important. And that's how I train my students to think about things is, if we're going to make sure that AI is beneficial for people and inclusive to many people, you better be taking into account the human side of things. But you also need equally know, from my perspective, as people that build AI, you need to know how to do that as well. So it's a big ask, but I think it's what the next generation of people studying, and implementing and developing AI need. They need both of these perspectives. Collaboration amongst both of those is great.But if we can start training people for this early on, to your point in some of your studies that you look at, if we can start instilling this mindset in kids from an early age of AI is not just computer science, it's human science as well, it's both of these things together, instilling that from an early age and having actual interdisciplinary training and degrees for that is going to be really important. So I went all over the place. I'll do this throughout the interview, I'll ramble- | Can you give us an example of a real world situation where that research could be applied, where AI and humans are working together?\"]",
          "[\"Yeah, so I got a computer science degree, and then during my computer science degree, I worked at a few different jobs using it. So I worked, like, IT management in ceilings working IT, I then had two stints doing software development work, one for a manufacturing company and then another for a larger tech company. And then around the time of graduating, I decided to go into grad school instead. And now that I'm in grad school, most of what I do is research with AI systems and humans using AI systems. | Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at.\"]",
          "['I pretty much always wanted to be more in the realm of technology and computer science, but also just really working with people. I was a little intimidated by computer science as a field at first. So, I went with psych. And then, I always knew I wanted to go to grad school. And so, I was originally thinking a good marriage between the two would be human factor psychology, because that is also very involved with technology and design and development. But I didn\\'t even know about the human-centered computing program until my current advisor, I found some papers by him that were about human-AI teaming and team cognition in those teams. And so, I reached out to him and the lab that I was working at the time, I did a collaboration with Nathan and he basically just reached out at me and was like, \"Hey, I\\'d like to have you in the lab. What do you think?\" And I\\'m thinking to myself, \"Oh, this is an opportunity to get to learn computer science and be a little bit more involved in that and still retain the usefulness of my undergrad.\" So, I jumped right at that opportunity.']",
          "['That\\'s interesting. How did you actually... I know you said you had a psychology degree from Clemson and then you kind of moved into the human-centered computing program. What made you interested in that? How did you become interested in working with AI and machine learning? | I pretty much always wanted to be more in the realm of technology and computer science, but also just really working with people. I was a little intimidated by computer science as a field at first. So, I went with psych. And then, I always knew I wanted to go to grad school. And so, I was originally thinking a good marriage between the two would be human factor psychology, because that is also very involved with technology and design and development. But I didn\\'t even know about the human-centered computing program until my current advisor, I found some papers by him that were about human-AI teaming and team cognition in those teams. And so, I reached out to him and the lab that I was working at the time, I did a collaboration with Nathan and he basically just reached out at me and was like, \"Hey, I\\'d like to have you in the lab. What do you think?\" And I\\'m thinking to myself, \"Oh, this is an opportunity to get to learn computer science and be a little bit more involved in that and still retain the usefulness of my undergrad.\" So, I jumped right at that opportunity.']",
          "[\"Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at. | Yeah, so work that I've done has been outlining how there are different camps I've worked in. Number one is outlining how humans and AI interact with each other, specifically my dissertation work is on how humans and AI systems can impact and influence each other in a task. So how a human could be susceptible to letting a robot or an AI system tell it what to do and take commands from it or vice versa. So looking at what comprises that, how humans should lead AI or how an AI should lead a human, things like that. So that trade off and then I've also done work looking at how ethics in AI systems can be implemented, created and how it interacts with humans. So, it's fairly big part is the ethical implications of AI systems and how those ethical implications ultimately impact the utility of AI. And then the last, there are other smaller things that I've had to do. But then the last main one I worked on is AI. There's a grant that I work on fairly often that is a grant that looks at using machine intelligence and recommender systems to provide recommendations to teachers for professional development. So it's a lot of providing recommendations for their professional development and I work on the side of that where I work on building this system and outline in that aspect.\"]",
          "[\"Yeah. So pretty much been an academic in training for a very long time. My dad was a dean at Penn State, so I grew up in academia. I knew about academia. I never wanted to be in academia, this is the funny part of the story. I remember telling both of my parents, why would I ever, ever want to do what my dad does? It sounds like the most boring thing in the world. And then I have ended up mimicking his career in many ways. So my professional career is very interdisciplinary. I have a degree in psychology, I have a degree in information science, and then a postdoc basically on computer science. And then I'm a professor of computer science, essentially. So I expand the spectrum from social and hard computational perspectives, and I think that's really important when you talk about AI. It can't just be one or the other. It can't just be a bunch of psychologists ruminating about what they think is important or sociologists thinking what they think is important about AI. You need real computer scientists in the room as well to understand the feasibility of how these things are going to happen. So both of these entities need to be talking to each other. I kind of planned my career that way, that I knew from a very early age that I was interested in the intersection of just people and technology. So I started off learning, I built my foundation with people, with the psychology, understanding that. And throughout my career, I've kept that foundation. It's what grounds me as a researcher and from my worldview of things. I've kept that humanistic foundation, but I built on top of that through additional degrees, more of a technology flare from information sciences and computer science. And now you kind of get whatever the heck I am nowadays. It's like a morphous blob of social and computer science. But I think it's important. And that's how I train my students to think about things is, if we're going to make sure that AI is beneficial for people and inclusive to many people, you better be taking into account the human side of things. But you also need equally know, from my perspective, as people that build AI, you need to know how to do that as well. So it's a big ask, but I think it's what the next generation of people studying, and implementing and developing AI need. They need both of these perspectives. Collaboration amongst both of those is great.But if we can start training people for this early on, to your point in some of your studies that you look at, if we can start instilling this mindset in kids from an early age of AI is not just computer science, it's human science as well, it's both of these things together, instilling that from an early age and having actual interdisciplinary training and degrees for that is going to be really important. So I went all over the place. I'll do this throughout the interview, I'll ramble- | Yeah. So what we study in my research group, almost everything we study nowadays is on this concept known as human AI teaming or human autonomy teaming. So it's the idea of humans teaming with AIs or autonomous teammates for the completion of a shared goal or a shared task. So everything we study nowadays is related to this concept, and there's a lot of different perspectives and paradigms and ways to look at that concept. And we really try to run the gamut on this. So the thing I talk about all the time when we talk about human AI teaming is that there's a bidirectional quality to it. You have to check off the boxes for the human. You have to check off the boxes for the AI. So the human needs to know how to interact with the AI. It needs to have an expectation of what the AI is. But also the AI needs to know what the heck humans are. It needs to know what teaming is, what matters. And this is where we have to get better. We have to develop autonomous teammates that actually know how to interact with humans, because it can't just be a one sided paradigm. And that's what it is right now. We stick people into a human AI team and we say, go work with this AI. But the AI has no clue how to work with you as a human. So what happens is that the human has to take on this brunt of dealing with basically a bad teammate. So what we're trying to do is number one, understand perceptions that humans have of AI as teammates, and reverse engineer those perceptions so we can build more effective, good AI teammates. So like I was talking about before, you can clearly see how there's the psychology point of view with the perceptions of humans and AI as teammates, but then on the other side, it's the computer science side of things. How do we actually build AI's that understand communication, coordination, awareness, things like team cognition, really critical aspects of teaming that have to be built into human AI teams?\"]",
          "['I pretty much always wanted to be more in the realm of technology and computer science, but also just really working with people. I was a little intimidated by computer science as a field at first. So, I went with psych. And then, I always knew I wanted to go to grad school. And so, I was originally thinking a good marriage between the two would be human factor psychology, because that is also very involved with technology and design and development. But I didn\\'t even know about the human-centered computing program until my current advisor, I found some papers by him that were about human-AI teaming and team cognition in those teams. And so, I reached out to him and the lab that I was working at the time, I did a collaboration with Nathan and he basically just reached out at me and was like, \"Hey, I\\'d like to have you in the lab. What do you think?\" And I\\'m thinking to myself, \"Oh, this is an opportunity to get to learn computer science and be a little bit more involved in that and still retain the usefulness of my undergrad.\" So, I jumped right at that opportunity. | It\\'s definitely different than undergrad. I got my undergrad at Clemson too, in genetics. So, I understand. I\\'m doing a similar thing as you. Would you tell me a little bit more about your AI teaming and cognition, your research, what you\\'re studying?']",
          "[\"Yeah. So pretty much been an academic in training for a very long time. My dad was a dean at Penn State, so I grew up in academia. I knew about academia. I never wanted to be in academia, this is the funny part of the story. I remember telling both of my parents, why would I ever, ever want to do what my dad does? It sounds like the most boring thing in the world. And then I have ended up mimicking his career in many ways. So my professional career is very interdisciplinary. I have a degree in psychology, I have a degree in information science, and then a postdoc basically on computer science. And then I'm a professor of computer science, essentially. So I expand the spectrum from social and hard computational perspectives, and I think that's really important when you talk about AI. It can't just be one or the other. It can't just be a bunch of psychologists ruminating about what they think is important or sociologists thinking what they think is important about AI. You need real computer scientists in the room as well to understand the feasibility of how these things are going to happen. So both of these entities need to be talking to each other. I kind of planned my career that way, that I knew from a very early age that I was interested in the intersection of just people and technology. So I started off learning, I built my foundation with people, with the psychology, understanding that. And throughout my career, I've kept that foundation. It's what grounds me as a researcher and from my worldview of things. I've kept that humanistic foundation, but I built on top of that through additional degrees, more of a technology flare from information sciences and computer science. And now you kind of get whatever the heck I am nowadays. It's like a morphous blob of social and computer science. But I think it's important. And that's how I train my students to think about things is, if we're going to make sure that AI is beneficial for people and inclusive to many people, you better be taking into account the human side of things. But you also need equally know, from my perspective, as people that build AI, you need to know how to do that as well. So it's a big ask, but I think it's what the next generation of people studying, and implementing and developing AI need. They need both of these perspectives. Collaboration amongst both of those is great.But if we can start training people for this early on, to your point in some of your studies that you look at, if we can start instilling this mindset in kids from an early age of AI is not just computer science, it's human science as well, it's both of these things together, instilling that from an early age and having actual interdisciplinary training and degrees for that is going to be really important. So I went all over the place. I'll do this throughout the interview, I'll ramble-\"]",
          "[\"Yeah, so I got a computer science degree, and then during my computer science degree, I worked at a few different jobs using it. So I worked, like, IT management in ceilings working IT, I then had two stints doing software development work, one for a manufacturing company and then another for a larger tech company. And then around the time of graduating, I decided to go into grad school instead. And now that I'm in grad school, most of what I do is research with AI systems and humans using AI systems.\"]",
          "[\"I mean, I got into computer science generally. I was interested in computer games, video games. I was kind of interested in the AI that goes into all the games and the enemies that you fight against or whatever. But I started out more interested in graphics and animation and my undergrad was more, I took more classes that are about computer graphics and animation with a little bit of AI, just kind of out of interest. But then in grad school, it was really just in talking with some of the potential professors that were there that I could work with. So my main advisor [inaudible 00:23:30] he kind of really started to introduce me to some of the ideas of, HCI and just thinking about interaction and thinking about how to... Because I was still interested in animation and animated characters, but thinking about how could you actually design animated characters that interact with people? And so I just got super interested in thinking about how does interaction work and then more and more interested in how does humans' social interaction work in the first place? It's so complex. So my passion has become understanding as much as I can about human social interaction and how to computationalize it enough that you can start to develop intelligence systems that participate in that.\"]",
          "[\"Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at. | Yeah, I'm actually familiar with that one, because it's the CU-TLP program. We have some people in the learning sciences that are working on that. So I'm familiar with this one. Would you mind giving me a little bit more detail about your project that involves AI ethics? What were some of the results that came out of that?\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "0_computer_ai_school",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "0_computer_ai_school"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          5.424173355102539,
          5.156403541564941,
          5.383983612060547,
          5.189922332763672,
          5.294247150421143,
          5.379180908203125,
          5.138608455657959,
          5.361144542694092,
          5.246120929718018,
          5.325964450836182,
          5.142171859741211,
          5.3661370277404785,
          5.073493003845215,
          5.2678117752075195
         ],
         "y": [
          3.1067748069763184,
          3.644672155380249,
          3.1288809776306152,
          3.5967445373535156,
          3.2243363857269287,
          3.1691768169403076,
          3.6690828800201416,
          3.1515798568725586,
          3.2703628540039062,
          3.1848654747009277,
          3.6885647773742676,
          3.143679618835449,
          3.675661563873291,
          3.35802960395813
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate. | Yeah, definitely. Those are great examples of ways that AI have helped us. So kind of on the reverse, what are ways that maybe AI has harmed us and who in particular have they harmed?\"]",
          "[\"So one job I had, when I was on a team, we would go out to different sites of critical infrastructure and basically assess their security posture and detect if they had any breaches or things like that and that all I've also done instant response missions, where a corporation may detect that they've had a breach of their security defenses and we come in and figure out where it came from, how to fix it and get rid of all the traces that are still on their systems or the networks. | Okay, great. So is human AI teaming a mix of AI agents and humans working together to complete a task?\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate.\"]",
          "[\"So I'm currently a fourth year PhD student in human centered computing, where we basically just study how humans interact with different technologies. And in our lab, our research focus is mainly human AI teams, human AI teaming. So we basically study how humans interact with AI teammates in a given environment like gaming, where AI is pretty common to see. And before that I did my bachelor and master in engineering. So basically it's kind of like the algorithm behind the thing. That's basically my background. | Would you mind telling me a little bit more about AI teaming?\"]",
          "[\"I know you provided an example with games because that's probably what you work on, but do you know of any other examples with AI teaming that's outside of the gaming world? | So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate.\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate. | So there are several examples I think I can talk about the first one is Tesla, is a kind of AI driving. You can consider as a teamwork because their goal is to get to the destination safely or so, or you can consider it as AI, just easy to use as a tool to drive. So that could be an example of humans using AI to help them. And another one theory we use every day, Google home, that type of thing is also. We ask them okay, turn on the light or share what's the weather today. So that's another example of humans using AI to get the information and save their time. And I think also healthcare, I think I read papers before, but I don't really know examples in my life, but I think I have read that healthcare use machine learning, especially when diagnosing some images of humans. What is it called that kind of scanning pictures and can help them to diagnose whether it's benign or a bad cancer or so, and in addition to that that's most of the examples. And also I mentioned before data scientists that would use different models to help them predict things.\"]",
          "[\"So I'm currently a fourth year PhD student in human centered computing, where we basically just study how humans interact with different technologies. And in our lab, our research focus is mainly human AI teams, human AI teaming. So we basically study how humans interact with AI teammates in a given environment like gaming, where AI is pretty common to see. And before that I did my bachelor and master in engineering. So basically it's kind of like the algorithm behind the thing. That's basically my background. | So basically the human AI teaming concept is that human and AI teammates coordinate and collaborate to finish a set of goals basically as [inaudible 00:01:32] goes. And what we have done previously is given environment norm in games and give them a series of team tasks that they need to either share information with AI or share with them and also receive information that shared by the AI teammates and finish the task. Or we have also done research where the AI has their own responsibility, but this you need to coordinate their tasks like connect with each other. So that they collaborate and finish the team go. So that's basically how we define human AI teams.\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate. | Yeah, that makes sense. That makes sense. Interesting. So I noticed you just mentioned maybe using AI to help humans. Do you know of ways that AI have helped humans and like what groups of humans have they been able to help?\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "1_ai_teammates_collaboration",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "1_ai_teammates_collaboration"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          5.120504379272461,
          5.514145374298096,
          5.156079292297363,
          5.542291641235352,
          5.132425308227539,
          5.112311840057373,
          5.368783950805664,
          5.158330917358398,
          5.26310920715332
         ],
         "y": [
          0.7083033323287964,
          0.5779914259910583,
          0.7261972427368164,
          0.6245245933532715,
          0.7160820960998535,
          0.7202532887458801,
          0.6505456566810608,
          0.7246624827384949,
          0.6810699701309204
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say. | That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning?\"]",
          "['A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on.']",
          "[\"Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say.\"]",
          "['A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on. | Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay.']",
          "['And what sort of technology or programming do you use for these robots or have you been using? Do you use artificial intelligence or machine learning? | A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on.']",
          "['A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on. | Well, okay. So you\\'ve already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything?']",
          "[\"That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way? | Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say.\"]",
          "[\"Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say. | Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "2_robot_text_speech",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "2_robot_text_speech"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          6.550387382507324,
          6.171736717224121,
          6.575832366943359,
          6.153163909912109,
          6.130721092224121,
          6.113368034362793,
          6.6086530685424805,
          6.551652908325195,
          6.356939315795898
         ],
         "y": [
          -2.935823917388916,
          -2.940520763397217,
          -2.931295394897461,
          -2.9453117847442627,
          -2.9420359134674072,
          -2.981520414352417,
          -2.924203872680664,
          -2.9363772869110107,
          -2.942136287689209
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "['I know that there\\'s a K12 curriculum that someone\\'s made for AI ethics. And so I just try to defer to that because I don\\'t want to collect K12 kids. It looks legit. They break it down in a way that\\'s not just bias. So that passes my muster of a lot of people, a lot of well meaning allies who are sometimes the most dangerous people, go jump to bias and go no further, which is why I tried to bring up all of these examples that weren\\'t bias. So there\\'s this K12 AI ethics curriculum somewhere. I think it\\'s AI K12 or something. It does a good job of helping students see things like it\\'s a computer. It has sensors. What\\'s the biology of the machine in a sense. It has these parts to it, something that students might not have ever thought about. One of my first activities I did when I taught CIT105, which was Intro to Computers for people who had never touched a computer in their lives, which was a good percentage of my students, the first thing I did is I would have a video of cats, just like a kitten livestream playing on the projector as they walked in, nice calming activity. And then once we got started, I said, \"Okay, tell me every piece of technology between these kittens and us.\" And it\\'s just illuminating seeing there\\'s this, there\\'s this part, and having them just name all the parts, just giving them permission to sit there and name the parts, I think, is important. Just kind of peel back that curtain and take things less. They start with letting students know there\\'re sensors. It makes decisions in these ways. Data\\'s collected in this way. It\\'s reasoned about in this way. Problems come up in this way, and blah, blah, blah. They spell it out very well and they have a nice breakdown of curriculum. I\\'m like this looks fine to me and I have to trust them because I don\\'t know a fuck at all about K12. | AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "['Okay, here\\'s how I\\'m going to think about it. In the Pasco case, I\\'ll use that example for both because it\\'s good to ground myself in a fucking case so I don\\'t go crazy. Looking at that case and thinking about how I speak to my students about AI ethics, which I speak to them the way I do because of my interviews of the 10 people I had. And they all had different views on things. It\\'s like, well, how can I do this in a way that\\'s kind of in agreement with all these different views, knowing that again, the course is going to be kind of pigeonholed the way that Allan did it. How can I introduce to things? And one of the things that people talk about a lot in the ethics, when you\\'re giving people who aren\\'t ethicists, they\\'re just a fucking programmer. You want them to think through things, is the stake stakeholders model. We just ask what\\'s at stake? But remember week one, I\\'m doing a pre-assignment where they don\\'t have any understanding what I\\'m talking about. So I can\\'t use the word stakes because they might take that in a too narrow sense. And so I say to them is, \"In this case, what are the things that we want to get right? Yes, there\\'s this really shitty thing that happened in Florida. The article ends on a very sad note, but there are things that we\\'re trying to get as humans in this. All the bad things that happened, we wanted those not to go that way, but we don\\'t want to just burn out everything to happen because again, we\\'re trying to get at things. So I have the students focus on this shared human endeavor of things we try to get right. And so in that case, the police department did a shitty thing, but they\\'re trying to get right reducing crime. They\\'re trying to right serving their community\\'s interest. They\\'re trying to get right distributing their resources well. Whether they actually got those things right in a way that still abides by fair terms is also preparation is a whole nother story, and we\\'ll let the Department of Justice... we\\'ll see what their investigation finds. But it has a lot of promise. And I try not to focus on promise because coming into viewing AI from the point of view of promise and predictions and it\\'s... you mention advancements. My first instinct there is no, let\\'s not talk about advancements first. There\\'s a huge history of technology here. There\\'s a huge history of institutions. Policing hasn\\'t always been the best and still isn\\'t always the best. And so if something\\'s being embedded there, we better really look at that history. We can\\'t look to advancement and promises of AI as a way to ignore how we got here. So I tried to avoid that framing. But we are trying to use it to get at good things. So I can\\'t say what is actually done well, but I can say we\\'re generally trying to get at good values with it. So let\\'s get that right, kind of be my call to action. And that involves also looking at the history of how we get to places. Anna Hoffman, AI ethics researcher from Northwest, I can\\'t remember where she\\'s from, but she\\'s awesome. You should look up her work. I have the idea of let\\'s not talk about entry points from promises, or futures, or potential of AI because of a talk she had. She\\'s like I\\'m in this pandemic and I\\'m tired and I\\'m sad. I\\'m a tired, stressed professor and I\\'m tired of looking at AI. So I\\'m going to come at this in a fun way, which is what does AI look like if we start from the position of infinite love for trans people? Let\\'s just take a fundamentally different approach to how we might think about AI. And it was a really fucking fun talk. But one of the things is looking at the medias around promises of AI and what gets left out when we focus too much on promises. So I\\'m doubly rambling at this point, but the things we\\'re trying to get right with AI, it\\'s good to enumerate those. And we kind of reground ourselves in the value of what we\\'re actually trying to do. And a lot of harms come out when we lose track of that and focus too much on it can just solve a lot of these things. When it\\'s put into context that it ought not be in, harms can come up. When it\\'s put into context too quickly, harms can come up. So I\\'m going to stop rambling and drink water.\\n']",
          "['Yeah, I\\'m actually familiar with that one, because it\\'s the CU-TLP program. We have some people in the learning sciences that are working on that. So I\\'m familiar with this one. Would you mind giving me a little bit more detail about your project that involves AI ethics? What were some of the results that came out of that? | Yeah. So on the day-to-day, once again, if I talk about it from a project to project standpoint, there are some easy ways to look at it, which are the TLP project, that\\'s coding and expert system, which means I\\'m looking at how experts look at things and I\\'m creating rules for a system to make judgements based on those rules. So, on a day-to-day, what I do is, I check those rules. I look at them, but most of what I do is just make sure it\\'s still working. Make sure nothing\\'s gone wrong, make sure I look at the results. I\\'m like, \"okay, these results came out, they don\\'t look bad.\" A lot of it, a lot of time goes into those, a lot, at a concentrated period and then a lot of it\\'s just watching it happen. So that\\'s the one. And then for my dissertation work, I actually use machine intelligent systems that were designed by other groups. So I got them from a group that the platform I chose to use, they had created these bots for that platform. And so what I actually do is just, I found these bots made by that platform that are open source and I slightly tweaked them. So similar to before it\\'s less of a like I need to change and develop so many things every day and more like, \"okay, I just need to keep this up and running, make sure everything\\'s fine. Make sure nothing\\'s changing.\" So just a lot of oversight.']",
          "[\"Yes, it's a data ethics course. We talk about AI a fuck ton. So besides camps, teaching people how to code, I'm a PA right now with YJ Kim here at the Wisconsin Center for Ed Research and I know David Shaffer. And through him I've heard about goal like everyone else in this community. So I hear about Clemson all the time. But where was I going? So I have a PA ship here. I've TAed here, TA for code and power a lot with Dr. Royston and that's kind of your classic critical theory approach to AI and a lot of... but like meritocracy, Google image search results for black women showing images of gorillas, those kinds of cases. Yeah, it's kind of fucked up. So talk about how code and power get gets embedded. And it's very critical around institutions and getting students to think about that stuff and learn about implicit biases and things like that. We actually have them take the implicit bias test and then think about the limitations of that test and try and reason about what data says. But-\"]",
          "[\"Yeah, I'm part of the teaching faculty, so most of my focus is on teaching, but another responsibility I have is I serve as the executive director for Clemson's AI Research Institute for Science and Engineering, which is a new institute that came online in the summer of 2020, so right in the middle of COVID, and is only now sort of getting going. It was founded by Dr. Feng Luo, who is a professor in the school of computing. I'm helping him realize his vision for AI RISE, is what we call it, and it really is a combination of providing educational opportunities across Clemson to train faculty and researchers and students. It's to bring in the community, the upstate and the entire state, to help educate everyone really, on AI, and what AI is because it's such an overloaded, overused term and everybody maybe thinks they know what it is, but I think everybody has probably a different idea of what it is. AI always comes up in whatever subject I'm teaching. One of the classes I'm teaching this semester is on computing, ethics, and society. So obviously, talk a lot about AI and the moral issues associated with the use of AI and the bias that is proven to be in a lot of systems that employ AI today, and it has certainly lots of positive impacts, but also a lot of negative impacts. We talked a lot about that in that class. It always comes up because it's everywhere, honestly. | Yeah. I think maybe some of the obvious ones that people can relate to are benefits associated with recommendations. We rely on, and we always have, even before AI, rely on recommendations to help us make buying decisions that are best for us. You see it mostly in streaming media, for example. It's like, you watch this movie or you watch this genre, you might be interested in this series, and it is helpful, definitely is helpful, and then when we buy stuff, same kind of thing. People are looking at this, they ended up buying this. You were looking at this, maybe you'd be interested in this. It's subtle. It's kind of built in and it helps us make these important or maybe not so important decisions where we spend our money and things like that, and that affects so many people, and our reliance on recommendation systems has been around forever, and I think AI has really tapped into it. It's very freaky, and the other thing is, a lot of companies use AI just to speed up what they can do and increase their capacity. Screening resumes, looking for the right candidates to hire. There's so many opportunities out there. So much of the workforce is in motion right now and companies that are hiring to keep up, they employ different AI solutions that they probably bought from somebody else, who decided, hey, it's probably good business for me to do this. Those are just a couple examples.\"]",
          "['Okay, here\\'s how I\\'m going to think about it. In the Pasco case, I\\'ll use that example for both because it\\'s good to ground myself in a fucking case so I don\\'t go crazy. Looking at that case and thinking about how I speak to my students about AI ethics, which I speak to them the way I do because of my interviews of the 10 people I had. And they all had different views on things. It\\'s like, well, how can I do this in a way that\\'s kind of in agreement with all these different views, knowing that again, the course is going to be kind of pigeonholed the way that Allan did it. How can I introduce to things? And one of the things that people talk about a lot in the ethics, when you\\'re giving people who aren\\'t ethicists, they\\'re just a fucking programmer. You want them to think through things, is the stake stakeholders model. We just ask what\\'s at stake? But remember week one, I\\'m doing a pre-assignment where they don\\'t have any understanding what I\\'m talking about. So I can\\'t use the word stakes because they might take that in a too narrow sense. And so I say to them is, \"In this case, what are the things that we want to get right? Yes, there\\'s this really shitty thing that happened in Florida. The article ends on a very sad note, but there are things that we\\'re trying to get as humans in this. All the bad things that happened, we wanted those not to go that way, but we don\\'t want to just burn out everything to happen because again, we\\'re trying to get at things. So I have the students focus on this shared human endeavor of things we try to get right. And so in that case, the police department did a shitty thing, but they\\'re trying to get right reducing crime. They\\'re trying to right serving their community\\'s interest. They\\'re trying to get right distributing their resources well. Whether they actually got those things right in a way that still abides by fair terms is also preparation is a whole nother story, and we\\'ll let the Department of Justice... we\\'ll see what their investigation finds. But it has a lot of promise. And I try not to focus on promise because coming into viewing AI from the point of view of promise and predictions and it\\'s... you mention advancements. My first instinct there is no, let\\'s not talk about advancements first. There\\'s a huge history of technology here. There\\'s a huge history of institutions. Policing hasn\\'t always been the best and still isn\\'t always the best. And so if something\\'s being embedded there, we better really look at that history. We can\\'t look to advancement and promises of AI as a way to ignore how we got here. So I tried to avoid that framing. But we are trying to use it to get at good things. So I can\\'t say what is actually done well, but I can say we\\'re generally trying to get at good values with it. So let\\'s get that right, kind of be my call to action. And that involves also looking at the history of how we get to places. Anna Hoffman, AI ethics researcher from Northwest, I can\\'t remember where she\\'s from, but she\\'s awesome. You should look up her work. I have the idea of let\\'s not talk about entry points from promises, or futures, or potential of AI because of a talk she had. She\\'s like I\\'m in this pandemic and I\\'m tired and I\\'m sad. I\\'m a tired, stressed professor and I\\'m tired of looking at AI. So I\\'m going to come at this in a fun way, which is what does AI look like if we start from the position of infinite love for trans people? Let\\'s just take a fundamentally different approach to how we might think about AI. And it was a really fucking fun talk. But one of the things is looking at the medias around promises of AI and what gets left out when we focus too much on promises. So I\\'m doubly rambling at this point, but the things we\\'re trying to get right with AI, it\\'s good to enumerate those. And we kind of reground ourselves in the value of what we\\'re actually trying to do. And a lot of harms come out when we lose track of that and focus too much on it can just solve a lot of these things. When it\\'s put into context that it ought not be in, harms can come up. When it\\'s put into context too quickly, harms can come up. So I\\'m going to stop rambling and drink water.\\n | I\\'m going to bring up... I had this already. Nope, that\\'s my writing sample. I have just been writing all this down for applications to places. And I\\'m at the point where if I write something down, I\\'m going to forget it immediately, which is the opposite problem because now I\\'m not constantly thinking about it. So AI or AI ethics is a bundle of ethical issues. And so thinking about populations depends on your entry point to thinking what AI ethics is. And so I\\'m just reading my notes, honestly. We can\\'t violate public trust, is kind of one thing. That comes up a lot in the discussions around autonomous vehicles, that we want to have autonomous vehicles for X, Y, Z reason. But in order for that to happen, the public kind of has to trust in the system. We have to agree. We have to know that it\\'s not going to run over black people more than white people. If it gets down to that moment, we have to know that it isn\\'t going to confuse us, not know that a cyclist is there or the actual case of where Uber hit a woman. I don\\'t know if it was Uber or not, but there was an autonomous vehicle that hit a woman because she jaywalked. Well, jaywalking should result in a fine, if that, not in death from a car that didn\\'t see you. The idea that the road is owned by cars is a relatively new one in human history. Roads were owned by the people walking on them. And so that one, public needs trust in it and that we don\\'t want to violate that trust. So in a way we can harm everyone when we release things too early in that sense or have things that create harms. Also, the people who are literally hit by the car, I think get harmed the most. Let\\'s see, there\\'s a lot of talk around misinformation being amplified on social media. And there\\'s also talk on certain things when you have newsfeeds, like Facebook, that put the things at the top that they think you want to see or that they want you to see. That\\'s in their benefit for you to see. Or TikTok, as fun as it is to find your own extremely niche set of friends on TikTok, and ridiculously fast, what\\'s getting left out? Who\\'s getting pushed down? And so there\\'s a lot of harms that come up when misinformation gets brought up to the top and there\\'s a lot of harms that get caused when certain communities are just completely pushed down and systematically given lower scores in the algorithm. And so there\\'s a lot of talk on TikTok around trans communities and people of color, people who don\\'t look pretty getting rated lower by the algorithm, so they\\'re not going to have as much of a viewership. And so when your money is tied to being a content creator, it sucks. There\\'s also the whole thing that if you\\'re queer and online, you are subject to harassment. You always have to have a few backup accounts because one of them is going to get blocked out because people are going to mass... people who don\\'t like you are going to mass report everything you do until you get flagged by the algorithm as bad and systematically have your account removed, even though you didn\\'t do anything wrong. That happens all the fucking times with trans content creators. There\\'s like all these, \"Hey, my thing got deleted again.\" And it\\'s just happens. So there\\'s a lot of algorithms that play a part in that. But also the algorithm isn\\'t separate from the system of humans interacting with it. So fuck, I talked a long list, when it\\'s applied in places like policing, compulsory education, medicine, et cetera. Those already have a lot of scrutiny on them, legal and public scrutiny on them. So it\\'s very important that we get AI right in those cases because we\\'re constantly looking at them for whatever reason. Reason might be that we have a lot of public scrutiny on policing and education because we want a fair and just society. And we see those institutions that are very important to the function of a fair and just society. So if we fuck up there at all, everyone pays attention. Work, AI changes the nature of work, and when the nature of work changes, some people are benefited and some people are completely displaced.']",
          "['So the data ethics course is a combi credit. There\\'s a combi version and a non-combi version. When I taught it over summer, it was eight weeks online, a hundred-ish students, some combi, some non-combi. I had two TAs and also I was the TA for it twice after. So we designed it. I was a TA, I was a TA, and then I taught it and I\\'m praying to teach it again this summer. But it\\'s a lot of writing because it\\'s a big combi component. And Alan Ruble was the instructor who made it, who\\'s my advisor. And when teachers design courses, they use the pedagogy they were taught with is the most number one thing I learned from talking with all these AI ethics teachers and all these different backgrounds is they might have similar motivation, similar goals, similar big picture aims where all we give a shit about AI ethics. But then what they actually do in the classroom is just what they were taught with. And I\\'m just like, \"Okay, that\\'s not very critical of you, but good.\" But it\\'s good to see all these things and these different views besides my own too because again, I teach the way that I\\'ve been teaching. I teach drawing on my English background, not my computer science background, even though I teach computer science style courses because in English, they have you take pedagogy classes. So I\\'m drawing on that. I have weekly journals that I make my students do because I found that useful in a class I took. It\\'s like, \"I like that so I\\'m going to make my students do it.\" But I\\'ve adapted to make sense for coding. So in that class, it\\'s a lot of writing and reading because that\\'s what the instructor who designed did a lot because he\\'s a philosopher. And in philosophy you do a lot of reading and writing. I added in because I like them the studio discussions. The students read an article or watch... do some kind of prep for the discussion. They meet with their group on Microsoft Teams or similar tool. They record it. They post the recording to the discussion board and then they watch X number of recordings and respond to them. I did a pre-post set up for that for the course where they read the first and second house at the time of the Pasco PD case that is happening in Florida. Pasco police department made an algorithm to predict who would commit crime or would be likely offenders and then went out and targeted people based on it. It\\'s currently going under investigation by the Department of Justice, which means you\\'re going to have a lot of fact finding about this case. So that\\'s a good case to know when you\\'re teaching data ethics.']",
          "[\"Yeah, I'm part of the teaching faculty, so most of my focus is on teaching, but another responsibility I have is I serve as the executive director for Clemson's AI Research Institute for Science and Engineering, which is a new institute that came online in the summer of 2020, so right in the middle of COVID, and is only now sort of getting going. It was founded by Dr. Feng Luo, who is a professor in the school of computing. I'm helping him realize his vision for AI RISE, is what we call it, and it really is a combination of providing educational opportunities across Clemson to train faculty and researchers and students. It's to bring in the community, the upstate and the entire state, to help educate everyone really, on AI, and what AI is because it's such an overloaded, overused term and everybody maybe thinks they know what it is, but I think everybody has probably a different idea of what it is. AI always comes up in whatever subject I'm teaching. One of the classes I'm teaching this semester is on computing, ethics, and society. So obviously, talk a lot about AI and the moral issues associated with the use of AI and the bias that is proven to be in a lot of systems that employ AI today, and it has certainly lots of positive impacts, but also a lot of negative impacts. We talked a lot about that in that class. It always comes up because it's everywhere, honestly. | Thank you. We appreciate those examples. I think most people have been encountering the AI that makes decisions or suggest things for them. Amazon, Netflix, they all have that. Now the reverse of that question, how has AI or machine learning harmed us and who have they harmed?\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "3_ai_ethics_students",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "3_ai_ethics_students"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          9.680002212524414,
          9.68522834777832,
          9.570234298706055,
          9.425143241882324,
          9.454899787902832,
          9.62798023223877,
          9.72597885131836,
          9.44367790222168,
          9.576642990112305
         ],
         "y": [
          4.341440200805664,
          4.353306770324707,
          3.5639123916625977,
          4.482575416564941,
          4.48945426940918,
          4.303798198699951,
          4.313150405883789,
          4.486721038818359,
          4.291794776916504
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids. | I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "[\"Definitely. Yeah. Okay. So just shifting a little bit. This is more directly related to some of the work that we do in my lab. What are your thoughts about youth learning about artificial intelligence or machine learning even as young as elementary or middle school? | I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "[\"Definitely. Yeah. Okay. So just shifting a little bit. This is more directly related to some of the work that we do in my lab. What are your thoughts about youth learning about artificial intelligence or machine learning even as young as elementary or middle school? | I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids.\"]",
          "[\"I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids.\"]",
          "[\"I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it. | Yeah. I think you start very simply with the state logic that defines most AI systems because that not only school teach them about how the AI system functions, but also just a lot about very basic decision and logic trees, which is important for kids to understand. So I think really starting from that, if this, then this or this kind of breakdown.\"]",
          "[\"Yeah. As, so as someone who works so closely with AI, can you imagine an elementary school student or a middle schooler or even younger than that, what sort of, how could you break that down? Like AI or machine learning to get them kind of exposed to that? | I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "[\"I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids. | Yeah. As, so as someone who works so closely with AI, can you imagine an elementary school student or a middle schooler or even younger than that, what sort of, how could you break that down? Like AI or machine learning to get them kind of exposed to that?\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "4_logical_teaching_language",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "4_logical_teaching_language"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          10.346963882446289,
          10.457669258117676,
          10.083995819091797,
          10.219813346862793,
          10.39130973815918,
          10.464801788330078,
          10.197093963623047,
          10.308807373046875
         ],
         "y": [
          1.0609320402145386,
          0.9930699467658997,
          1.0452313423156738,
          0.9749050140380859,
          1.0936176776885986,
          1.042852520942688,
          1.0792914628982544,
          1.0414142608642578
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids. | I mean, I understand that. So going off of what you said, what do you think it would be important for youth to know about AI and machine learning? What are some key things that you think they should take or learn about not necessarily going into the ethical issues, if you don't want to think about that?\"]",
          "[\"I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids. | I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"Yeah, I mean, I understand that so going off of what you said, what do you think it would be important for you to know about AI and machine learning like what are some key things that you think they should take or learn about not necessarily going into the ethical issues if you don't want to. | I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"I mean, I understand that. So going off of what you said, what do you think it would be important for youth to know about AI and machine learning? What are some key things that you think they should take or learn about not necessarily going into the ethical issues, if you don't want to think about that? | I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"Thank you for those responses. Can you think of any tools, activities, or resources that could be used to help young people to start thinking about AI and machine learning and kind of the algorithms behind them? | I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids.\"]",
          "[\"I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids.\"]",
          "[\"Yeah, I mean, I understand that so going off of what you said, what do you think it would be important for you to know about AI and machine learning like what are some key things that you think they should take or learn about not necessarily going into the ethical issues if you don't want to. | I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "5_game_kids_information",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "5_game_kids_information"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          10.77742862701416,
          10.92542839050293,
          11.027385711669922,
          11.073771476745605,
          10.791205406188965,
          10.842635154724121,
          10.864466667175293,
          10.900331497192383
         ],
         "y": [
          2.4235470294952393,
          2.3613760471343994,
          2.2625701427459717,
          2.2728631496429443,
          2.349039316177368,
          2.381434679031372,
          2.3903591632843018,
          2.348741292953491
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"It'll be interesting to see what type of advances the AI will add to our medical system. | Well, I do a lot of presentations that cover these basics to get people on the same page. It's really the ability of something to mimic the capabilities of the human mind by learning from data and experiencing things. This general definition, the ability again, to have a system mimic the capabilities that we have, but what we have, in reality, is something quite different. In general, I think about that. And then within that, there're subsets, like machine learning and deep learning. I think when people think of AI, they think of robots killing all the humans.\"]",
          "[\"One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system. | It'll be interesting to see what type of advances the AI will add to our medical system.\"]",
          "[\"Yeah. Yeah. It's cool. And what's interesting is it's almost like a broad computer science department that just happens to be at Microsoft. So there's a lot of us doing AI stuff, but it's kind of the whole spectrum of computer science research. | That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way?\"]",
          "[\"Well, I do a lot of presentations that cover these basics to get people on the same page. It's really the ability of something to mimic the capabilities of the human mind by learning from data and experiencing things. This general definition, the ability again, to have a system mimic the capabilities that we have, but what we have, in reality, is something quite different. In general, I think about that. And then within that, there're subsets, like machine learning and deep learning. I think when people think of AI, they think of robots killing all the humans.\"]",
          "[\"I mean, I would say it's broadly sort of the study of how to get computers and technologies to do things that humans would regard as intelligent. And I don't know. I say it like that, because I think it's sometimes a shifting boundary. Sometimes AI will accomplish something in AI and then people are like, wow, maybe that wasn't actually that intelligent after all. But I would say murkily, it's getting computers to do things that human humans would see as intelligent.\"]",
          "[\"That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way? | That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning?\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "6_implemented_mimic_capabilities",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "6_implemented_mimic_capabilities"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          8.171701431274414,
          8.190521240234375,
          8.364278793334961,
          8.216117858886719,
          8.430310249328613,
          8.46866512298584,
          8.30693244934082
         ],
         "y": [
          -0.36074990034103394,
          -0.3737368881702423,
          -0.36057424545288086,
          -0.5034041404724121,
          -0.2828569710254669,
          -0.31046849489212036,
          -0.36529842019081116
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "['Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "['Trying to think. The first thing that jumps to my mind is that any of these algorithms is responding to you and your digital choices, I guess. The importance of their data, and I know the erosion of privacy or the apathy towards the erosion of our privacy is continuing. I\\'m trying to think. I\\'m not sure how to teach people about things that are going to... or how to teach the youth about issues of ethics and bias because being involved in that conversation, I don\\'t know. It requires, like we were talking about... I don\\'t know, it\\'s just hard to teach kids, I feel like. Those are hard topics to broach. And so, I\\'m thinking that might be something restricted to like high school, but I don\\'t know. The sooner you start teaching things to kids the better, because they have continual reinforcement throughout all of their years. I don\\'t know. What do you think? How do we teach kids concepts like that? | Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "[\"Well, that's one of the reason we're interviewing all of you, is because we're trying to create a tool to help teach young children or young people about machine learning and AI and hopefully integrate some social and ethical issues into that. That's our goal, is that we're trying to figure out how to do that. Do you have any ideas of games or resources that you've seen that engage youth in these kinds of conversations or just conversations about AI or machine learning? | I mean, just pull anything out. I mean, a phone. This Google Home I have here, AI is everywhere. There is not a thing... I'm sure there's an AI processing algorithm in my camera right now that is doing something with my face. Any browser, your email. I mean, it's everywhere. Getting them to connect that training data and all this data that AI uses to continually teach itself and learn is coming from you and it's coming from everywhere. That might be a little scary for them to think about. But I think it's important to know.\"]",
          "['Well, that\\'s one of the reason we\\'re interviewing all of you, is because we\\'re trying to create a tool to help teach young children or young people about machine learning and AI and hopefully integrate some social and ethical issues into that. That\\'s our goal, is that we\\'re trying to figure out how to do that. Do you have any ideas of games or resources that you\\'ve seen that engage youth in these kinds of conversations or just conversations about AI or machine learning? | Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "['Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty. | I mean, just pull anything out. I mean, a phone. This Google Home I have here, AI is everywhere. There is not a thing... I\\'m sure there\\'s an AI processing algorithm in my camera right now that is doing something with my face. Any browser, your email. I mean, it\\'s everywhere. Getting them to connect that training data and all this data that AI uses to continually teach itself and learn is coming from you and it\\'s coming from everywhere. That might be a little scary for them to think about. But I think it\\'s important to know.']",
          "['Yeah. That makes a lot of sense. What do you think is the most important things that you should know about with AI and machine learning? | Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "7_models_reinforcement_training",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "7_models_reinforcement_training"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          12.055062294006348,
          11.992201805114746,
          11.434066772460938,
          12.048192024230957,
          12.077861785888672,
          12.088082313537598,
          11.949244499206543
         ],
         "y": [
          2.980982542037964,
          2.868868112564087,
          2.412879705429077,
          2.9324822425842285,
          2.9713571071624756,
          2.972187042236328,
          2.856459617614746
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools. | That's so interesting, like this hybrid task force and like what the responsibilities should be of the AI and what the responsibilities should be the human. I think that's fascinating. Okay. So more about you personally. So what sort of activities related to AI would you do in a typical week in your position now?\"]",
          "[\"Okay, great. So is human AI teaming a mix of AI agents and humans working together to complete a task? | Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools.\"]",
          "[\"Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools.\"]",
          "[\"Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools. | I mean, right now it's a lot of just research and what their capabilities are and then trying to model what they might be able to do. So I'm designing a new experiment right now and we're going to fake the AI. We're not going to make the real system, but we have to be able to make it seem real to the person doing the experiments. There's a lot of research there about what the AI should be capable of and what the person would see and interact with the AI.\"]",
          "[\"So one job I had, when I was on a team, we would go out to different sites of critical infrastructure and basically assess their security posture and detect if they had any breaches or things like that and that all I've also done instant response missions, where a corporation may detect that they've had a breach of their security defenses and we come in and figure out where it came from, how to fix it and get rid of all the traces that are still on their systems or the networks. | So something, we noticed that a lot of the tools, so to speak that we use in computer security are very automated and are going towards the trend of what you would consider AI autonomous agents and I started to become interested in Dr. Nathan McNeese work on human AI teaming because those systems are almost getting to the point in computer security where they would have a full team role. And just trying, I wanted to start looking into the factors that would make those teams more successful as those tools become more and more autonomous.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "8_security_relationship_tools",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "8_security_relationship_tools"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          6.37176513671875,
          6.1529035568237305,
          6.373172760009766,
          6.35825777053833,
          6.330784797668457,
          6.317376613616943
         ],
         "y": [
          0.08994117379188538,
          0.10286520421504974,
          0.06997128576040268,
          0.0738721713423729,
          0.14664755761623383,
          0.09665948152542114
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay. | Yeah. I mean, that\\'s definitely related because a lot of AI and ML technologies surveil us, right, or take that data and then do something with it. So in that this case, it\\'s about the data collection and how comfortable we are.']",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay.']",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay. | Yeah. In my work, not really. Not in an AI/ML way, one of the things we do is we\\'ll set up auto video recording when kids are reading with a robot in the home. And so that was when we spent a lot of time thinking about and working with families and what\\'s going to make them comfortable. And most of them were pretty fine with it. They just wanted it to be really, really clear when recording was happening. So it\\'s actually impacts the choice of robots that I use. So the Misty robot I have has a little LED on its advisor that we can make really bright. And so it\\'s there because when it\\'s video recording, and it actually has a nice scene, but it\\'s also really good indicator so that we train families to know that if you see that light, it\\'s video recording. And we also train the families to know exactly how to shut that off in a one-touch thing. There\\'s a spot on the robot that if you touch it there, the video recording will shut down.We also give them free license if they don\\'t want to do that and just want to snap the whole thing off, that\\'s okay too. So it\\'s not in an AI/ML way, but in a privacy ethics issue, that\\'s probably the number one thing I think we run into with my work.']",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay. | Well, okay. So you\\'ve already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything?']",
          "['And what sort of technology or programming do you use for these robots or have you been using? Do you use artificial intelligence or machine learning? | Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay.']",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "9_robot_space_robots",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "9_robot_space_robots"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          -9.031725883483887,
          -9.038060188293457,
          -9.0466947555542,
          -9.035221099853516,
          -9.043390274047852,
          -9.039018630981445
         ],
         "y": [
          1.180759072303772,
          1.186915636062622,
          1.1955353021621704,
          1.18415105342865,
          1.192270040512085,
          1.1879262924194336
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "['Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it. | Yeah. And that interaction between human and the self-driving car, the human and the technology, is really interesting. If you can\\'t look at them in the eye, how do you interact with them to ensure to minimize risk, right? That\\'s a whole other dimension of designing these technologies and testing them. And also-']",
          "['Yeah. I love that, Joe. And I can see that in your work, the way you\\'ve described it. That\\'s great. Okay. So let\\'s switch. So talking a little bit about why we\\'re doing this interview, right? We\\'re taking what people are saying and trying to apply it for learning experiences for young people. So what are your thoughts just generally about elementary school, middle school-age kids learning about either AI machine learning and the social and ethical impacts or both? [crosstalk 00:29:56] ideas around that, what they should learn, what\\'s important for them to know? Can they [crosstalk 00:30:00] in those issues | Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "['Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it. | Yeah. One that just comes to mind that didn\\'t come up is self-driving car technology. And that is dangerous in a much more salient way, right? And I think getting people to understand, when I see a car and I look over, I evaluate what the person behind that wheel is attending to, right? Before I cross the street, I like to make sure that I see that person look at me so that I know that they\\'re sensing me, right? So if and when it comes to be that I look over and there\\'s nobody at that wheel, there really needs to be an acclimation to understanding, \"Okay, the situation is that there\\'s a machine driving that car. How does it make mistakes?\" I know how people make mistakes, right? I know what it is. If they didn\\'t look at me and I start to walk out and they just roll even at a red light, that could catch me. So I pay attention to that. What are the types of errors that machines make in that scenario are important, that we know that sometimes, it might not recognize a stop sign at all. People are typically better at it. Maybe not typically, but we learn to understand the types of errors that can be common in these systems, or even uncommon, if they\\'re going to be catastrophic, and are aware enough in a way that we can respond in reasonable actions to it. I mean, right now, if I saw a driverless car, I would go nowhere near the street because they\\'re prone to weird accidents that I don\\'t understand right now. So I can\\'t figure them out well enough to do it. But eventually, if they become commonplace, we\\'ll have to know how that works.']",
          "['Yeah. Absolutely. You answered everything. I\\'m looking at my follow-ups. I\\'m like, \"Oh, you got that. You got that.\" Yeah. I mean, if you want to expand a little bit, if you were to take Jules for example, right, your daughter, and she\\'s six, and what would you want her to know about machine learning or about at that age, anything or about how harmful it can be? Would you talk to her about privacy? Would you talk to her about misrepresentation or discrimination? Where would you go? | Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "['Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "10_understand_thing_crosstalk",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "10_understand_thing_crosstalk"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          11.628433227539062,
          11.574045181274414,
          11.602795600891113,
          11.612406730651855,
          11.637407302856445,
          11.611018180847168
         ],
         "y": [
          6.38617467880249,
          6.328699588775635,
          6.386445999145508,
          6.373608112335205,
          6.389512062072754,
          6.372888088226318
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "['Yeah. It\\'ll definitely change the whole algorithm. Switching gears a little bit, we\\'re going to start talking about youth. What are your thoughts about youth learning about AI or machine learning? | I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic.']",
          "['I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic. | I think to not be fearful of it, but to understand at its core what it is and how it works, so that there\\'s no fear, for one thing. And then, when they\\'re learning that, that they can also, I think, open up their imaginations to what it could be used for, for the benefit of global society because I think what\\'s been happening with technology is, it used to be inaccessible to people who had great ideas, and now with things like cloud computing and tons of open source software, some very specialized around AI, if you have a good idea, you can try some things out. If you get everybody\\'s brain in the game, we have a lot of serious problems that we need to address, and I think young people need to understand that no one\\'s going to solve those problems, and you need to think about... I should say, other people are not always going to solve the problem. You have the ability to solve some of these problems or to contribute to the knowledge. We have to start addressing these difficult global challenges, and we have to stop, I think, focusing so much on things like TikTok. Just think about all the technology that went into building TikTok. It\\'s like, I\\'m sure there are valid and great use cases for TikTok, but I would say by and large, from my small sample, this is kind of ridiculous. It\\'s entertaining, and you get that dopamine hit. I understand that, but I just think, of these technologists that develop things like TikTok or Facebook, I\\'m like, we probably could have ended food distribution problems or just poverty, environmental concerns, but instead, we have TikTok. It\\'s kind of depressing a little bit. I\\'m hoping that you get to the youth and have them understand this. It can only be a good thing, I think.']",
          "['I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic. | That is essential, yes. You kind of touched on this, but what do you think is important for youth to know about AI or machine learning?']",
          "[\"Potentially everyone. I think there's a lot of promise and therapy and education and lots of places. | Awesome.  I'm kind of switching gears a little bit, what are your thoughts about youth learning about AI and machine learning?\"]",
          "[\"That is essential, yes. You kind of touched on this, but what do you think is important for youth to know about AI or machine learning? | I think to not be fearful of it, but to understand at its core what it is and how it works, so that there's no fear, for one thing. And then, when they're learning that, that they can also, I think, open up their imaginations to what it could be used for, for the benefit of global society because I think what's been happening with technology is, it used to be inaccessible to people who had great ideas, and now with things like cloud computing and tons of open source software, some very specialized around AI, if you have a good idea, you can try some things out. If you get everybody's brain in the game, we have a lot of serious problems that we need to address, and I think young people need to understand that no one's going to solve those problems, and you need to think about... I should say, other people are not always going to solve the problem. You have the ability to solve some of these problems or to contribute to the knowledge. We have to start addressing these difficult global challenges, and we have to stop, I think, focusing so much on things like TikTok. Just think about all the technology that went into building TikTok. It's like, I'm sure there are valid and great use cases for TikTok, but I would say by and large, from my small sample, this is kind of ridiculous. It's entertaining, and you get that dopamine hit. I understand that, but I just think, of these technologists that develop things like TikTok or Facebook, I'm like, we probably could have ended food distribution problems or just poverty, environmental concerns, but instead, we have TikTok. It's kind of depressing a little bit. I'm hoping that you get to the youth and have them understand this. It can only be a good thing, I think.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "11_tiktok_youth_learning",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "11_tiktok_youth_learning"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          9.33345890045166,
          9.314286231994629,
          9.387203216552734,
          9.34505844116211,
          9.448311805725098,
          9.365663528442383
         ],
         "y": [
          0.7049638032913208,
          0.6528802514076233,
          0.6822174191474915,
          0.4085400104522705,
          0.9198180437088013,
          0.6736839413642883
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that.\"]",
          "[\"Yeah. Yeah. It's cool. And what's interesting is it's almost like a broad computer science department that just happens to be at Microsoft. So there's a lot of us doing AI stuff, but it's kind of the whole spectrum of computer science research. | Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that.\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that. | That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning?\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that. | Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say.\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that. | That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way?\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "12_gaze_task_robot",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "12_gaze_task_robot"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          8.936081886291504,
          8.92806339263916,
          8.923410415649414,
          8.989255905151367,
          8.921092987060547,
          8.939580917358398
         ],
         "y": [
          -4.118736743927002,
          -4.114222049713135,
          -4.105356216430664,
          -4.171630382537842,
          -4.105321407318115,
          -4.123053550720215
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there. | Yeah. I mean, so like my young son, if we're doing something like virtual learning, he's got to be on virtual learning for that day or that week. It's very difficult for him to pay attention to just a screen for longer than like 10 to 15 minutes. But maybe if it was like a very immersive environment where you had some AI students that were almost collaborating with you, it might be something he could engage in and get a lot more from that environment.\"]",
          "[\"And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there. | Yeah. That's great. Like simulating that environment a little bit closer to what we had in person potentially.\"]",
          "[\"And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there.\"]",
          "[\"I mean, I know privacy is always a big issue, especially when you're talking about collecting major data, in order for something to apply or use machine learning. It's got to be collecting a lot of data about environment and people it's working with. So you have people who are comfortable sharing different levels of information and different levels of being information collected about them. And then also if you have an agent like that constantly collecting data, wherever it's working or interacting, there's the concept of like, okay, at what point do you require people to be like, oh, where this is going on and happening and require some sort of consent versus like it's just, it's so ubiquitous that everybody just knows it's going on. There's probably a tipping point somewhere there, but I think that are long ways off from that. So I think the privacy concerns are going to be pretty, pretty important. | I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "13_environments_gaming_virtual",
         "text": [
          "",
          "",
          "",
          "",
          "13_environments_gaming_virtual"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          7.1548027992248535,
          7.150892734527588,
          7.155707359313965,
          11.131241798400879,
          8.148160934448242
         ],
         "y": [
          -0.13807526230812073,
          -0.12123070657253265,
          -0.12790723145008087,
          2.752378463745117,
          0.5912913084030151
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important. | Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.\"]",
          "[\"Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas? | Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important.\"]",
          "[\"Yeah, I learned that a little bit the hard way, but just one mistake and then we fixed it. Okay, great. So imagining it sounds like you think it is important to teach kids about both those things like AI concepts, but also maybe some of the ethics around privacy and other issues and maybe getting more important as the years go on. Do you know of or can you think of any specific tools or activities or ways to engage youth in both of those things? | I think it's going to be a lot easier than we think just because they're so ingrained in the technology itself. My daughter is just about to turn one and she can already swipe on a tablet because she's watches her brother swipe on a tablet. And I think just those simple movement to think, they're going to be so more used to it than we are getting used to it. I don't think it'll be that much of a stretch just relating it to the devices they already use or have , I mean, even Alexa is a type of AI, because it does learn things about you, it learns to pick up your voice and your inflections and specific words and what you like. So, I mean just being able to talk about the everyday devices that they use is going to be easy for them.\"]",
          "[\"Yeah, I mean, I just have to be broad as close to them as they can like little things like, hey, you can't turn on this device in your sister's room and listen, because there's a privacy implication there. Just don't make it about huge asylum impact. It's got to be close ecosystems to themselves. | Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas?\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "14_alexa_experiences_words",
         "text": [
          "",
          "",
          "",
          "",
          "14_alexa_experiences_words"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          11.88298225402832,
          11.971765518188477,
          12.0518159866333,
          12.110036849975586,
          12.004150390625
         ],
         "y": [
          0.7221508026123047,
          0.8226968050003052,
          0.9113656878471375,
          0.9844354391098022,
          0.8601621389389038
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "D1",
          "x": -10.40369896888733,
          "y": 2.001938533782959,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "D2",
          "x": 2.3800362825393675,
          "xshift": 10,
          "y": 8.801252007484436
         }
        ],
        "height": 750,
        "shapes": [
         {
          "line": {
           "color": "#CFD8DC",
           "width": 2
          },
          "type": "line",
          "x0": 2.3800362825393675,
          "x1": 2.3800362825393675,
          "y0": -4.797374939918518,
          "y1": 8.801252007484436
         },
         {
          "line": {
           "color": "#9E9E9E",
           "width": 2
          },
          "type": "line",
          "x0": -10.40369896888733,
          "x1": 15.163771533966065,
          "y0": 2.001938533782959,
          "y1": 2.001938533782959
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Documents and Topics</b>",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1200,
        "xaxis": {
         "visible": false
        },
        "yaxis": {
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alt_topic_model.visualize_documents(docs_clo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular ol' BERT\n",
    "representation_model = BERTopic()\n",
    "topic_model = BERTopic(representation_model=representation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and fit BERT\n",
    "topics, probs = topic_model.fit_transform(docs_clo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>38</td>\n",
       "      <td>-1_the_that_and_to</td>\n",
       "      <td>[the, that, and, to, it, of, so, is, are, you]</td>\n",
       "      <td>[['A bit. Yeah. So the primary ones that I\\'d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>0_and_the_to_that</td>\n",
       "      <td>[and, the, to, that, of, it, in, so, is, you]</td>\n",
       "      <td>[[\"Yeah. So pretty much been an academic in tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1_of_and_was_that</td>\n",
       "      <td>[of, and, was, that, the, or, gaze, it, kind, ...</td>\n",
       "      <td>[[\"Yeah. Yeah. It's cool. And what's interesti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                Name  \\\n",
       "0     -1     38  -1_the_that_and_to   \n",
       "1      0    198   0_and_the_to_that   \n",
       "2      1     12   1_of_and_was_that   \n",
       "\n",
       "                                      Representation  \\\n",
       "0     [the, that, and, to, it, of, so, is, are, you]   \n",
       "1      [and, the, to, that, of, it, in, so, is, you]   \n",
       "2  [of, and, was, that, the, or, gaze, it, kind, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [['A bit. Yeah. So the primary ones that I\\'d ...  \n",
       "1  [[\"Yeah. So pretty much been an academic in tr...  \n",
       "2  [[\"Yeah. Yeah. It's cool. And what's interesti...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning the model with KeyBERTInspired\n",
    "#embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "#hbdscan_model = KMeans(n_clusters=5)\n",
    "key_representation_model = KeyBERTInspired()\n",
    "umap_model = UMAP(n_neighbors=30, n_components=5, min_dist=0.0, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=8, metric='euclidean', prediction_data=True)\n",
    "\n",
    "key_topic_model = BERTopic(representation_model=key_representation_model, umap_model=umap_model, hdbscan_model=hdbscan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and fit keyBERT\n",
    "key_topics, key_probs = key_topic_model.fit_transform(docs_clo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>87</td>\n",
       "      <td>-1_ai_robots_learning_robot</td>\n",
       "      <td>[ai, robots, learning, robot, machine, data, h...</td>\n",
       "      <td>[['A bit. Yeah. So the primary ones that I\\'d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0_programming_ai_kids_teaching</td>\n",
       "      <td>[programming, ai, kids, teaching, learning, yo...</td>\n",
       "      <td>[['Yeah. That makes a lot of sense. What do yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1_ai_teammates_teaming_collaboration</td>\n",
       "      <td>[ai, teammates, teaming, collaboration, resear...</td>\n",
       "      <td>[[\"So a lot of times human... we would call it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>2_ai_ethics_moral_questions</td>\n",
       "      <td>[ai, ethics, moral, questions, students, do, d...</td>\n",
       "      <td>[['Okay, here\\'s how I\\'m going to think about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>3_ai_intelligence_intelligent_artificial</td>\n",
       "      <td>[ai, intelligence, intelligent, artificial, le...</td>\n",
       "      <td>[[\"It'll be interesting to see what type of ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4_ai_privacy_kids_intelligence</td>\n",
       "      <td>[ai, privacy, kids, intelligence, unethically,...</td>\n",
       "      <td>[[\"I mean, I understand that. So going off of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>5_academia_ai_grad_tech</td>\n",
       "      <td>[academia, ai, grad, tech, studying, career, p...</td>\n",
       "      <td>[[\"Yeah. So pretty much been an academic in tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6_bias_ai_biased_learning</td>\n",
       "      <td>[bias, ai, biased, learning, data, research, t...</td>\n",
       "      <td>[[\"Yeah. You look at AI used to make decisions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7_consumer_teaching_do_about</td>\n",
       "      <td>[consumer, teaching, do, about, things, what, ...</td>\n",
       "      <td>[['Yeah. Absolutely. You answered everything. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                      Name  \\\n",
       "0     -1     87               -1_ai_robots_learning_robot   \n",
       "1      0     51            0_programming_ai_kids_teaching   \n",
       "2      1     27      1_ai_teammates_teaming_collaboration   \n",
       "3      2     22               2_ai_ethics_moral_questions   \n",
       "4      3     15  3_ai_intelligence_intelligent_artificial   \n",
       "5      4     14            4_ai_privacy_kids_intelligence   \n",
       "6      5     14                   5_academia_ai_grad_tech   \n",
       "7      6      9                 6_bias_ai_biased_learning   \n",
       "8      7      9              7_consumer_teaching_do_about   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [ai, robots, learning, robot, machine, data, h...   \n",
       "1  [programming, ai, kids, teaching, learning, yo...   \n",
       "2  [ai, teammates, teaming, collaboration, resear...   \n",
       "3  [ai, ethics, moral, questions, students, do, d...   \n",
       "4  [ai, intelligence, intelligent, artificial, le...   \n",
       "5  [ai, privacy, kids, intelligence, unethically,...   \n",
       "6  [academia, ai, grad, tech, studying, career, p...   \n",
       "7  [bias, ai, biased, learning, data, research, t...   \n",
       "8  [consumer, teaching, do, about, things, what, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [['A bit. Yeah. So the primary ones that I\\'d ...  \n",
       "1  [['Yeah. That makes a lot of sense. What do yo...  \n",
       "2  [[\"So a lot of times human... we would call it...  \n",
       "3  [['Okay, here\\'s how I\\'m going to think about...  \n",
       "4  [[\"It'll be interesting to see what type of ad...  \n",
       "5  [[\"I mean, I understand that. So going off of ...  \n",
       "6  [[\"Yeah. So pretty much been an academic in tr...  \n",
       "7  [[\"Yeah. You look at AI used to make decisions...  \n",
       "8  [['Yeah. Absolutely. You answered everything. ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequent topic using keyBERT\n",
    "key_topic_model.get_topic_info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.31353962421417236,
          0.32143348455429077,
          0.3544536828994751,
          0.3828449845314026,
          0.39501532912254333
         ],
         "xaxis": "x",
         "y": [
          "learning  ",
          "teaching  ",
          "kids  ",
          "ai  ",
          "programming  "
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#0072B2"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.277252197265625,
          0.30708813667297363,
          0.34798291325569153,
          0.36607396602630615,
          0.5518252849578857
         ],
         "xaxis": "x2",
         "y": [
          "research  ",
          "collaboration  ",
          "teaming  ",
          "teammates  ",
          "ai  "
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "color": "#CC79A7"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.17597343027591705,
          0.17840000987052917,
          0.3325502574443817,
          0.4431866705417633,
          0.48521149158477783
         ],
         "xaxis": "x3",
         "y": [
          "students  ",
          "questions  ",
          "moral  ",
          "ethics  ",
          "ai  "
         ],
         "yaxis": "y3"
        },
        {
         "marker": {
          "color": "#E69F00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.2766598165035248,
          0.35800665616989136,
          0.46132078766822815,
          0.4729863107204437,
          0.5932798385620117
         ],
         "xaxis": "x4",
         "y": [
          "learning  ",
          "artificial  ",
          "intelligent  ",
          "intelligence  ",
          "ai  "
         ],
         "yaxis": "y4"
        },
        {
         "marker": {
          "color": "#56B4E9"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.3086230158805847,
          0.31537389755249023,
          0.3206532597541809,
          0.34682390093803406,
          0.3889157772064209
         ],
         "xaxis": "x5",
         "y": [
          "unethically  ",
          "intelligence  ",
          "kids  ",
          "privacy  ",
          "ai  "
         ],
         "yaxis": "y5"
        },
        {
         "marker": {
          "color": "#009E73"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.3506460189819336,
          0.3553115129470825,
          0.36412081122398376,
          0.37088462710380554,
          0.40566757321357727
         ],
         "xaxis": "x6",
         "y": [
          "studying  ",
          "tech  ",
          "grad  ",
          "ai  ",
          "academia  "
         ],
         "yaxis": "y6"
        },
        {
         "marker": {
          "color": "#F0E442"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.24917250871658325,
          0.25173330307006836,
          0.30386942625045776,
          0.36367174983024597,
          0.4881976842880249
         ],
         "xaxis": "x7",
         "y": [
          "data  ",
          "learning  ",
          "biased  ",
          "ai  ",
          "bias  "
         ],
         "yaxis": "y7"
        },
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.19691455364227295,
          0.20290908217430115,
          0.2113822102546692,
          0.22689366340637207,
          0.2952789068222046
         ],
         "xaxis": "x8",
         "y": [
          "things  ",
          "about  ",
          "do  ",
          "teaching  ",
          "consumer  "
         ],
         "yaxis": "y8"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 0",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 1",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 2",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 3",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 4",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 5",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 6",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 7",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 500,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "Topic Word Scores",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis7": {
         "anchor": "y7",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis8": {
         "anchor": "y8",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis8": {
         "anchor": "x8",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "key_topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"How can we engage youth in learning about AI and machine learning and ethics? Have you ever used any activities? Do you know of any resources out there of [inaudible 00:24:53] or any ideas of how we can? | If you sit around in a room and you hear a bunch of people just talking, you learn a lot. They're not teaching you, you're just learning a lot. You're just kind of catching it. I think by employing things like gaming and some of these thought workshops, like what would you do in this case? It's interesting. That's why I really like teaching the class I'm teaching this semester, which is computing, ethics, and global society. Because some things that seem simple, are not. They're tough decisions. They may benefit some people. They're going to hurt some people. Is it the right thing to do? Even though you can do it, should you do it? It's-\"]",
          "['AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "['So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]- | Yes, it\\'s a data ethics course. We talk about AI a fuck ton. So besides camps, teaching people how to code, I\\'m a PA right now with YJ Kim here at the Wisconsin Center for Ed Research and I know David Shaffer. And through him I\\'ve heard about goal like everyone else in this community. So I hear about Clemson all the time. But where was I going? So I have a PA ship here. I\\'ve TAed here, TA for code and power a lot with Dr. Royston and that\\'s kind of your classic critical theory approach to AI and a lot of... but like meritocracy, Google image search results for black women showing images of gorillas, those kinds of cases. Yeah, it\\'s kind of fucked up. So talk about how code and power get gets embedded. And it\\'s very critical around institutions and getting students to think about that stuff and learn about implicit biases and things like that. We actually have them take the implicit bias test and then think about the limitations of that test and try and reason about what data says. But-']",
          "['Sure. Yeah. Currently an assistant professor in computer science and learning sciences at University of Illinois, Chicago. The work I do is in social robots for educational purposes that are designed as learning companions, where they work with or around kids to help them learn in lots of different scenarios. My training with it is... My PhD\\'s in learning sciences from University of Wisconsin, but I have a PhD minor in computer science where I did a lot of human robot interaction work. So I\\'m trained on the HR, the design of the robots and the interactions, and on learning and learning theory. And so the combination of those two is to design learning interactions for kids to really enhance their learning experiences. | A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on.']",
          "[\"That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning? | Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay.']",
          "[\"So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful.\"]",
          "[\"Well, that's one of the reason we're interviewing all of you, is because we're trying to create a tool to help teach young children or young people about machine learning and AI and hopefully integrate some social and ethical issues into that. That's our goal, is that we're trying to figure out how to do that. Do you have any ideas of games or resources that you've seen that engage youth in these kinds of conversations or just conversations about AI or machine learning? | I mean, just pull anything out. I mean, a phone. This Google Home I have here, AI is everywhere. There is not a thing... I'm sure there's an AI processing algorithm in my camera right now that is doing something with my face. Any browser, your email. I mean, it's everywhere. Getting them to connect that training data and all this data that AI uses to continually teach itself and learn is coming from you and it's coming from everywhere. That might be a little scary for them to think about. But I think it's important to know.\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that. | That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning?\"]",
          "[\"Sure. Yeah. Currently an assistant professor in computer science and learning sciences at University of Illinois, Chicago. The work I do is in social robots for educational purposes that are designed as learning companions, where they work with or around kids to help them learn in lots of different scenarios. My training with it is... My PhD's in learning sciences from University of Wisconsin, but I have a PhD minor in computer science where I did a lot of human robot interaction work. So I'm trained on the HR, the design of the robots and the interactions, and on learning and learning theory. And so the combination of those two is to design learning interactions for kids to really enhance their learning experiences.\"]",
          "[\"Are you talking about just learning AI and ML in general or the ethical issues? | Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess.\"]",
          "[\"Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right? | Oh yeah. My son figured out, I guess about a month ago, how to actually tell Alexa how to do things. And now it's hilarious because I have the app on my phone where I can see all the devices and I can see what Alexa, hears him saying.\"]",
          "[\"Yeah. In my work, not really. Not in an AI/ML way, one of the things we do is we'll set up auto video recording when kids are reading with a robot in the home. And so that was when we spent a lot of time thinking about and working with families and what's going to make them comfortable. And most of them were pretty fine with it. They just wanted it to be really, really clear when recording was happening. So it's actually impacts the choice of robots that I use. So the Misty robot I have has a little LED on its advisor that we can make really bright. And so it's there because when it's video recording, and it actually has a nice scene, but it's also really good indicator so that we train families to know that if you see that light, it's video recording. And we also train the families to know exactly how to shut that off in a one-touch thing. There's a spot on the robot that if you touch it there, the video recording will shut down.We also give them free license if they don't want to do that and just want to snap the whole thing off, that's okay too. So it's not in an AI/ML way, but in a privacy ethics issue, that's probably the number one thing I think we run into with my work. | Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "[\"Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n | Yeah. And you haven't had any problems recognizing faces. It recognizes kids, all different kinds of kids?\"]",
          "[\"Yeah, so work that I've done has been outlining how there are different camps I've worked in. Number one is outlining how humans and AI interact with each other, specifically my dissertation work is on how humans and AI systems can impact and influence each other in a task. So how a human could be susceptible to letting a robot or an AI system tell it what to do and take commands from it or vice versa. So looking at what comprises that, how humans should lead AI or how an AI should lead a human, things like that. So that trade off and then I've also done work looking at how ethics in AI systems can be implemented, created and how it interacts with humans. So, it's fairly big part is the ethical implications of AI systems and how those ethical implications ultimately impact the utility of AI. And then the last, there are other smaller things that I've had to do. But then the last main one I worked on is AI. There's a grant that I work on fairly often that is a grant that looks at using machine intelligence and recommender systems to provide recommendations to teachers for professional development. So it's a lot of providing recommendations for their professional development and I work on the side of that where I work on building this system and outline in that aspect. | Yeah, I'm actually familiar with that one, because it's the CU-TLP program. We have some people in the learning sciences that are working on that. So I'm familiar with this one. Would you mind giving me a little bit more detail about your project that involves AI ethics? What were some of the results that came out of that?\"]",
          "[\"I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that.\"]",
          "[\"It's definitely different than undergrad. I got my undergrad at Clemson too, in genetics. So, I understand. I'm doing a similar thing as you. Would you tell me a little bit more about your AI teaming and cognition, your research, what you're studying?\"]",
          "[\"I would say it's a computer or an agent that can take information and learn from it and make prediction or decision based on their ease. I don't know, learning process. | I would say my machine learning to me, my understanding is it's kind of the algorithm behind the scene. I consider artificial intelligence as kind of like with, how to say, a subject being there. It could be virtual, it could be a robot being there, but machine learning is more like the algorithm behind it. Kind of like its mind or core. That kind of feeling. It's kind of like if I use human as an example, it's just like artificial intelligence is a human and machine learning is kind of his brain to think, to help it to learn and make predictions.\"]",
          "['A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on.']",
          "[\"I mean, I'm a cynic in this regard. I think everything in this country and this world is motivated by money and financial interest, and corporate entities maintain that at the highest level. And depending on the governments, they either promote that or try to curb that back. But at the end of the day, it's all about money. So I think when you talk about AI and dealing with the problems that are dealt with, with AI, you have to understand that people are making money off of AI. Even if it's terrible AI in terms of being this most biased, terrible agent in the world, someone's going to make money off of it potentially. And I'm a cynic in that regard. People will put money above much of their own ethics in some cases. | Yeah. Yeah. I think so too. And they're so creative, the kids that ... All kids really, but the kids we're working with are just phenomenal. And once they get into it, they design such amazing things to help people. They're really interested in designing robots to help and robots for social good. So they're really understanding this stuff. And even if the goal is not for them to go be computer scientists or to go build these, but to have that fundamental understanding so they can be critical consumers, so they stop and say, wait, this is wrong. I need to say something. That's what we're hoping, obviously. I mean, we're not going to follow them, but we're hoping that what we do has a little bit of that impact anyway.\"]",
          "[\"Yeah. I mean, that's definitely related because a lot of AI and ML technologies surveil us, right, or take that data and then do something with it. So in that this case, it's about the data collection and how comfortable we are. | Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything? | Yeah. I mean, that's definitely related because a lot of AI and ML technologies surveil us, right, or take that data and then do something with it. So in that this case, it's about the data collection and how comfortable we are.\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that.\"]",
          "[\"I mean, just pull anything out. I mean, a phone. This Google Home I have here, AI is everywhere. There is not a thing... I'm sure there's an AI processing algorithm in my camera right now that is doing something with my face. Any browser, your email. I mean, it's everywhere. Getting them to connect that training data and all this data that AI uses to continually teach itself and learn is coming from you and it's coming from everywhere. That might be a little scary for them to think about. But I think it's important to know.\"]",
          "[\"Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right? | Yeah. It's like everyone's got an Alexa or something like that in their house. And it is vocal recognition software, but if it can't understand you, it can't understand you. So it already has some of that bias in it or it can't understand certain dialects and accents and things like that.\"]",
          "['A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on. | Well, okay. So you\\'ve already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything?']",
          "[\"But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam. | Yeah I mean that is you just gave us a lot of good information Thank you so, can you think of any ways to help us connect these topics like thinking about data machine learning AI to their everyday lives and make it meaningful for them.\"]",
          "[\"Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say. | That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning?\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that. | That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way?\"]",
          "[\"Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right? | Yeah. Yeah, absolutely. It's getting a little better though. I have a young son too and Alexa is starting to understand him.\"]",
          "[\"I'm trying to remember. I sent it to my mom a while ago. If I ever find it, I will send you again. There was a really good group I found a while ago, that did coding education for individuals, but they split it up by grade and it was research group I think I have Kentucky or something, but they do second grade should learn this and this. And second grade is when they start them on scratch | But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam.\"]",
          "['All good, there\\'s too many schools. Summer 2019, it was in-person. Summer 2020 it wasn\\'t held for the obvious reason. But because it wasn\\'t held in 2020, we\\'d started doing during the semester some three Saturdays in a row camps instead of three weeks continuous. So I didn\\'t make as much money that year, because I didn\\'t have the... it pays a thousand a week, which is good money when you\\'re trying to pay fucking rent over the summer and your [inaudible 00:03:34] doesn\\'t get paid over the summer. So I taught instead 2020, I did remote. We were trying out a few different ways of running it. Right. And we kind of did some experiments with it and it was a class on... shoot, what the fuck was it called? Analysis Skills for College Success: Research and Data Analysis, something like that. It was essentially a research methods course for high schoolers. So they know that the word research doesn\\'t mean Googling shit. It means thinking through things in a certain way, but it had to work no matter what major the student might go into. So I got to teach this really, really mixed methods, fun little three week camp thing to students drawing on the fact that I have a crazy, weird, mixed background and I continue doing weird mixed stuff. But all the offerings of that, every time I\\'ve done it, students the first time had fun. They were really attentive and stuff. Last time I had one student who actually did anything. And so students have just been increasingly burnt out from that. But summer \\'20, this most recent summer, we did it online. So I came into my office on Zoom and just was on Zoom for nine to three, but breaks, trying to mirror as much as we could of the in-person experience from the two years prior. It was better because students were a bit more captive. They are honors kids with rich parents, honestly. And they were excited. They wanted to go to this camp. They know it was canceled the previous year. They\\'re big nerds. I never went to those camps, but I\\'m glad to teach the kids who go to those camp. So we\\'ve adapted with the pandemic with that. But it\\'s all just various kinds of teaching. That\\'s my one-line bio that I teach kids and adults how to code which is the [crosstalk 00:05:38].\\n | So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]-']",
          "[\"I mean, I'm a cynic in this regard. I think everything in this country and this world is motivated by money and financial interest, and corporate entities maintain that at the highest level. And depending on the governments, they either promote that or try to curb that back. But at the end of the day, it's all about money. So I think when you talk about AI and dealing with the problems that are dealt with, with AI, you have to understand that people are making money off of AI. Even if it's terrible AI in terms of being this most biased, terrible agent in the world, someone's going to make money off of it potentially. And I'm a cynic in that regard. People will put money above much of their own ethics in some cases. | No. I think what you're doing is important. I think that AI, machine learning, reinforcement learning, neural networks aren't going anywhere. They're going to become a bigger entity and play a bigger role in society as time goes by. So the people that are really going to matter in terms of making sure that this works for other people, our youth, their kids, they're the ones that are going to actually have the big impact on being able to change this and understand it at a better level then people that are our age it. It starts with the education of the things that we're talking about, understanding how this can go wrong, understanding the errors so you're educated on that and you can avoid that. In many ways, I think what people like me and a lot of people that are similar to me, we just try to put bandaids on this because we're too far along on the road. And youth have the ability to not just put bandaids on things, they can actually fix things, I think.\"]",
          "[\"Yeah. Yeah. It's cool. And what's interesting is it's almost like a broad computer science department that just happens to be at Microsoft. So there's a lot of us doing AI stuff, but it's kind of the whole spectrum of computer science research. | Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that.\"]",
          "['Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick.']",
          "['A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on. | Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay.']",
          "['Yeah, a weird one. I was looking at job applications recently. I was just on LinkedIn, saw a few and this is a very interesting one. There\\'s a weird and this is where my dissertation\\'s about acceptance and over-reliance and things like that. And there\\'s a weird over-reliance for some of these things that creates someone missing the point almost. So I was looking at data science positions and they were, \"oh yeah, so data science work, you need to be able to do machine learning stuff for data science and predictive analysis.\" And so for that, they\\'re something called random force generation, which is a machine learning technique where you just put a bunch of data in and then say what you want to predict. And then it predicts the stuff with a bunch of complicated math and that\\'s all fine and dandy and it works out great. But you can also do that with basic statistical analysis. So it\\'s almost one of the concerns I run into is, people use it when they don\\'t need to like, don\\'t use a tool that isn\\'t needed or don\\'t use a tool that\\'s too much for the issue almost, which is an issue I\\'ve seen. And something I run into as a larger issue is over designing and over implementing some of that tech. | No, I actually feel that because I recently joined Golnaz\\'s lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations?']",
          "[\"No, I actually feel that because I recently joined Golnaz's lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations? | Yeah, so I think piggybacking off of my last answer, I think the most optimistic thing is work future and enabling a work future. But that is also simultaneously the worst aspect of it if done incorrectly. So generally speaking, I like the idea of relieving human workload, but I think the larger concern is in actual phasing out of them from a work perspective. So in other words, an over-reliance on a machine system for the sake of using a machine system as opposed to a human I think is a main issue. And I think ultimately it affects lower level, blue collar workers more and it affects a larger population that has received less expertise, education, throughout their lifestyle, throughout their entire life. So for instance, a PhD individual would probably not have a larger issue with that, given that the specialized area of knowledge that they received, but someone who has a more general work area where the knowledge application is not as deep, but it's a task based knowledge. I think that's the person that has the greatest ability to impact. And I think ultimately it comes down to who we let use that system because there's easy ways to mitigate that, creating systems where for instance, there's the idea of creating head to access for robots, whereas the utilization of a robotic employee doesn't allow you to just skirt the idea of paying for that employee. It still should be something that CRE, it is something you use in generating revenues, therefore it's something that needs to also be accounted for. So I think that's the main issue, the work future I want is also a good and a bad thing, depending on who's in charge of it.\"]",
          "[\"Yeah. I mean, so like my young son, if we're doing something like virtual learning, he's got to be on virtual learning for that day or that week. It's very difficult for him to pay attention to just a screen for longer than like 10 to 15 minutes. But maybe if it was like a very immersive environment where you had some AI students that were almost collaborating with you, it might be something he could engage in and get a lot more from that environment. | I mean, the aspect is so, AI is only as good as the data we give it, things like that. And I think there's a lot of discussion and research right now about a lot of AI being really biased to majorities because that's the data that they have access to. And so there might not be the ability to accurately assess or communicate with minority populations. And it could, I think there's some discussion about it kind of reinforcing biases and stereotypes because it's just operating off of the set of data that it's given.\"]",
          "[\"That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way? | Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say.\"]",
          "['Yeah. I love that, Joe. And I can see that in your work, the way you\\'ve described it. That\\'s great. Okay. So let\\'s switch. So talking a little bit about why we\\'re doing this interview, right? We\\'re taking what people are saying and trying to apply it for learning experiences for young people. So what are your thoughts just generally about elementary school, middle school-age kids learning about either AI machine learning and the social and ethical impacts or both? [crosstalk 00:29:56] ideas around that, what they should learn, what\\'s important for them to know? Can they [crosstalk 00:30:00] in those issues | Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick.']",
          "[\"Good. I mean that's fascinating. That's great. So you touched on the fact that you had some experience teaching young people about computer science. If you were going to teach, just think about what you could do if you were going to teach some young people about machine learning or AI, what type of activities or resources would you maybe use in order to do that? | I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that.\"]",
          "['I can only steal ideas for that because I usually don\\'t work with kids that young. I try to avoid compulsory schooling age stuff because I\\'m just like there\\'s so much more to consider and so many more standards you have to meet, which is why summer camps are good because you can do whatever you want within reason. But there is this book. I wish this had the fun cover version of it. It\\'s Living in data by Jer Thorp. And he\\'s an installation artist. It\\'s fascinating fucking reading. You should really find a recording of it if he has any book talks. It\\'s the same stuff in the book. But he says it and it has pictures.He\\'s an installation artist. Him and his group of his grad students, one thing they did is they did this installation in New York, right on a city street. And it\\'s this heart. You look at it, it\\'s a heart. But it\\'s all these pipes that are taller or shorter or whatever. It makes this heart shape. And people are like, \"Cool, we\\'ll take selfies in front of it.\" And they go look at it closer. And it\\'s a bar chart. And it labels on there where the population in New York has come from, from around the world.So then he sees people taking selfies with the bar, finding where their family is from. And then they\\'ve caught two people having weddings in front of it. And he\\'s like, \"This is the world\\'s only bar chart that has also been a wedding venue.\" And it\\'s just fascinating. He has all these different ways of getting people to experience data differently as a way to get out of their head and think about things in a really cool way. So I asked what his views on data ethics were. He said, \"Well, I have a 300 page answer to it. It\\'s called the book.\" So I had to get the book and fuck, there was another one. There is this fun activity that I saw someone do and post results of where they had real young kids. I can\\'t remember what age he said, but it had them draw what they thought Alexa looks like. Just take this disembodied voice that we as adults might take for granted and say, \"Well, kids, how do you think this looks?\" Something else that Jer Thorp did is working with kids, I just remembered this, is he had the students draw on big printouts of maps of their city, things that were important to them or things that they had noticed, like where are broken sidewalks? Where is the good place to get food? All these things that have meaning to the kids as they\\'ve experienced their own city. And they take this giant kid map and overlay it on old voting things. And you can see, well, here\\'s the red lining that happened. Here\\'s the history that you can see in the voting record aligning from forever ago with what you could see in your day to day life. And it was just this fascinating moment of overlaying and you go, \"Oh fuck.\" You don\\'t know that that\\'s what you\\'re drawing, but it\\'s what you\\'re drawing. And then they overlay it. It\\'s like, well, there you go. This data has all a long history and you can see it kids. So that I thought was really cool.  | I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI. ']",
          "[\"I have to think. And I can maybe send you some links, but I know there's research going on, for example, because Microsoft owns Minecraft. There's research going on. I think they've released it as... But there's really research going on like how do you do reinforcement learning in Minecraft? How do you use Minecraft to create intelligent agents? And I think they've released tools and things that people can explore that on their own with Minecraft. So that's one thing. There's another project that comes to mind. I think it's called Make Code. I don't know. Is that a thing? Let me look really quick. I thought that has some element of... Oh this also has something to do with Minecraft, I guess. Microsoft free online learn to code platform. Yeah. So let me send you this link.\"]",
          "['Well, as we said before we started recording I\\'m a tired, stressed grad student. I do a lot. I have a bajillion hats. I\\'m from Kentucky, before here and before that I lived in Georgia. In Kentucky, I got a Master\\'s in Computer Science and a Master\\'s in English. I worked at Fruit of the Loom, whose headquarters was in Bowling Green, Kentucky, where I was. And I did database stuff for them for six months. And that team, it was kind of rite of passage to accidentally lose the company a million dollars because they make enough that that\\'s laughable. And we\\'re just like, \"Holy shit, I\\'m going to get fired.\" And they\\'re like, \"No, no, no, we\\'ve all done it. Let\\'s go fix this.\" So I quit that job because I wanted to teach, and a full-time teaching position opened at the community college where I was an adjunct. So I taught for three years while I did my English masters and I moved here. So I do summer camps with kids and stuff, something I started doing in Kentucky that I still do here with [Wickedy 00:01:53] here. It was called VAMPY in Kentucky. I don\\'t know what it is about making fun five letter acronym names for these things. So I work with Wickedy here and [Bedra 00:02:03] pre-college, doing stuff with high school students. The youngest I\\'ve done is fourth graders all the way up to 60 year old guy changing careers for the 12th time in the community college, teaching them how to program. Or last summer I was just fucking bored from the pandemics. Well, I get to pick the topics. I\\'m going to have fun with this. We made art in class, but we wrote code to generate art for us. And these kids from Korea are fucking phenomenal, just putting that out there. They blew us all away with the stuff they did. Let\\'s see...']",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything?\"]",
          "[\"Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say.\"]",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay. | Well, okay. So you\\'ve already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything?']",
          "['I can only steal ideas for that because I usually don\\'t work with kids that young. I try to avoid compulsory schooling age stuff because I\\'m just like there\\'s so much more to consider and so many more standards you have to meet, which is why summer camps are good because you can do whatever you want within reason. But there is this book. I wish this had the fun cover version of it. It\\'s Living in data by Jer Thorp. And he\\'s an installation artist. It\\'s fascinating fucking reading. You should really find a recording of it if he has any book talks. It\\'s the same stuff in the book. But he says it and it has pictures.He\\'s an installation artist. Him and his group of his grad students, one thing they did is they did this installation in New York, right on a city street. And it\\'s this heart. You look at it, it\\'s a heart. But it\\'s all these pipes that are taller or shorter or whatever. It makes this heart shape. And people are like, \"Cool, we\\'ll take selfies in front of it.\" And they go look at it closer. And it\\'s a bar chart. And it labels on there where the population in New York has come from, from around the world.So then he sees people taking selfies with the bar, finding where their family is from. And then they\\'ve caught two people having weddings in front of it. And he\\'s like, \"This is the world\\'s only bar chart that has also been a wedding venue.\" And it\\'s just fascinating. He has all these different ways of getting people to experience data differently as a way to get out of their head and think about things in a really cool way. So I asked what his views on data ethics were. He said, \"Well, I have a 300 page answer to it. It\\'s called the book.\" So I had to get the book and fuck, there was another one. There is this fun activity that I saw someone do and post results of where they had real young kids. I can\\'t remember what age he said, but it had them draw what they thought Alexa looks like. Just take this disembodied voice that we as adults might take for granted and say, \"Well, kids, how do you think this looks?\" Something else that Jer Thorp did is working with kids, I just remembered this, is he had the students draw on big printouts of maps of their city, things that were important to them or things that they had noticed, like where are broken sidewalks? Where is the good place to get food? All these things that have meaning to the kids as they\\'ve experienced their own city. And they take this giant kid map and overlay it on old voting things. And you can see, well, here\\'s the red lining that happened. Here\\'s the history that you can see in the voting record aligning from forever ago with what you could see in your day to day life. And it was just this fascinating moment of overlaying and you go, \"Oh fuck.\" You don\\'t know that that\\'s what you\\'re drawing, but it\\'s what you\\'re drawing. And then they overlay it. It\\'s like, well, there you go. This data has all a long history and you can see it kids. So that I thought was really cool. ']",
          "['That\\'s awesome. I mean we need more people to understand coding in general. So I think it\\'s great that you do that work. Has any of the work you\\'ve done with those type of groups ever bridged into AI before or machine learning? | So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]-']",
          "[\"I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that. | Yeah. Just teach the basics. That makes a lot of sense. So what are your thoughts about youth learning or youth being introduced to some of the ethical or social issues around AI and machine learning?\"]",
          "['So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]-']",
          "[\"We were talking about it a lot in recognition software and things like that, where the data sets that it's usually using to recognize and communicate with people is generally very Western white male and wrongly classifies people and communicates with them as if they were that group, which makes it very difficult for people to not only work but connect to it and get the benefits as so much to some other populations. | Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right?\"]",
          "[\"Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight.\"]",
          "['Yeah. Absolutely. You answered everything. I\\'m looking at my follow-ups. I\\'m like, \"Oh, you got that. You got that.\" Yeah. I mean, if you want to expand a little bit, if you were to take Jules for example, right, your daughter, and she\\'s six, and what would you want her to know about machine learning or about at that age, anything or about how harmful it can be? Would you talk to her about privacy? Would you talk to her about misrepresentation or discrimination? Where would you go? | Yeah. One that just comes to mind that didn\\'t come up is self-driving car technology. And that is dangerous in a much more salient way, right? And I think getting people to understand, when I see a car and I look over, I evaluate what the person behind that wheel is attending to, right? Before I cross the street, I like to make sure that I see that person look at me so that I know that they\\'re sensing me, right? So if and when it comes to be that I look over and there\\'s nobody at that wheel, there really needs to be an acclimation to understanding, \"Okay, the situation is that there\\'s a machine driving that car. How does it make mistakes?\" I know how people make mistakes, right? I know what it is. If they didn\\'t look at me and I start to walk out and they just roll even at a red light, that could catch me. So I pay attention to that. What are the types of errors that machines make in that scenario are important, that we know that sometimes, it might not recognize a stop sign at all. People are typically better at it. Maybe not typically, but we learn to understand the types of errors that can be common in these systems, or even uncommon, if they\\'re going to be catastrophic, and are aware enough in a way that we can respond in reasonable actions to it. I mean, right now, if I saw a driverless car, I would go nowhere near the street because they\\'re prone to weird accidents that I don\\'t understand right now. So I can\\'t figure them out well enough to do it. But eventually, if they become commonplace, we\\'ll have to know how that works.']",
          "[\"Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess.\"]",
          "['Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick. | Yeah. Absolutely. You answered everything. I\\'m looking at my follow-ups. I\\'m like, \"Oh, you got that. You got that.\" Yeah. I mean, if you want to expand a little bit, if you were to take Jules for example, right, your daughter, and she\\'s six, and what would you want her to know about machine learning or about at that age, anything or about how harmful it can be? Would you talk to her about privacy? Would you talk to her about misrepresentation or discrimination? Where would you go?']",
          "[\"No, I actually feel that because I recently joined Golnaz's lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations? | Yeah. I mean, we've actually already seen that with some of like there's AI robots that Ford uses in their factories and stuff. So we've already seen that happening and I'm sure it's going to grow from here. So same kind of question, but in reverse. So how do advances in AI or machine learning harm us and are there any particular populations that it harms? Oh, oh no. Okay. All right. I'm going to go ahead and ask the question again just in case. So how do advances in AI or machine learning harm us, harm humans and who in particular is it harming?\"]",
          "[\"Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "[\"It depends on the context. So if it's a Google image search engine, many kids say, yeah, I'm not represented here, but that's how the world is. The consequence isn't huge. I search for computer science professor and I don't see myself as a African American woman. Okay, that hurts. But I don't think Google should mess with that. Then when it comes to hiring algorithms, yeah, they see that as problematic. So the context really matters and the effect of the consequence as they see it matters to them.\"]",
          "[\"That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way? | Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything? | Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "[\"Sure. Yeah. Currently an assistant professor in computer science and learning sciences at University of Illinois, Chicago. The work I do is in social robots for educational purposes that are designed as learning companions, where they work with or around kids to help them learn in lots of different scenarios. My training with it is... My PhD's in learning sciences from University of Wisconsin, but I have a PhD minor in computer science where I did a lot of human robot interaction work. So I'm trained on the HR, the design of the robots and the interactions, and on learning and learning theory. And so the combination of those two is to design learning interactions for kids to really enhance their learning experiences. | And what sort of technology or programming do you use for these robots or have you been using? Do you use artificial intelligence or machine learning?\"]",
          "['Those are even still around. Yeah. Those are really fun. And that\\'s-by high school coding class we did Java programming for Mindstorms and it was really interesting, but there\\'s so much more you could do with it after that. So if you had kids start with it and grow up with it, the potential they have there is pretty cool I think. | Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences.']",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything? | Yeah. In my work, not really. Not in an AI/ML way, one of the things we do is we'll set up auto video recording when kids are reading with a robot in the home. And so that was when we spent a lot of time thinking about and working with families and what's going to make them comfortable. And most of them were pretty fine with it. They just wanted it to be really, really clear when recording was happening. So it's actually impacts the choice of robots that I use. So the Misty robot I have has a little LED on its advisor that we can make really bright. And so it's there because when it's video recording, and it actually has a nice scene, but it's also really good indicator so that we train families to know that if you see that light, it's video recording. And we also train the families to know exactly how to shut that off in a one-touch thing. There's a spot on the robot that if you touch it there, the video recording will shut down.We also give them free license if they don't want to do that and just want to snap the whole thing off, that's okay too. So it's not in an AI/ML way, but in a privacy ethics issue, that's probably the number one thing I think we run into with my work.\"]",
          "['I met a guy in person, he used to help make movies. He\\'s like, \"yeah, my job used to take a team. And now it\\'s just one dude at one program.\" And so he\\'s changing careers into learning how to code, and so there\\'s that. Consumers might not be aware that they\\'re interacting with AI when they\\'re shopping online, except it might not be aware to the extent that they are and that can have harms to principles like consent, being aware enough of what\\'s going on to be able to fully make a decision and not doing that you\\'re interacting with AI or that visual. Invisible changes are happening on the screen behind the scenes. It might be bad depending on that context. And it\\'s fucking hard to regulate AI. | Yes, exactly. There\\'s a lot of other things that go into it. So we\\'re going to pivot a little bit more. I know you don\\'t necessarily always work with youth, but what are your thoughts about youth learning about machine learning or learning about algorithms and AI?\\n']",
          "[\"I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that. | Oh, I think it's super important. And I think that should be right with lesson number one. I mean, especially with machine learning, because again machine learning is algorithms that are trained on data and the data has to come from somewhere. And so there's the ethical questions around where does the data come from? And there are ethical questions of when someone's going to use this algorithm, this model. And so where are they going to use it? Who's going to use it? What are they going to use it for? So I think those are really important just to talk about early, I would say.\"]",
          "[\"Yeah, so work that I've done has been outlining how there are different camps I've worked in. Number one is outlining how humans and AI interact with each other, specifically my dissertation work is on how humans and AI systems can impact and influence each other in a task. So how a human could be susceptible to letting a robot or an AI system tell it what to do and take commands from it or vice versa. So looking at what comprises that, how humans should lead AI or how an AI should lead a human, things like that. So that trade off and then I've also done work looking at how ethics in AI systems can be implemented, created and how it interacts with humans. So, it's fairly big part is the ethical implications of AI systems and how those ethical implications ultimately impact the utility of AI. And then the last, there are other smaller things that I've had to do. But then the last main one I worked on is AI. There's a grant that I work on fairly often that is a grant that looks at using machine intelligence and recommender systems to provide recommendations to teachers for professional development. So it's a lot of providing recommendations for their professional development and I work on the side of that where I work on building this system and outline in that aspect.\"]",
          "[\"But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam.\"]",
          "[\"I have to think. And I can maybe send you some links, but I know there's research going on, for example, because Microsoft owns Minecraft. There's research going on. I think they've released it as... But there's really research going on like how do you do reinforcement learning in Minecraft? How do you use Minecraft to create intelligent agents? And I think they've released tools and things that people can explore that on their own with Minecraft. So that's one thing. There's another project that comes to mind. I think it's called Make Code. I don't know. Is that a thing? Let me look really quick. I thought that has some element of... Oh this also has something to do with Minecraft, I guess. Microsoft free online learn to code platform. Yeah. So let me send you this link. | So this is another kind of project that I remember seeing out of Microsoft Research. And it's about coding in general, I think and computer science, but I think it gets into some things that have some AI. So yeah, I think that those are the places I'd start.\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that. | Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say.\"]",
          "['No, I actually feel that because I recently joined Golnaz\\'s lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations? | Yeah, I think there\\'s a couple things with that in terms of, if I\\'m thinking about helping humans, I think it\\'s more about something that\\'s really cool about how humans work is that a lot of times they find easy small problems and they figure out how to fix it. And then it turns out that problem is a really big problem because it affects a lot of people. And so AI systems create this interesting idea where in my opinion, AI systems have a very low barrier of entry or at least including machine learning, you can use machine learning stuff if you can take two weekend classes and I think get pretty up to speed with it if you have the right education and teaching resources behind it. And the key to that is that, you can start solving really small problems in your life really quickly with really simple systems. And eventually what that does is it enables people on a micro level to make their life a bit easier, but also understand technology from this smaller perspective. And then I have a second way as well, but in my head there\\'s a small piece there where it\\'s the toolbox for people to get involved in computing stuff, I think actually lowers with that. So it helps people make things easier. Because I would say back in the day, coding on C and Fortran and stuff was awful and computers were millions of dollars and now it\\'s, \"oh you can do very easy things with very little.\" So that\\'s that The second one I have is, more of on the large scale meta-perspective is I think one of the reasons I also got into AI and find it really interesting is I like to see it from a perspective like aiding humans in reducing workload and replacing a human systems within a group and not having humans pick up that work. So more of the idea of freeing up humans to do things that matter to them, but also freeing up humans to work on stuff that they find important. I think that\\'s the 10 years from now potential is, I think in terms of workload and work future it has the most potential to impact and improve people.']",
          "['And what sort of technology or programming do you use for these robots or have you been using? Do you use artificial intelligence or machine learning? | Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay.']",
          "['Yeah. In that sense, that I think, I would fall in line with a little bit more traditional perspective, because machine learning doesn\\'t necessarily always have to... It\\'s not is a user-facing part of what\\'s happening. So it\\'s essentially how to take a bunch of inputs, teaching a machine how to interpret those inputs, and to organize, categorize, or plan actions based on those inputs. So it\\'s different levels of black boxiness that go along with it. But yeah, it\\'s essentially the training machines to have a space in between input and output that is nonlinear, I guess. So I mean, the traditional perspective is you have this set of data that\\'s coded with these sets. And so you train that way. And then a new set of data that isn\\'t coded, the machine should take what it learned from this first one, apply it there, and come up with the same codes. Those codes could then be actions to do. Those codes could be categories. Those codes could be things like emotions, right? So, \"Here\\'s 10,000 pictures of people who look angry. Here\\'s 10,000 more. Which of these are angry?\" kind of thing. So that, I think. And now that I\\'m thinking about it, that\\'s almost sneaky or more problematic sometimes because you don\\'t necessarily always have the user interacting with it while you\\'re developing these things and testing them. And it can be to such a scale sometimes that the errors and the problems in there are easy to miss, right? That, \"Hey, we got 99% accuracy,\" but that means if there\\'s 100,000 images in that set that you\\'re classifying on, 1% is actually a lot. And if that 1% impacts me and you\\'re just going to take this thing off the shelf that\\'s 99% accurate, and you\\'re going to take it off the shelf, and it\\'s going to make a medical diagnosis, and I get the 1% problem, that\\'s pretty impactful. So again, that\\'s one of the things I talk a lot with graduate students who are like, \"Oh, I\\'ll just grab the thing and we\\'ll just figure it out. It\\'ll tell us what to do.\" No, that\\'s not safe in a lot of the things that we\\'re doing. So, yeah. So I guess, I don\\'t know. I mean, that\\'s a too-long explanation of what machine learning is. ']",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay. | Yeah. I mean, that\\'s definitely related because a lot of AI and ML technologies surveil us, right, or take that data and then do something with it. So in that this case, it\\'s about the data collection and how comfortable we are.']",
          "[\"Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these- | It depends on the context. So if it's a Google image search engine, many kids say, yeah, I'm not represented here, but that's how the world is. The consequence isn't huge. I search for computer science professor and I don't see myself as a African American woman. Okay, that hurts. But I don't think Google should mess with that. Then when it comes to hiring algorithms, yeah, they see that as problematic. So the context really matters and the effect of the consequence as they see it matters to them.\"]",
          "[\"So I'm currently a fourth year PhD student in human centered computing, where we basically just study how humans interact with different technologies. And in our lab, our research focus is mainly human AI teams, human AI teaming. So we basically study how humans interact with AI teammates in a given environment like gaming, where AI is pretty common to see. And before that I did my bachelor and master in engineering. So basically it's kind of like the algorithm behind the thing. That's basically my background.\"]",
          "['Unfortunately, it can kind of amplify some of the systemic issues that are already happening. | One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system.']",
          "['Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick. | Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "['And what sort of technology or programming do you use for these robots or have you been using? Do you use artificial intelligence or machine learning? | A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on.']",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay. | Yeah. In my work, not really. Not in an AI/ML way, one of the things we do is we\\'ll set up auto video recording when kids are reading with a robot in the home. And so that was when we spent a lot of time thinking about and working with families and what\\'s going to make them comfortable. And most of them were pretty fine with it. They just wanted it to be really, really clear when recording was happening. So it\\'s actually impacts the choice of robots that I use. So the Misty robot I have has a little LED on its advisor that we can make really bright. And so it\\'s there because when it\\'s video recording, and it actually has a nice scene, but it\\'s also really good indicator so that we train families to know that if you see that light, it\\'s video recording. And we also train the families to know exactly how to shut that off in a one-touch thing. There\\'s a spot on the robot that if you touch it there, the video recording will shut down.We also give them free license if they don\\'t want to do that and just want to snap the whole thing off, that\\'s okay too. So it\\'s not in an AI/ML way, but in a privacy ethics issue, that\\'s probably the number one thing I think we run into with my work.']",
          "[\"Yeah. We have thought of that, integrating Snapchat filters or something like that. Something that they would be interested in. | Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess.\"]",
          "[\"Okay, great. And then, so related to what we were talking about, what about ethical or social issues that we discussed or even beyond that maybe we haven't discussed, how would you, what are your thoughts about youth learning about that in conjunction with AI and ML? | Yeah, absolutely. Kids are getting Chromebooks in second grade now. The schools are distributing. We just talked about our children, interacting with Alexa that's a intelligent system, so.\"]",
          "[\"But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam. | Those are even still around. Yeah. Those are really fun. And that's-by high school coding class we did Java programming for Mindstorms and it was really interesting, but there's so much more you could do with it after that. So if you had kids start with it and grow up with it, the potential they have there is pretty cool I think.\"]",
          "[\"Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say. | Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "['Yeah. And that also makes it to where it creates a mythos around it, that makes it really intimidating for some people like, \"is this new fancy amazing,\". Like that job application, I was like, \"yeah, it\\'s not hard to use, machine learning though.\" My brother learned it in a week and does it for his job, and he goes to his job and he tells his job, \"oh, I implement machine learning for this.\" And they\\'re just amazed by it. He learned that in two days. So it creates this weird thing where it is simultaneous, over-engineering the problem, but also intimidating new people from getting involved in it because it is considered magic. | No, I actually feel that because I recently joined Golnaz\\'s lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations?']",
          "['Well, that\\'s one of the reason we\\'re interviewing all of you, is because we\\'re trying to create a tool to help teach young children or young people about machine learning and AI and hopefully integrate some social and ethical issues into that. That\\'s our goal, is that we\\'re trying to figure out how to do that. Do you have any ideas of games or resources that you\\'ve seen that engage youth in these kinds of conversations or just conversations about AI or machine learning? | Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          null
         ],
         "marker": {
          "color": "#CFD8DC",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "other",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          11.236148834228516,
          6.890235900878906,
          11.242330551147461,
          4.804920196533203,
          9.869685173034668,
          -0.026946861296892166,
          7.363082408905029,
          8.477874755859375,
          9.907516479492188,
          4.812585353851318,
          7.843062400817871,
          7.746677875518799,
          7.889434337615967,
          9.76307487487793,
          9.233637809753418,
          10.618874549865723,
          4.686226844787598,
          5.709117412567139,
          10.239506721496582,
          9.21827507019043,
          9.759180068969727,
          9.684539794921875,
          9.864505767822266,
          8.112015724182129,
          7.732394695281982,
          10.281012535095215,
          6.045387268066406,
          10.038687705993652,
          9.840866088867188,
          7.722480773925781,
          6.090056896209717,
          9.754408836364746,
          9.265088081359863,
          9.886094093322754,
          10.334917068481445,
          10.267542839050293,
          10.000953674316406,
          6.312251091003418,
          7.463981628417969,
          10.06980037689209,
          10.31617259979248,
          10.628307342529297,
          11.247777938842773,
          6.041105270385742,
          3.4440054893493652,
          9.64536190032959,
          10.059279441833496,
          -0.03903131186962128,
          11.265899658203125,
          11.205629348754883,
          10.620281219482422,
          11.231634140014648,
          7.692567825317383,
          7.684367656707764,
          8.534278869628906,
          7.857462406158447,
          10.37785530090332,
          6.302496910095215,
          9.753066062927246,
          8.847978591918945,
          9.896806716918945,
          9.737720489501953,
          4.846074104309082,
          9.437051773071289,
          9.66425895690918,
          9.814506530761719,
          10.62013053894043,
          9.228554725646973,
          6.1015238761901855,
          5.944932460784912,
          9.874722480773926,
          6.32502555847168,
          -0.004549968522042036,
          10.143876075744629,
          -0.02956201322376728,
          8.888065338134766,
          5.298046112060547,
          6.214406490325928,
          10.327054023742676,
          10.27479076385498,
          -0.05880451202392578,
          7.842353343963623,
          7.651050090789795,
          6.084433078765869,
          10.076764106750488,
          6.306377410888672,
          8.827049255371094,
          8.13908863067627
         ],
         "y": [
          10.834328651428223,
          10.09924030303955,
          12.623576164245605,
          11.051657676696777,
          4.122503280639648,
          -4.511791229248047,
          12.16834545135498,
          11.002870559692383,
          5.7283549308776855,
          11.055466651916504,
          10.760437965393066,
          8.865798950195312,
          11.481475830078125,
          6.801372051239014,
          9.79681396484375,
          13.826665878295898,
          7.88788366317749,
          8.461538314819336,
          3.237236499786377,
          9.77808666229248,
          6.793603897094727,
          7.099503517150879,
          5.770238399505615,
          10.973862648010254,
          8.830707550048828,
          3.1893558502197266,
          5.458131790161133,
          3.536860466003418,
          5.795987129211426,
          8.833459854125977,
          5.418823719024658,
          12.646382331848145,
          9.769206047058105,
          5.728301525115967,
          13.272180557250977,
          3.2093324661254883,
          13.011472702026367,
          9.193495750427246,
          8.326292037963867,
          3.5660932064056396,
          13.234941482543945,
          13.82454776763916,
          12.617260932922363,
          5.50775671005249,
          11.050729751586914,
          7.174380779266357,
          3.532548189163208,
          -4.52389669418335,
          12.604894638061523,
          12.592840194702148,
          13.827055931091309,
          12.611572265625,
          8.670635223388672,
          7.674933910369873,
          12.33227252960205,
          10.806163787841797,
          13.311640739440918,
          9.307502746582031,
          6.803805828094482,
          9.035675048828125,
          4.052094459533691,
          6.918507099151611,
          11.077478408813477,
          12.096857070922852,
          7.127452850341797,
          10.385785102844238,
          13.828109741210938,
          9.784811019897461,
          5.412148952484131,
          5.655734539031982,
          5.750210762023926,
          9.269614219665527,
          -4.489416599273682,
          13.310210227966309,
          -4.514439105987549,
          9.071601867675781,
          8.135356903076172,
          7.764639854431152,
          13.264788627624512,
          3.199983596801758,
          -4.543687343597412,
          10.800501823425293,
          10.867177963256836,
          5.427983283996582,
          3.5166685581207275,
          9.290984153747559,
          11.590591430664062,
          8.178667068481445
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Definitely. Yeah. Okay. So just shifting a little bit. This is more directly related to some of the work that we do in my lab. What are your thoughts about youth learning about artificial intelligence or machine learning even as young as elementary or middle school? | I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "[\"So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful. | Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important.\"]",
          "['Oh yeah. Well, so for the most part, I have a relatively Wisconsin accent. And so the models that we used were... we tried to do was out of Carnegie Mellon. I forget the name of the program that did it, but it was one that could run on board. And that did relatively standard US dialect, was generally really successful. The student who was running that was from India, though, and her accent was completely inaccessible to the robot. And then she worked with another student who was Korean and also really struggled with her accent as well. So between the two of them, they were just really frustrated trying to get... And we were just trying to set up simple keyword kind of things like, \"Hey, Alexa,\" kind of stuff. We were just trying to do simple keywords with the robot, and it got nowhere. So that was the Carnegie Mellon one. And then we\\'ve been a little bit more successful. I haven\\'t used it, but the folks I worked with at Wisconsin who followed up with some of this, they did add some keywords to our last field-based one over the summer. And I don\\'t know if they tested it much as far as accents go. I know one of the researchers on the team is Turkish, and she didn\\'t seem to have much trouble. But again, we really constrained the word list for that. And the cloud-based ones I think are a little bit better now than the ones we were... Sync, that\\'s the CMU one that we were using. So I think they\\'re a little bit better now. And so that\\'s one that has been starting to get cooked into the robots I have. I just haven\\'t sent them out in the field doing that yet, but just simple, yes, no kind of stuff seems to work, and that hasn\\'t... We haven\\'t seen any evidence that that level has been impacted by any accent or dialects.  | Yeah. I mean, it\\'s essentially... So it has to appear intelligent, I think, right? So I don\\'t know if it has to actually be intelligent. And so that\\'s where I\\'d classify some of the things that I do that are... If you really dig down into it, it would probably be classified in algorithms, right? It\\'s an algorithm. So I did a book selection algorithm. But I think it\\'s important to think about the user\\'s perspective on these things. And so I have a pretty broad idea about AI in that if it appears intelligent, if it comes across as intelligent... I mean, maybe if it even wasn\\'t the intent that it comes across as intelligent, but it appears that way, I think we have to treat it as AI because of the impact it would have on the person that it\\'s interacting with, right? So my book selection algorithm is not complicated at all. It\\'s essentially like a sorting algorithm and we add in a couple of inputs about what the kids\\' book preferences are like, their reading skill level, the amount of time they read, that kind of stuff. And then we tag books for all those features, and we just make a priority queue out of it. Really simple, early CS stuff. But to the kids, it came across as intelligent. And I think that\\'s the key factor, that when we interviewed them afterwards, kids felt that the robot was paying attention to them and that the suggestions the robot made for books were personal, that they were about them. And so that to me is now where you\\'re in artificial intelligence and you really have to then take that seriously, because if the person believes that they\\'re working with an intelligent machine, then you have to treat that carefully.']",
          "[\"Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas? | So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful.\"]",
          "['I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic. | That is essential, yes. You kind of touched on this, but what do you think is important for youth to know about AI or machine learning?']",
          "[\"Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas? | Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important.\"]",
          "[\"I think it's important, but I think it's important from two perspectives. The thing I think it's more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it's very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society's going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there's it's good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it's a two front benefit from it. | Yeah. That makes sense. I am still learning how to code. So I'll get there one day.\"]",
          "[\"I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it. | Yeah. I think you start very simply with the state logic that defines most AI systems because that not only school teach them about how the AI system functions, but also just a lot about very basic decision and logic trees, which is important for kids to understand. So I think really starting from that, if this, then this or this kind of breakdown.\"]",
          "[\"So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful. | Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.\"]",
          "[\"Yeah, I mean, I just have to be broad as close to them as they can like little things like, hey, you can't turn on this device in your sister's room and listen, because there's a privacy implication there. Just don't make it about huge asylum impact. It's got to be close ecosystems to themselves. | Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas?\"]",
          "['Machine learning is an AI that essentially learns by itself and it improves by ingesting more data to perform a task with better efficiency and accuracy. That\\'s kind of what I think of it. That\\'s why when people start talking about training data, their eyes glass over, but it is about that. You\\'re just educating this AI, helping it to learn, like what a cat looks like. So that, when you say, \"Is this a cat?\", it knows what a cat looks like, and that\\'s what we see in image recognition and stuff like that. That\\'s why you can look for cats on the internet. It\\'s just that, more and more data, but the downside of it is, you\\'re assuming that what you\\'re feeding it is accurate and complete. | Yeah. It\\'ll definitely change the whole algorithm. Switching gears a little bit, we\\'re going to start talking about youth. What are your thoughts about youth learning about AI or machine learning?']",
          "['Trying to think. The first thing that jumps to my mind is that any of these algorithms is responding to you and your digital choices, I guess. The importance of their data, and I know the erosion of privacy or the apathy towards the erosion of our privacy is continuing. I\\'m trying to think. I\\'m not sure how to teach people about things that are going to... or how to teach the youth about issues of ethics and bias because being involved in that conversation, I don\\'t know. It requires, like we were talking about... I don\\'t know, it\\'s just hard to teach kids, I feel like. Those are hard topics to broach. And so, I\\'m thinking that might be something restricted to like high school, but I don\\'t know. The sooner you start teaching things to kids the better, because they have continual reinforcement throughout all of their years. I don\\'t know. What do you think? How do we teach kids concepts like that? | Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "['Yeah. It\\'ll definitely change the whole algorithm. Switching gears a little bit, we\\'re going to start talking about youth. What are your thoughts about youth learning about AI or machine learning? | I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic.']",
          "[\"Starting with logic trees and then some state machine logic, things like that. | Yeah, absolutely. Kids are getting Chromebooks in second grade now. The schools are distributing. We just talked about our children, interacting with Alexa that's a intelligent system, so.\"]",
          "['I think it\\'s important, but I think it\\'s important from two perspectives. The thing I think it\\'s more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it\\'s very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society\\'s going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there\\'s it\\'s good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it\\'s a two front benefit from it. | Yeah. I think my full pathâ€¦ My mother\\'s an educational therapist who works at a high school/middle school. So I\\'ve talked about this a fair bit. I always think if I start with general coding, it starts in second to third grade and I think you can start introducing levels of machine intelligence as early as fifth grade, because machine intelligence doesn\\'t have to be demonstrated from a coding perspective. It can be demonstrated from just what are these things? What do they do? What do they look like? And the benefit is it\\'s an interesting part about it. There\\'s so many really good abstractions of coding principles. I said earlier, I said, \"oh a big coding thing for machine intelligence is something called a random forest. And a random forest is called that because it\\'s a bunch of binary decision trees grouped together and so they call it a forest.\" Computing and AI and all those things are already filled with a very large amount of abstract representations of what they are. And so translating that and it\\'s because a lot of us aren\\'t very smart and so it makes it easier to teach us, but there are also things that make it easier to lower the barrier of entry for a lot of these concepts, like talking about a decision tree from the perspective of an actual tree makes it easier for people to understand. So I think you can at least get that high level interest in there at fifth grade, or maybe even slightly earlier with fourth grade, just from a concept perspective under the assumption that there\\'s already some computing knowledge before that. And then, I think from a coding perspective, getting in there at late middle school is always my ideal because then it\\'s, \"oh this is something that you can pursue further in high school, but also you are smart enough right now to understand that.\" And that\\'s because I also think that as someone who did a lot of coding, a big issue with learning to code complex things is, for me it was a lot tied to my math maturity. So I found that as I gained greater math maturity, I also gained a lot better ability just to understand coding quickly. And so in terms of hitting the hardcore coding concepts of it, I wouldn\\'t want to do it before that math maturity, is at least somewhere. I felt like I started to get that in late middle school. So I think that\\'s the time to do that.']",
          "[\"Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important. | Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.\"]",
          "[\"It's the same bucket. If it's a random person on the street, one, why the fuck are they talking to you about it, machine learning. Usually when we have that discussion, the technical discussion of AI versus machine learning, machine learning might be seen as a sub-case of AI a broad category and machine learning being a set of particular techniques for how we optimize AI. So having the machine learn from data, again, a big metaphor about comparing the human brain to computers, which is a bad thing to do for your own wellbeing. We aren't machines. Machines don't think like us. We call it... this is from Ellen Ullman's Close to the Machine, fantastic book. It's a memoir. I have it with me. And she talks about how we call it the machine. We call it a memory, but it's not right. So one, I don't like the name machine learning when people use it too much to compare to how babies. I'm like no, fuck, that's not it. But a way of optimizing around data, and so it's just really cool statistics. And it's really close to data science, where you're like you have data, but what meaning can you get out of this for a social reason? What's blind on this? I don't really care about the difference between, but then AI a more general label. If I have a bunch of if/then statements. Is that AI? Well, yeah, it used to be. We were trying to solve like chess originally or checkers. We had some pretty simple sequence of steps because that's how we thought the human brain worked or at least that area thought things worked. So I don't know. I use them broadly because I don't think the specifics matter. It's when we get to the ethics side of things. | No that's great, thank you. So this might go more into some of your ethics background, but how do some advances in AI or machine learning help humans?\"]",
          "[\"I think you can introduce it pretty young depending on how you do it. I taught a fourth and fifth grade after school class on computer science and programming and we were just kind of learning about how to do programming and scratch and they were making little games, but it was more about kind of how do you... It's about computational thinking, right? It's how do you break a problem down and think of a solution step by step and those are pretty young kids. And I think the idea of extending that to how can you think about machines or computers that are intelligent and what does it mean to kind of have a system learned from examples rather than telling it instructions step by step? I think you can start to kind of explain those concepts at a pretty young age, at a high level. You don't need to get into the weeds of a deep learning algorithm or anything even at the high school level. I don't think it's important to kind of get into the details of how any particular ML algorithm is built. But I think it's important to think about just what does it even mean to have a machine learning model and what does it mean to learn from data and learning about the importance of knowing where the data comes from and blah, blah, blah. So I think it could be pretty early. | I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that.\"]",
          "[\"Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important.\"]",
          "[\"I think it's wildly important. I don't think that programming should be something you go to college for. I think this is something that everybody should learn. It literally should just be like typing class. And I think AI should be... I don't know. Because some people, I don't know, I don't want to make people go that deep into something that they might not be that interested in. You know, I think everybody should learn programming because I think it'll become somewhat of a basic skill too. But actually, you know what, we got AI that are starting to program now and you got the discussion that manual programming is going to become obsolete because AI can program everything. So, that just kind of speaks to the complexity of what AI represents. I think it should be included more in the curriculum. I'll meet in the middle. I think it should be included more in the curriculum. I might not put it as an entire class. However, I do think that children should be given many more opportunities to learn about these technical things than they currently are. I think we should give them more to challenge them with. I think kids can do a lot more than we think they can.\"]",
          "[\"I think it's going to be a lot easier than we think just because they're so ingrained in the technology itself. My daughter is just about to turn one and she can already swipe on a tablet because she's watches her brother swipe on a tablet. And I think just those simple movement to think, they're going to be so more used to it than we are getting used to it. I don't think it'll be that much of a stretch just relating it to the devices they already use or have , I mean, even Alexa is a type of AI, because it does learn things about you, it learns to pick up your voice and your inflections and specific words and what you like. So, I mean just being able to talk about the everyday devices that they use is going to be easy for them. | Yeah, I mean, I just have to be broad as close to them as they can like little things like, hey, you can't turn on this device in your sister's room and listen, because there's a privacy implication there. Just don't make it about huge asylum impact. It's got to be close ecosystems to themselves.\"]",
          "[\"Definitely. Yeah. Okay. So just shifting a little bit. This is more directly related to some of the work that we do in my lab. What are your thoughts about youth learning about artificial intelligence or machine learning even as young as elementary or middle school? | I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids.\"]",
          "[\"No. I think what you're doing is important. I think that AI, machine learning, reinforcement learning, neural networks aren't going anywhere. They're going to become a bigger entity and play a bigger role in society as time goes by. So the people that are really going to matter in terms of making sure that this works for other people, our youth, their kids, they're the ones that are going to actually have the big impact on being able to change this and understand it at a better level then people that are our age it. It starts with the education of the things that we're talking about, understanding how this can go wrong, understanding the errors so you're educated on that and you can avoid that. In many ways, I think what people like me and a lot of people that are similar to me, we just try to put bandaids on this because we're too far along on the road. And youth have the ability to not just put bandaids on things, they can actually fix things, I think. | Yeah. Yeah. I think so too. And they're so creative, the kids that ... All kids really, but the kids we're working with are just phenomenal. And once they get into it, they design such amazing things to help people. They're really interested in designing robots to help and robots for social good. So they're really understanding this stuff. And even if the goal is not for them to go be computer scientists or to go build these, but to have that fundamental understanding so they can be critical consumers, so they stop and say, wait, this is wrong. I need to say something. That's what we're hoping, obviously. I mean, we're not going to follow them, but we're hoping that what we do has a little bit of that impact anyway.\"]",
          "['That\\'s great, that\\'s a good perspective. So what do you think about youth learning about the ethical issues of AI? | AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "[\"I think to not be fearful of it, but to understand at its core what it is and how it works, so that there's no fear, for one thing. And then, when they're learning that, that they can also, I think, open up their imaginations to what it could be used for, for the benefit of global society because I think what's been happening with technology is, it used to be inaccessible to people who had great ideas, and now with things like cloud computing and tons of open source software, some very specialized around AI, if you have a good idea, you can try some things out. If you get everybody's brain in the game, we have a lot of serious problems that we need to address, and I think young people need to understand that no one's going to solve those problems, and you need to think about... I should say, other people are not always going to solve the problem. You have the ability to solve some of these problems or to contribute to the knowledge. We have to start addressing these difficult global challenges, and we have to stop, I think, focusing so much on things like TikTok. Just think about all the technology that went into building TikTok. It's like, I'm sure there are valid and great use cases for TikTok, but I would say by and large, from my small sample, this is kind of ridiculous. It's entertaining, and you get that dopamine hit. I understand that, but I just think, of these technologists that develop things like TikTok or Facebook, I'm like, we probably could have ended food distribution problems or just poverty, environmental concerns, but instead, we have TikTok. It's kind of depressing a little bit. I'm hoping that you get to the youth and have them understand this. It can only be a good thing, I think.\"]",
          "[\"For kids, especially younger, the more tactile they can get on anything the better and the more able they are to really pay attention. So just if we can make physical representations of those trees and the logic and things that move to the better. So one of my sons teach preschool teacher, they have were teaching them shapes and they have like shaped man who gets on the floor and they put them all the pieces together to make shapes and patterns and things. You can do the same thing with logic trees and decision making and then even understanding things like privacy, where you know how far something spreads or what you're taking. So just the more tactile you can really make something for kids at that age the better. | I think it's going to be a lot easier than we think just because they're so ingrained in the technology itself. My daughter is just about to turn one and she can already swipe on a tablet because she's watches her brother swipe on a tablet. And I think just those simple movement to think, they're going to be so more used to it than we are getting used to it. I don't think it'll be that much of a stretch just relating it to the devices they already use or have , I mean, even Alexa is a type of AI, because it does learn things about you, it learns to pick up your voice and your inflections and specific words and what you like. So, I mean just being able to talk about the everyday devices that they use is going to be easy for them.\"]",
          "[\"That's awesome. I love that. What are your thoughts about youth learning about AI or machine learning? | I think it's wildly important. I don't think that programming should be something you go to college for. I think this is something that everybody should learn. It literally should just be like typing class. And I think AI should be... I don't know. Because some people, I don't know, I don't want to make people go that deep into something that they might not be that interested in. You know, I think everybody should learn programming because I think it'll become somewhat of a basic skill too. But actually, you know what, we got AI that are starting to program now and you got the discussion that manual programming is going to become obsolete because AI can program everything. So, that just kind of speaks to the complexity of what AI represents. I think it should be included more in the curriculum. I'll meet in the middle. I think it should be included more in the curriculum. I might not put it as an entire class. However, I do think that children should be given many more opportunities to learn about these technical things than they currently are. I think we should give them more to challenge them with. I think kids can do a lot more than we think they can.\"]",
          "[\"That is essential, yes. You kind of touched on this, but what do you think is important for youth to know about AI or machine learning? | I think to not be fearful of it, but to understand at its core what it is and how it works, so that there's no fear, for one thing. And then, when they're learning that, that they can also, I think, open up their imaginations to what it could be used for, for the benefit of global society because I think what's been happening with technology is, it used to be inaccessible to people who had great ideas, and now with things like cloud computing and tons of open source software, some very specialized around AI, if you have a good idea, you can try some things out. If you get everybody's brain in the game, we have a lot of serious problems that we need to address, and I think young people need to understand that no one's going to solve those problems, and you need to think about... I should say, other people are not always going to solve the problem. You have the ability to solve some of these problems or to contribute to the knowledge. We have to start addressing these difficult global challenges, and we have to stop, I think, focusing so much on things like TikTok. Just think about all the technology that went into building TikTok. It's like, I'm sure there are valid and great use cases for TikTok, but I would say by and large, from my small sample, this is kind of ridiculous. It's entertaining, and you get that dopamine hit. I understand that, but I just think, of these technologists that develop things like TikTok or Facebook, I'm like, we probably could have ended food distribution problems or just poverty, environmental concerns, but instead, we have TikTok. It's kind of depressing a little bit. I'm hoping that you get to the youth and have them understand this. It can only be a good thing, I think.\"]",
          "[\"Actually, I was surprised. I remember when I learned programming, I think I started learning programming after I went to college. The first year freshman, I feel like currently kids, they learn. They actually have the channels to interact with different technologies really early. They probably have a Google home or Siri on their mom or dad's phone or like their iPad or so. So I feel they're exposed to those advanced technologies, including machine learning or artificial intelligence a lot. And I feel like, I'm not sure, I know high school kids definitely learn programming. I'm not totally sure about middle school kids, but I would say even if it's not in school, they still have a lot of chance to interact with technologies. Not necessarily to learn how it works, but more just to get to know it. And I currently don't see, at least I haven't really thought about the harm of they learning machine learning at a really young age. There might be [inaudible 00:17:12] issues. I'm not sure, but I think they might trust more than we do. I don't really trust the Google, not trust, it's more like, I just don't feel very comfortable having Google home listening to my voice, but they probably got the young generation because they interacted with technologies much earlier than we did. So that might have built the trust in machine learning or artificial intelligence.\"]",
          "[\"I think it's wildly important. I don't think that programming should be something you go to college for. I think this is something that everybody should learn. It literally should just be like typing class. And I think AI should be... I don't know. Because some people, I don't know, I don't want to make people go that deep into something that they might not be that interested in. You know, I think everybody should learn programming because I think it'll become somewhat of a basic skill too. But actually, you know what, we got AI that are starting to program now and you got the discussion that manual programming is going to become obsolete because AI can program everything. So, that just kind of speaks to the complexity of what AI represents. I think it should be included more in the curriculum. I'll meet in the middle. I think it should be included more in the curriculum. I might not put it as an entire class. However, I do think that children should be given many more opportunities to learn about these technical things than they currently are. I think we should give them more to challenge them with. I think kids can do a lot more than we think they can. | You know there's different ways to like introduce these things like you can introduce you know the concepts of AI to elementary school or elementary aged children, and then you know, just as they grow in the middle school and high school.  Those concepts can be expanded upon and elaborated, and you can get into those details.  And the specifics like when I substitute teacher who there I can't remember her name, I think it was very she has shared like technology class and she was a you know it's like physical programming, where you have the little robot that you guys along the line you know so like you know, an elementary school, you can-Yeah, you can talk to him about what.  You know what these.  Like how basically how these devices function, you know, probably not get super in depth with it, but like you know, once you get a middle school, you can start talking to them about  more advanced aspects, like the you know physical programming that my name is Barry was trying to show them, and you know, in the in the high school, you can really kind of get down to the nitty gritty.  So I think I think it should be taught at all ages, but obviously it should just follow the curve of development\"]",
          "['Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "[\"Yeah, I learned that a little bit the hard way, but just one mistake and then we fixed it. Okay, great. So imagining it sounds like you think it is important to teach kids about both those things like AI concepts, but also maybe some of the ethics around privacy and other issues and maybe getting more important as the years go on. Do you know of or can you think of any specific tools or activities or ways to engage youth in both of those things? | I think it's going to be a lot easier than we think just because they're so ingrained in the technology itself. My daughter is just about to turn one and she can already swipe on a tablet because she's watches her brother swipe on a tablet. And I think just those simple movement to think, they're going to be so more used to it than we are getting used to it. I don't think it'll be that much of a stretch just relating it to the devices they already use or have , I mean, even Alexa is a type of AI, because it does learn things about you, it learns to pick up your voice and your inflections and specific words and what you like. So, I mean just being able to talk about the everyday devices that they use is going to be easy for them.\"]",
          "['I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic. | I think to not be fearful of it, but to understand at its core what it is and how it works, so that there\\'s no fear, for one thing. And then, when they\\'re learning that, that they can also, I think, open up their imaginations to what it could be used for, for the benefit of global society because I think what\\'s been happening with technology is, it used to be inaccessible to people who had great ideas, and now with things like cloud computing and tons of open source software, some very specialized around AI, if you have a good idea, you can try some things out. If you get everybody\\'s brain in the game, we have a lot of serious problems that we need to address, and I think young people need to understand that no one\\'s going to solve those problems, and you need to think about... I should say, other people are not always going to solve the problem. You have the ability to solve some of these problems or to contribute to the knowledge. We have to start addressing these difficult global challenges, and we have to stop, I think, focusing so much on things like TikTok. Just think about all the technology that went into building TikTok. It\\'s like, I\\'m sure there are valid and great use cases for TikTok, but I would say by and large, from my small sample, this is kind of ridiculous. It\\'s entertaining, and you get that dopamine hit. I understand that, but I just think, of these technologists that develop things like TikTok or Facebook, I\\'m like, we probably could have ended food distribution problems or just poverty, environmental concerns, but instead, we have TikTok. It\\'s kind of depressing a little bit. I\\'m hoping that you get to the youth and have them understand this. It can only be a good thing, I think.']",
          "[\"I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids.\"]",
          "[\"Yeah. Yeah. I think so too. And they're so creative, the kids that ... All kids really, but the kids we're working with are just phenomenal. And once they get into it, they design such amazing things to help people. They're really interested in designing robots to help and robots for social good. So they're really understanding this stuff. And even if the goal is not for them to go be computer scientists or to go build these, but to have that fundamental understanding so they can be critical consumers, so they stop and say, wait, this is wrong. I need to say something. That's what we're hoping, obviously. I mean, we're not going to follow them, but we're hoping that what we do has a little bit of that impact anyway.\"]",
          "[\"Geez. Hopefully they help everyone. I don't know. I mean, I just see it as, it's like any technology, I mean it has the potential to increase human productivity. It has the potential to increase human collaboration, creativity. I don't know. It's hard to answer because I think there's so many different kinds of technology that you could lump under AI and it's almost like who's helped by computers? | Awesome.  I'm kind of switching gears a little bit, what are your thoughts about youth learning about AI and machine learning?\"]",
          "['I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic.']",
          "['Yeah. I think at a high level, my generation, it was always, for your education, you really need to understand reading and writing and arithmetic, math. I think to add to that, now you have to understand computational thinking. You have to understand just some basics of how computers work, things like that, just basic stuff, because you just have to. You probably grew up in a world that already had the internet, that already had things like GPS, that already had smartphone, or something close to it. I didn\\'t. My kids, they think I [inaudible 00:19:33] prehistoric times or something. It\\'s like cave man, how did you get anything done? But now, everybody that\\'s being born is born into a world where computing is everywhere. It\\'s not just reading and writing, and arithmetic. It\\'s reading, writing, arithmetic, and computational thinking, understanding how stuff works. In the spirit of that, you have to understand, I think at least conceptually, what AI is, and what it is not presently. We\\'re in no danger of robots taking over the world anytime soon. | I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic.']",
          "[\"I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids. | Yeah. As, so as someone who works so closely with AI, can you imagine an elementary school student or a middle schooler or even younger than that, what sort of, how could you break that down? Like AI or machine learning to get them kind of exposed to that?\"]",
          "[\"Oh, demographically. I think I did actually, the first thought was people who are pretty good at programming and math probably, and who are interested in creating really advanced technology that's very smart. I can think about humans and demographic. I don't actually really have a thought of a specific group who design those. I would say just people who are good at programming and math or either one of those. | Actually, I was surprised. I remember when I learned programming, I think I started learning programming after I went to college. The first year freshman, I feel like currently kids, they learn. They actually have the channels to interact with different technologies really early. They probably have a Google home or Siri on their mom or dad's phone or like their iPad or so. So I feel they're exposed to those advanced technologies, including machine learning or artificial intelligence a lot. And I feel like, I'm not sure, I know high school kids definitely learn programming. I'm not totally sure about middle school kids, but I would say even if it's not in school, they still have a lot of chance to interact with technologies. Not necessarily to learn how it works, but more just to get to know it. And I currently don't see, at least I haven't really thought about the harm of they learning machine learning at a really young age. There might be [inaudible 00:17:12] issues. I'm not sure, but I think they might trust more than we do. I don't really trust the Google, not trust, it's more like, I just don't feel very comfortable having Google home listening to my voice, but they probably got the young generation because they interacted with technologies much earlier than we did. So that might have built the trust in machine learning or artificial intelligence.\"]",
          "[\"Potentially everyone. I think there's a lot of promise and therapy and education and lots of places. | Awesome.  I'm kind of switching gears a little bit, what are your thoughts about youth learning about AI and machine learning?\"]",
          "[\"I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids. | I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "['Yes, venturing indicates well is a different story. So what age do you think students or young people should start learning about AI and machine learning? | AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "[\"Yeah, I mean, I just have to be broad as close to them as they can like little things like, hey, you can't turn on this device in your sister's room and listen, because there's a privacy implication there. Just don't make it about huge asylum impact. It's got to be close ecosystems to themselves.\"]",
          "[\"Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas? | Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.\"]",
          "[\"I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "[\"Yeah. As, so as someone who works so closely with AI, can you imagine an elementary school student or a middle schooler or even younger than that, what sort of, how could you break that down? Like AI or machine learning to get them kind of exposed to that? | I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "['Yeah. That makes a lot of sense. What do you think is the most important things that you should know about with AI and machine learning? | Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "['Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty. | I mean, just pull anything out. I mean, a phone. This Google Home I have here, AI is everywhere. There is not a thing... I\\'m sure there\\'s an AI processing algorithm in my camera right now that is doing something with my face. Any browser, your email. I mean, it\\'s everywhere. Getting them to connect that training data and all this data that AI uses to continually teach itself and learn is coming from you and it\\'s coming from everywhere. That might be a little scary for them to think about. But I think it\\'s important to know.']",
          "['Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.']",
          "[\"I think it's important, but I think it's important from two perspectives. The thing I think it's more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it's very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society's going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there's it's good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it's a two front benefit from it.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "0_programming_ai_kids",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "0_programming_ai_kids"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          6.518843173980713,
          7.3213791847229,
          8.001652717590332,
          7.467591285705566,
          6.414478302001953,
          7.919892311096191,
          6.570053577423096,
          6.401341438293457,
          7.299810409545898,
          8.059905052185059,
          6.710376262664795,
          8.871356010437012,
          6.50956392288208,
          7.703597068786621,
          6.5987114906311035,
          7.647505283355713,
          6.8050761222839355,
          6.355661869049072,
          7.684714317321777,
          6.309948921203613,
          7.924321174621582,
          6.501916885375977,
          6.3654656410217285,
          6.99587345123291,
          6.374532222747803,
          7.345966815948486,
          6.3137311935424805,
          6.451375961303711,
          7.029313564300537,
          6.303325653076172,
          8.867878913879395,
          7.97996187210083,
          6.337076187133789,
          6.468420028686523,
          7.59909725189209,
          6.485291481018066,
          6.3296332359313965,
          6.2285943031311035,
          6.4540910720825195,
          6.895106315612793,
          6.419849395751953,
          6.429513931274414,
          6.950306415557861,
          8.141989707946777,
          7.9928388595581055,
          6.497422218322754,
          6.410664081573486,
          8.850361824035645,
          8.874244689941406,
          7.655575275421143,
          6.598841190338135,
          7.083216190338135
         ],
         "y": [
          11.579556465148926,
          12.154730796813965,
          12.043624877929688,
          12.07498836517334,
          10.327831268310547,
          12.063300132751465,
          10.747723579406738,
          11.675886154174805,
          12.212640762329102,
          12.001413345336914,
          10.265426635742188,
          11.646409034729004,
          10.253894805908203,
          11.682973861694336,
          10.72038745880127,
          12.241569519042969,
          10.029929161071777,
          11.518939971923828,
          12.176200866699219,
          11.079895973205566,
          12.019720077514648,
          11.478452682495117,
          10.60824203491211,
          10.08730697631836,
          10.61144733428955,
          12.174701690673828,
          11.075757026672363,
          10.598530769348145,
          11.83938217163086,
          11.128257751464844,
          11.660913467407227,
          11.904582023620605,
          10.400671005249023,
          11.528777122497559,
          12.23361587524414,
          10.119364738464355,
          10.303701400756836,
          10.740184783935547,
          11.640748977661133,
          11.884991645812988,
          10.222576141357422,
          11.639580726623535,
          10.155312538146973,
          11.789389610290527,
          12.140385627746582,
          11.826472282409668,
          11.736676216125488,
          11.616698265075684,
          11.663159370422363,
          12.264778137207031,
          10.766800880432129,
          11.340363502502441
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah. It's still, I think, very early for AI, but at small companies, if done properly, small companies can leverage AI to be bigger, and for a big company, you can definitely leverage AI to keep growing and to be more competitive. So, certainly at places like Morningstar, where there is so much data to be processed, we applied AI in a wide variety of fashions. We really focus on providing independent investment data and research. Mostly, that analysis is done by human experts, but there are times where there's so many possible investments to cover that you have to consider something like AI to look at all the information about a particular investment that may not be that well known or very popular, but for completeness, a company like Morningstar wants to provide some insights. Oftentimes, we will experiment with AI to consume all that available information and generate some information that a human analyst could also generate, but maybe doesn't have the time to. There's lots of applications for AI all over the place.\"]",
          "[\"Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools.\"]",
          "[\"And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there.\"]",
          "[\"Yeah. So there's code blue resuscitations in hospitals. I think you guys are probably familiar with that, when somebody goes into cardiac arrest. Whenever that happens, an alarm goes off in the hospital and everybody that is assigned to that code blue that day is supposed to, in theory, stop what they're doing and run to that room and help triage a patient in real time. And it's one of the best instances that I've ever studied of human-human teamwork. It's an absolute chaotic environment where you have extreme time pressures. People don't have, they have information disparities, inaccuracies, they don't know what's going on really so they're trying to understand at an individual level, what do I do? What do my team members do? Leadership is needed. One of the biggest problems that you see within those is what I was just saying, is information disparities and information needs not being able to access that very quickly. So in the communities I've been talking to regarding this, there's the development of utilizing robots and intelligent agents to aid in that collaborative decision making process, where the intelligent agent is providing, it's looking at all the data on the human, because as a human, we give a lot of biophysical data and can't possibly look at all of that in real time. It takes us a long time to go through all that. So an AI agent that has been trained on an algorithm to look at peaks and valleys of all of that, and then flag it can be trained to provide real time relevant information to that team when they need it. They don't have time to dig through the data, where the AI can dig through all that real time data and alert you of multiple things that are going wrong so you can better understand the medical operational environment. That's an example of where we're heading towards. | Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"So in work, we mainly test actually how humans react to AI with different attributes, kind of like human, maybe talk a lot or maybe do a lot or different features. And sometimes actually we would use a wizard [inaudible 00:08:08] just because AI teammate means they need to coordinate at a higher level instead of just like, okay, I get the data and analyze it. I give you a prediction result. So it's more than that. And that's why a lot of times we would use wizard of OZ to try to make sure it's kind of controlled, all the variables, the confounding variables are controlled, especially in experiments. And so in addition to that, in terms of [inaudible 00:08:41] we have use. Normally it's the machine learning algorithm that's used to kind of give them an instruction in terms of how they react in different situations. And in terms of AI in personal life, I actually don't really trust AI that much. Siri on my phone is always off. I don't really use it and I don't have a Google home. I just don't feel very comfortable that it listens to my voice all the time. Although I'm not saying anything that cannot be heard, it's kind of a privacy issue, I guess. For instance, if my family member really wants to have one, I would be fine with it. Although I personally would not choose to have one, if that makes sense?\"]",
          "[\"And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there. | Yeah. That's great. Like simulating that environment a little bit closer to what we had in person potentially.\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate. | Yeah, definitely. Those are great examples of ways that AI have helped us. So kind of on the reverse, what are ways that maybe AI has harmed us and who in particular have they harmed?\"]",
          "[\"I know you provided an example with games because that's probably what you work on, but do you know of any other examples with AI teaming that's outside of the gaming world? | So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate.\"]",
          "[\"And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there. | Yeah. I mean, so like my young son, if we're doing something like virtual learning, he's got to be on virtual learning for that day or that week. It's very difficult for him to pay attention to just a screen for longer than like 10 to 15 minutes. But maybe if it was like a very immersive environment where you had some AI students that were almost collaborating with you, it might be something he could engage in and get a lot more from that environment.\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate. | Yeah, that makes sense. That makes sense. Interesting. So I noticed you just mentioned maybe using AI to help humans. Do you know of ways that AI have helped humans and like what groups of humans have they been able to help?\"]",
          "[\"Sure. Basically, I have not been officially in the working world yet. I did my undergrad in psychology in Clemson and graduated December of 2018. Then my current grad program, which is in human-centered computing, did not start until the fall. So, I was a substitute teacher at a middle school for that whole semester. And then over the summer, I just kind of take a break, went to Europe, all that good stuff. Then I started in the fall and I've been a research assistant ever since, working on human-AI teaming projects. Specifically, my research is on team cognition in human-AI teams.  I don't know, we can get more into the weeds with that, but the only other professional work experience I have is working at a light during internship my junior year of undergrad. And they did benefits management stuff. To be honest with you, I know this is a bad AI. Basically, their entire company could go away because they... I don't know, my job, it was just so much data management and it's crazy because they hire people with four-year degrees to do these jobs that you could basically do a Python script for. | That's interesting. How did you actually... I know you said you had a psychology degree from Clemson and then you kind of moved into the human-centered computing program. What made you interested in that? How did you become interested in working with AI and machine learning?\"]",
          "[\"So I'm currently a fourth year PhD student in human centered computing, where we basically just study how humans interact with different technologies. And in our lab, our research focus is mainly human AI teams, human AI teaming. So we basically study how humans interact with AI teammates in a given environment like gaming, where AI is pretty common to see. And before that I did my bachelor and master in engineering. So basically it's kind of like the algorithm behind the thing. That's basically my background. | Would you mind telling me a little bit more about AI teaming?\"]",
          "[\"Okay, great. So is human AI teaming a mix of AI agents and humans working together to complete a task? | Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools.\"]",
          "[\"Yeah. And even just the tendency, like one thing I was discussing in a research project I'm working on right now is that a lot of people didn't trust an AI to work in a risky decision be because they thought it'd be too logical. Like it's going to make a decision based off what the exact probability of something happening is versus what a human in this scenario is probably going to start thinking of the worst case as possible and they may be tangentially probable, but it might be so bad that they're not going to make a decision. And that's part of a human emotional factor that's not going to be replicated by a machine. | And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there.\"]",
          "[\"Sure. Basically, I have not been officially in the working world yet. I did my undergrad in psychology in Clemson and graduated December of 2018. Then my current grad program, which is in human-centered computing, did not start until the fall. So, I was a substitute teacher at a middle school for that whole semester. And then over the summer, I just kind of take a break, went to Europe, all that good stuff. Then I started in the fall and I've been a research assistant ever since, working on human-AI teaming projects. Specifically, my research is on team cognition in human-AI teams.  I don't know, we can get more into the weeds with that, but the only other professional work experience I have is working at a light during internship my junior year of undergrad. And they did benefits management stuff. To be honest with you, I know this is a bad AI. Basically, their entire company could go away because they... I don't know, my job, it was just so much data management and it's crazy because they hire people with four-year degrees to do these jobs that you could basically do a Python script for.\"]",
          "[\"So one job I had, when I was on a team, we would go out to different sites of critical infrastructure and basically assess their security posture and detect if they had any breaches or things like that and that all I've also done instant response missions, where a corporation may detect that they've had a breach of their security defenses and we come in and figure out where it came from, how to fix it and get rid of all the traces that are still on their systems or the networks. | So something, we noticed that a lot of the tools, so to speak that we use in computer security are very automated and are going towards the trend of what you would consider AI autonomous agents and I started to become interested in Dr. Nathan McNeese work on human AI teaming because those systems are almost getting to the point in computer security where they would have a full team role. And just trying, I wanted to start looking into the factors that would make those teams more successful as those tools become more and more autonomous.\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate.\"]",
          "[\"It's definitely different than undergrad. I got my undergrad at Clemson too, in genetics. So, I understand. I'm doing a similar thing as you. Would you tell me a little bit more about your AI teaming and cognition, your research, what you're studying? | Sure. All the research I've done up to this point has been looking at basically what does team cognition currently look in human-AI teams? I did a study on the role of spatial awareness or the availability of spatial information. And then I did another study that was essentially just looking at, it was pretty exploratory in the fact that I just studied or I just collected measures of shared mental models, along with trust and a bunch of qualitative data. So, for the objective measure, I say objective, but it's hard to actually measure a construct like that objectively. We didn't get anything on the objective measures, but the qualitative data that we got was really interesting. The perceived team cognition too is lower when you're working with AI, which I thought was pretty interesting. And then, there was lower trust as well. And it gets worse when you work with two AIS and you're the only human. When you become a minority member of the team, versus when it's two humans and one AI. So, that's where my research has been. And basically what I want to move into is how to develop, or figuring out what qualities an AI can have that is going to best support shared understanding between humans in a human-AI team. And then going that extra step and creating basically a shared mental model of the AI teammate and its operation. And that sounds a lot like AI explainability and transparency, but I think you take it a step further in making sure that both human teammates have a shared understanding of that. You know, I can go deep into the weeds in terms of the theory behind it, because you got task mental models and team mental models, and I'm kind proposing that there should be a AI teammate mental model that the humans can share, but basically developing AI that are going to support the more traditional aspects of shared understanding for humans. And then also developing them in a way that makes it easy for the humans to develop a mental model of the AI teammate, because you know, working with AI is very different. They are very smart and very stupid at the same time.\"]",
          "[\"Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools. | I mean, right now it's a lot of just research and what their capabilities are and then trying to model what they might be able to do. So I'm designing a new experiment right now and we're going to fake the AI. We're not going to make the real system, but we have to be able to make it seem real to the person doing the experiments. There's a lot of research there about what the AI should be capable of and what the person would see and interact with the AI.\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate. | So there are several examples I think I can talk about the first one is Tesla, is a kind of AI driving. You can consider as a teamwork because their goal is to get to the destination safely or so, or you can consider it as AI, just easy to use as a tool to drive. So that could be an example of humans using AI to help them. And another one theory we use every day, Google home, that type of thing is also. We ask them okay, turn on the light or share what's the weather today. So that's another example of humans using AI to get the information and save their time. And I think also healthcare, I think I read papers before, but I don't really know examples in my life, but I think I have read that healthcare use machine learning, especially when diagnosing some images of humans. What is it called that kind of scanning pictures and can help them to diagnose whether it's benign or a bad cancer or so, and in addition to that that's most of the examples. And also I mentioned before data scientists that would use different models to help them predict things.\"]",
          "[\"So one job I had, when I was on a team, we would go out to different sites of critical infrastructure and basically assess their security posture and detect if they had any breaches or things like that and that all I've also done instant response missions, where a corporation may detect that they've had a breach of their security defenses and we come in and figure out where it came from, how to fix it and get rid of all the traces that are still on their systems or the networks. | Okay. Got it. Got it. So then how did you become interested in joining the research lab that you're in now and exploring more AI applications?\"]",
          "[\"So one job I had, when I was on a team, we would go out to different sites of critical infrastructure and basically assess their security posture and detect if they had any breaches or things like that and that all I've also done instant response missions, where a corporation may detect that they've had a breach of their security defenses and we come in and figure out where it came from, how to fix it and get rid of all the traces that are still on their systems or the networks. | Okay, great. So is human AI teaming a mix of AI agents and humans working together to complete a task?\"]",
          "[\"Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools. | That's so interesting, like this hybrid task force and like what the responsibilities should be of the AI and what the responsibilities should be the human. I think that's fascinating. Okay. So more about you personally. So what sort of activities related to AI would you do in a typical week in your position now?\"]",
          "[\"Yeah. So there's code blue resuscitations in hospitals. I think you guys are probably familiar with that, when somebody goes into cardiac arrest. Whenever that happens, an alarm goes off in the hospital and everybody that is assigned to that code blue that day is supposed to, in theory, stop what they're doing and run to that room and help triage a patient in real time. And it's one of the best instances that I've ever studied of human-human teamwork. It's an absolute chaotic environment where you have extreme time pressures. People don't have, they have information disparities, inaccuracies, they don't know what's going on really so they're trying to understand at an individual level, what do I do? What do my team members do? Leadership is needed. One of the biggest problems that you see within those is what I was just saying, is information disparities and information needs not being able to access that very quickly. So in the communities I've been talking to regarding this, there's the development of utilizing robots and intelligent agents to aid in that collaborative decision making process, where the intelligent agent is providing, it's looking at all the data on the human, because as a human, we give a lot of biophysical data and can't possibly look at all of that in real time. It takes us a long time to go through all that. So an AI agent that has been trained on an algorithm to look at peaks and valleys of all of that, and then flag it can be trained to provide real time relevant information to that team when they need it. They don't have time to dig through the data, where the AI can dig through all that real time data and alert you of multiple things that are going wrong so you can better understand the medical operational environment. That's an example of where we're heading towards. | Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight.\"]",
          "[\"Yeah. It's still, I think, very early for AI, but at small companies, if done properly, small companies can leverage AI to be bigger, and for a big company, you can definitely leverage AI to keep growing and to be more competitive. So, certainly at places like Morningstar, where there is so much data to be processed, we applied AI in a wide variety of fashions. We really focus on providing independent investment data and research. Mostly, that analysis is done by human experts, but there are times where there's so many possible investments to cover that you have to consider something like AI to look at all the information about a particular investment that may not be that well known or very popular, but for completeness, a company like Morningstar wants to provide some insights. Oftentimes, we will experiment with AI to consume all that available information and generate some information that a human analyst could also generate, but maybe doesn't have the time to. There's lots of applications for AI all over the place. | Yeah, I'm part of the teaching faculty, so most of my focus is on teaching, but another responsibility I have is I serve as the executive director for Clemson's AI Research Institute for Science and Engineering, which is a new institute that came online in the summer of 2020, so right in the middle of COVID, and is only now sort of getting going. It was founded by Dr. Feng Luo, who is a professor in the school of computing. I'm helping him realize his vision for AI RISE, is what we call it, and it really is a combination of providing educational opportunities across Clemson to train faculty and researchers and students. It's to bring in the community, the upstate and the entire state, to help educate everyone really, on AI, and what AI is because it's such an overloaded, overused term and everybody maybe thinks they know what it is, but I think everybody has probably a different idea of what it is. AI always comes up in whatever subject I'm teaching. One of the classes I'm teaching this semester is on computing, ethics, and society. So obviously, talk a lot about AI and the moral issues associated with the use of AI and the bias that is proven to be in a lot of systems that employ AI today, and it has certainly lots of positive impacts, but also a lot of negative impacts. We talked a lot about that in that class. It always comes up because it's everywhere, honestly.\"]",
          "[\"So I'm currently a fourth year PhD student in human centered computing, where we basically just study how humans interact with different technologies. And in our lab, our research focus is mainly human AI teams, human AI teaming. So we basically study how humans interact with AI teammates in a given environment like gaming, where AI is pretty common to see. And before that I did my bachelor and master in engineering. So basically it's kind of like the algorithm behind the thing. That's basically my background. | So basically the human AI teaming concept is that human and AI teammates coordinate and collaborate to finish a set of goals basically as [inaudible 00:01:32] goes. And what we have done previously is given environment norm in games and give them a series of team tasks that they need to either share information with AI or share with them and also receive information that shared by the AI teammates and finish the task. Or we have also done research where the AI has their own responsibility, but this you need to coordinate their tasks like connect with each other. So that they collaborate and finish the team go. So that's basically how we define human AI teams.\"]",
          "[\"Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "1_ai_teammates_teaming",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "1_ai_teammates_teaming"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          5.21583366394043,
          4.97788143157959,
          5.7193193435668945,
          4.813073635101318,
          5.777663230895996,
          5.722901821136475,
          3.8489177227020264,
          3.8385348320007324,
          5.725455284118652,
          3.8480782508850098,
          4.213190078735352,
          4.3931498527526855,
          4.781942844390869,
          5.844849586486816,
          4.169047832489014,
          4.9927449226379395,
          3.836082696914673,
          4.622936725616455,
          5.060861587524414,
          3.835073232650757,
          5.268923759460449,
          4.281712532043457,
          4.988269805908203,
          4.799274444580078,
          5.253909111022949,
          4.346325397491455,
          9.863598823547363,
          4.964427947998047
         ],
         "y": [
          7.015115261077881,
          7.070063591003418,
          7.144627094268799,
          7.057336807250977,
          7.292854309082031,
          7.157297611236572,
          7.353949069976807,
          7.348666667938232,
          7.157163143157959,
          7.354443073272705,
          7.584352493286133,
          7.688943862915039,
          7.088242053985596,
          7.2852463722229,
          7.569072246551514,
          7.184386730194092,
          7.338929653167725,
          7.746427059173584,
          7.004940032958984,
          7.345765113830566,
          8.06698989868164,
          7.570352077484131,
          7.068198204040527,
          7.049315452575684,
          7.0062336921691895,
          7.672627925872803,
          4.12382173538208,
          7.197976589202881
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yes, it's a data ethics course. We talk about AI a fuck ton. So besides camps, teaching people how to code, I'm a PA right now with YJ Kim here at the Wisconsin Center for Ed Research and I know David Shaffer. And through him I've heard about goal like everyone else in this community. So I hear about Clemson all the time. But where was I going? So I have a PA ship here. I've TAed here, TA for code and power a lot with Dr. Royston and that's kind of your classic critical theory approach to AI and a lot of... but like meritocracy, Google image search results for black women showing images of gorillas, those kinds of cases. Yeah, it's kind of fucked up. So talk about how code and power get gets embedded. And it's very critical around institutions and getting students to think about that stuff and learn about implicit biases and things like that. We actually have them take the implicit bias test and then think about the limitations of that test and try and reason about what data says. But-\"]",
          "['I\\'m going to bring up... I had this already. Nope, that\\'s my writing sample. I have just been writing all this down for applications to places. And I\\'m at the point where if I write something down, I\\'m going to forget it immediately, which is the opposite problem because now I\\'m not constantly thinking about it. So AI or AI ethics is a bundle of ethical issues. And so thinking about populations depends on your entry point to thinking what AI ethics is. And so I\\'m just reading my notes, honestly. We can\\'t violate public trust, is kind of one thing. That comes up a lot in the discussions around autonomous vehicles, that we want to have autonomous vehicles for X, Y, Z reason. But in order for that to happen, the public kind of has to trust in the system. We have to agree. We have to know that it\\'s not going to run over black people more than white people. If it gets down to that moment, we have to know that it isn\\'t going to confuse us, not know that a cyclist is there or the actual case of where Uber hit a woman. I don\\'t know if it was Uber or not, but there was an autonomous vehicle that hit a woman because she jaywalked. Well, jaywalking should result in a fine, if that, not in death from a car that didn\\'t see you. The idea that the road is owned by cars is a relatively new one in human history. Roads were owned by the people walking on them. And so that one, public needs trust in it and that we don\\'t want to violate that trust. So in a way we can harm everyone when we release things too early in that sense or have things that create harms. Also, the people who are literally hit by the car, I think get harmed the most. Let\\'s see, there\\'s a lot of talk around misinformation being amplified on social media. And there\\'s also talk on certain things when you have newsfeeds, like Facebook, that put the things at the top that they think you want to see or that they want you to see. That\\'s in their benefit for you to see. Or TikTok, as fun as it is to find your own extremely niche set of friends on TikTok, and ridiculously fast, what\\'s getting left out? Who\\'s getting pushed down? And so there\\'s a lot of harms that come up when misinformation gets brought up to the top and there\\'s a lot of harms that get caused when certain communities are just completely pushed down and systematically given lower scores in the algorithm. And so there\\'s a lot of talk on TikTok around trans communities and people of color, people who don\\'t look pretty getting rated lower by the algorithm, so they\\'re not going to have as much of a viewership. And so when your money is tied to being a content creator, it sucks. There\\'s also the whole thing that if you\\'re queer and online, you are subject to harassment. You always have to have a few backup accounts because one of them is going to get blocked out because people are going to mass... people who don\\'t like you are going to mass report everything you do until you get flagged by the algorithm as bad and systematically have your account removed, even though you didn\\'t do anything wrong. That happens all the fucking times with trans content creators. There\\'s like all these, \"Hey, my thing got deleted again.\" And it\\'s just happens. So there\\'s a lot of algorithms that play a part in that. But also the algorithm isn\\'t separate from the system of humans interacting with it. So fuck, I talked a long list, when it\\'s applied in places like policing, compulsory education, medicine, et cetera. Those already have a lot of scrutiny on them, legal and public scrutiny on them. So it\\'s very important that we get AI right in those cases because we\\'re constantly looking at them for whatever reason. Reason might be that we have a lot of public scrutiny on policing and education because we want a fair and just society. And we see those institutions that are very important to the function of a fair and just society. So if we fuck up there at all, everyone pays attention. Work, AI changes the nature of work, and when the nature of work changes, some people are benefited and some people are completely displaced.']",
          "[\"Yeah, there's a really thought provoking... Hang on, just let me see. It's a great thought provoking website. What is it called? Oh yeah. moralmachine.net. If you go to moralmachine.net and then click the judge link, what this is, is this research that's done by MIT and a handful of other universities, and it asks you some interesting questions because everybody's talking about self-driving cars, and how they're going to happen. They're already happening. It's going to happen, and the reason behind it is, humans are not good at driving. People always think they're good at driving, but they're not. So, if we get the self-driving stuff done correctly, even if it's not entirely self-driving, even if you're sitting in the driver's seat and have to take over, there are these interesting moral decisions to be made by the technology in the event of something unexpected. This website poses all these interesting questions about, if you were programming this self-driving car, what would you do in the situation? Your brakes fail. If you continue on your path, you're going to destroy three children in the pedestrian path. If you swerve, you're going to kill everybody in your car because you're going to crash. It's an interesting reveal of how we think and they, MIT and these other universities collect this data. I think for young people, gaming, just taking advantage of gaming technology to educate in a way that's maybe not right in your face, I think is a good way to learn. So much of learning is caught rather than taught. You just never know. | If you sit around in a room and you hear a bunch of people just talking, you learn a lot. They're not teaching you, you're just learning a lot. You're just kind of catching it. I think by employing things like gaming and some of these thought workshops, like what would you do in this case? It's interesting. That's why I really like teaching the class I'm teaching this semester, which is computing, ethics, and global society. Because some things that seem simple, are not. They're tough decisions. They may benefit some people. They're going to hurt some people. Is it the right thing to do? Even though you can do it, should you do it? It's-\"]",
          "['Okay, here\\'s how I\\'m going to think about it. In the Pasco case, I\\'ll use that example for both because it\\'s good to ground myself in a fucking case so I don\\'t go crazy. Looking at that case and thinking about how I speak to my students about AI ethics, which I speak to them the way I do because of my interviews of the 10 people I had. And they all had different views on things. It\\'s like, well, how can I do this in a way that\\'s kind of in agreement with all these different views, knowing that again, the course is going to be kind of pigeonholed the way that Allan did it. How can I introduce to things? And one of the things that people talk about a lot in the ethics, when you\\'re giving people who aren\\'t ethicists, they\\'re just a fucking programmer. You want them to think through things, is the stake stakeholders model. We just ask what\\'s at stake? But remember week one, I\\'m doing a pre-assignment where they don\\'t have any understanding what I\\'m talking about. So I can\\'t use the word stakes because they might take that in a too narrow sense. And so I say to them is, \"In this case, what are the things that we want to get right? Yes, there\\'s this really shitty thing that happened in Florida. The article ends on a very sad note, but there are things that we\\'re trying to get as humans in this. All the bad things that happened, we wanted those not to go that way, but we don\\'t want to just burn out everything to happen because again, we\\'re trying to get at things. So I have the students focus on this shared human endeavor of things we try to get right. And so in that case, the police department did a shitty thing, but they\\'re trying to get right reducing crime. They\\'re trying to right serving their community\\'s interest. They\\'re trying to get right distributing their resources well. Whether they actually got those things right in a way that still abides by fair terms is also preparation is a whole nother story, and we\\'ll let the Department of Justice... we\\'ll see what their investigation finds. But it has a lot of promise. And I try not to focus on promise because coming into viewing AI from the point of view of promise and predictions and it\\'s... you mention advancements. My first instinct there is no, let\\'s not talk about advancements first. There\\'s a huge history of technology here. There\\'s a huge history of institutions. Policing hasn\\'t always been the best and still isn\\'t always the best. And so if something\\'s being embedded there, we better really look at that history. We can\\'t look to advancement and promises of AI as a way to ignore how we got here. So I tried to avoid that framing. But we are trying to use it to get at good things. So I can\\'t say what is actually done well, but I can say we\\'re generally trying to get at good values with it. So let\\'s get that right, kind of be my call to action. And that involves also looking at the history of how we get to places. Anna Hoffman, AI ethics researcher from Northwest, I can\\'t remember where she\\'s from, but she\\'s awesome. You should look up her work. I have the idea of let\\'s not talk about entry points from promises, or futures, or potential of AI because of a talk she had. She\\'s like I\\'m in this pandemic and I\\'m tired and I\\'m sad. I\\'m a tired, stressed professor and I\\'m tired of looking at AI. So I\\'m going to come at this in a fun way, which is what does AI look like if we start from the position of infinite love for trans people? Let\\'s just take a fundamentally different approach to how we might think about AI. And it was a really fucking fun talk. But one of the things is looking at the medias around promises of AI and what gets left out when we focus too much on promises. So I\\'m doubly rambling at this point, but the things we\\'re trying to get right with AI, it\\'s good to enumerate those. And we kind of reground ourselves in the value of what we\\'re actually trying to do. And a lot of harms come out when we lose track of that and focus too much on it can just solve a lot of these things. When it\\'s put into context that it ought not be in, harms can come up. When it\\'s put into context too quickly, harms can come up. So I\\'m going to stop rambling and drink water.\\n']",
          "['I know that there\\'s a K12 curriculum that someone\\'s made for AI ethics. And so I just try to defer to that because I don\\'t want to collect K12 kids. It looks legit. They break it down in a way that\\'s not just bias. So that passes my muster of a lot of people, a lot of well meaning allies who are sometimes the most dangerous people, go jump to bias and go no further, which is why I tried to bring up all of these examples that weren\\'t bias. So there\\'s this K12 AI ethics curriculum somewhere. I think it\\'s AI K12 or something. It does a good job of helping students see things like it\\'s a computer. It has sensors. What\\'s the biology of the machine in a sense. It has these parts to it, something that students might not have ever thought about. One of my first activities I did when I taught CIT105, which was Intro to Computers for people who had never touched a computer in their lives, which was a good percentage of my students, the first thing I did is I would have a video of cats, just like a kitten livestream playing on the projector as they walked in, nice calming activity. And then once we got started, I said, \"Okay, tell me every piece of technology between these kittens and us.\" And it\\'s just illuminating seeing there\\'s this, there\\'s this part, and having them just name all the parts, just giving them permission to sit there and name the parts, I think, is important. Just kind of peel back that curtain and take things less. They start with letting students know there\\'re sensors. It makes decisions in these ways. Data\\'s collected in this way. It\\'s reasoned about in this way. Problems come up in this way, and blah, blah, blah. They spell it out very well and they have a nice breakdown of curriculum. I\\'m like this looks fine to me and I have to trust them because I don\\'t know a fuck at all about K12. | AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "[\"Yeah, I'm part of the teaching faculty, so most of my focus is on teaching, but another responsibility I have is I serve as the executive director for Clemson's AI Research Institute for Science and Engineering, which is a new institute that came online in the summer of 2020, so right in the middle of COVID, and is only now sort of getting going. It was founded by Dr. Feng Luo, who is a professor in the school of computing. I'm helping him realize his vision for AI RISE, is what we call it, and it really is a combination of providing educational opportunities across Clemson to train faculty and researchers and students. It's to bring in the community, the upstate and the entire state, to help educate everyone really, on AI, and what AI is because it's such an overloaded, overused term and everybody maybe thinks they know what it is, but I think everybody has probably a different idea of what it is. AI always comes up in whatever subject I'm teaching. One of the classes I'm teaching this semester is on computing, ethics, and society. So obviously, talk a lot about AI and the moral issues associated with the use of AI and the bias that is proven to be in a lot of systems that employ AI today, and it has certainly lots of positive impacts, but also a lot of negative impacts. We talked a lot about that in that class. It always comes up because it's everywhere, honestly. | Yeah. I think maybe some of the obvious ones that people can relate to are benefits associated with recommendations. We rely on, and we always have, even before AI, rely on recommendations to help us make buying decisions that are best for us. You see it mostly in streaming media, for example. It's like, you watch this movie or you watch this genre, you might be interested in this series, and it is helpful, definitely is helpful, and then when we buy stuff, same kind of thing. People are looking at this, they ended up buying this. You were looking at this, maybe you'd be interested in this. It's subtle. It's kind of built in and it helps us make these important or maybe not so important decisions where we spend our money and things like that, and that affects so many people, and our reliance on recommendation systems has been around forever, and I think AI has really tapped into it. It's very freaky, and the other thing is, a lot of companies use AI just to speed up what they can do and increase their capacity. Screening resumes, looking for the right candidates to hire. There's so many opportunities out there. So much of the workforce is in motion right now and companies that are hiring to keep up, they employ different AI solutions that they probably bought from somebody else, who decided, hey, it's probably good business for me to do this. Those are just a couple examples.\"]",
          "['I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI.  | It\\'s the same bucket. If it\\'s a random person on the street, one, why the fuck are they talking to you about it, machine learning. Usually when we have that discussion, the technical discussion of AI versus machine learning, machine learning might be seen as a sub-case of AI a broad category and machine learning being a set of particular techniques for how we optimize AI. So having the machine learn from data, again, a big metaphor about comparing the human brain to computers, which is a bad thing to do for your own wellbeing. We aren\\'t machines. Machines don\\'t think like us. We call it... this is from Ellen Ullman\\'s Close to the Machine, fantastic book. It\\'s a memoir. I have it with me. And she talks about how we call it the machine. We call it a memory, but it\\'s not right. So one, I don\\'t like the name machine learning when people use it too much to compare to how babies. I\\'m like no, fuck, that\\'s not it. But a way of optimizing around data, and so it\\'s just really cool statistics. And it\\'s really close to data science, where you\\'re like you have data, but what meaning can you get out of this for a social reason? What\\'s blind on this? I don\\'t really care about the difference between, but then AI a more general label. If I have a bunch of if/then statements. Is that AI? Well, yeah, it used to be. We were trying to solve like chess originally or checkers. We had some pretty simple sequence of steps because that\\'s how we thought the human brain worked or at least that area thought things worked. So I don\\'t know. I use them broadly because I don\\'t think the specifics matter. It\\'s when we get to the ethics side of things.']",
          "['Okay, here\\'s how I\\'m going to think about it. In the Pasco case, I\\'ll use that example for both because it\\'s good to ground myself in a fucking case so I don\\'t go crazy. Looking at that case and thinking about how I speak to my students about AI ethics, which I speak to them the way I do because of my interviews of the 10 people I had. And they all had different views on things. It\\'s like, well, how can I do this in a way that\\'s kind of in agreement with all these different views, knowing that again, the course is going to be kind of pigeonholed the way that Allan did it. How can I introduce to things? And one of the things that people talk about a lot in the ethics, when you\\'re giving people who aren\\'t ethicists, they\\'re just a fucking programmer. You want them to think through things, is the stake stakeholders model. We just ask what\\'s at stake? But remember week one, I\\'m doing a pre-assignment where they don\\'t have any understanding what I\\'m talking about. So I can\\'t use the word stakes because they might take that in a too narrow sense. And so I say to them is, \"In this case, what are the things that we want to get right? Yes, there\\'s this really shitty thing that happened in Florida. The article ends on a very sad note, but there are things that we\\'re trying to get as humans in this. All the bad things that happened, we wanted those not to go that way, but we don\\'t want to just burn out everything to happen because again, we\\'re trying to get at things. So I have the students focus on this shared human endeavor of things we try to get right. And so in that case, the police department did a shitty thing, but they\\'re trying to get right reducing crime. They\\'re trying to right serving their community\\'s interest. They\\'re trying to get right distributing their resources well. Whether they actually got those things right in a way that still abides by fair terms is also preparation is a whole nother story, and we\\'ll let the Department of Justice... we\\'ll see what their investigation finds. But it has a lot of promise. And I try not to focus on promise because coming into viewing AI from the point of view of promise and predictions and it\\'s... you mention advancements. My first instinct there is no, let\\'s not talk about advancements first. There\\'s a huge history of technology here. There\\'s a huge history of institutions. Policing hasn\\'t always been the best and still isn\\'t always the best. And so if something\\'s being embedded there, we better really look at that history. We can\\'t look to advancement and promises of AI as a way to ignore how we got here. So I tried to avoid that framing. But we are trying to use it to get at good things. So I can\\'t say what is actually done well, but I can say we\\'re generally trying to get at good values with it. So let\\'s get that right, kind of be my call to action. And that involves also looking at the history of how we get to places. Anna Hoffman, AI ethics researcher from Northwest, I can\\'t remember where she\\'s from, but she\\'s awesome. You should look up her work. I have the idea of let\\'s not talk about entry points from promises, or futures, or potential of AI because of a talk she had. She\\'s like I\\'m in this pandemic and I\\'m tired and I\\'m sad. I\\'m a tired, stressed professor and I\\'m tired of looking at AI. So I\\'m going to come at this in a fun way, which is what does AI look like if we start from the position of infinite love for trans people? Let\\'s just take a fundamentally different approach to how we might think about AI. And it was a really fucking fun talk. But one of the things is looking at the medias around promises of AI and what gets left out when we focus too much on promises. So I\\'m doubly rambling at this point, but the things we\\'re trying to get right with AI, it\\'s good to enumerate those. And we kind of reground ourselves in the value of what we\\'re actually trying to do. And a lot of harms come out when we lose track of that and focus too much on it can just solve a lot of these things. When it\\'s put into context that it ought not be in, harms can come up. When it\\'s put into context too quickly, harms can come up. So I\\'m going to stop rambling and drink water.\\n | I\\'m going to bring up... I had this already. Nope, that\\'s my writing sample. I have just been writing all this down for applications to places. And I\\'m at the point where if I write something down, I\\'m going to forget it immediately, which is the opposite problem because now I\\'m not constantly thinking about it. So AI or AI ethics is a bundle of ethical issues. And so thinking about populations depends on your entry point to thinking what AI ethics is. And so I\\'m just reading my notes, honestly. We can\\'t violate public trust, is kind of one thing. That comes up a lot in the discussions around autonomous vehicles, that we want to have autonomous vehicles for X, Y, Z reason. But in order for that to happen, the public kind of has to trust in the system. We have to agree. We have to know that it\\'s not going to run over black people more than white people. If it gets down to that moment, we have to know that it isn\\'t going to confuse us, not know that a cyclist is there or the actual case of where Uber hit a woman. I don\\'t know if it was Uber or not, but there was an autonomous vehicle that hit a woman because she jaywalked. Well, jaywalking should result in a fine, if that, not in death from a car that didn\\'t see you. The idea that the road is owned by cars is a relatively new one in human history. Roads were owned by the people walking on them. And so that one, public needs trust in it and that we don\\'t want to violate that trust. So in a way we can harm everyone when we release things too early in that sense or have things that create harms. Also, the people who are literally hit by the car, I think get harmed the most. Let\\'s see, there\\'s a lot of talk around misinformation being amplified on social media. And there\\'s also talk on certain things when you have newsfeeds, like Facebook, that put the things at the top that they think you want to see or that they want you to see. That\\'s in their benefit for you to see. Or TikTok, as fun as it is to find your own extremely niche set of friends on TikTok, and ridiculously fast, what\\'s getting left out? Who\\'s getting pushed down? And so there\\'s a lot of harms that come up when misinformation gets brought up to the top and there\\'s a lot of harms that get caused when certain communities are just completely pushed down and systematically given lower scores in the algorithm. And so there\\'s a lot of talk on TikTok around trans communities and people of color, people who don\\'t look pretty getting rated lower by the algorithm, so they\\'re not going to have as much of a viewership. And so when your money is tied to being a content creator, it sucks. There\\'s also the whole thing that if you\\'re queer and online, you are subject to harassment. You always have to have a few backup accounts because one of them is going to get blocked out because people are going to mass... people who don\\'t like you are going to mass report everything you do until you get flagged by the algorithm as bad and systematically have your account removed, even though you didn\\'t do anything wrong. That happens all the fucking times with trans content creators. There\\'s like all these, \"Hey, my thing got deleted again.\" And it\\'s just happens. So there\\'s a lot of algorithms that play a part in that. But also the algorithm isn\\'t separate from the system of humans interacting with it. So fuck, I talked a long list, when it\\'s applied in places like policing, compulsory education, medicine, et cetera. Those already have a lot of scrutiny on them, legal and public scrutiny on them. So it\\'s very important that we get AI right in those cases because we\\'re constantly looking at them for whatever reason. Reason might be that we have a lot of public scrutiny on policing and education because we want a fair and just society. And we see those institutions that are very important to the function of a fair and just society. So if we fuck up there at all, everyone pays attention. Work, AI changes the nature of work, and when the nature of work changes, some people are benefited and some people are completely displaced.']",
          "['Oh yeah. I think that\\'s more important, but it\\'s mostly because I think it\\'s more important because you as an individual interact with society through those platforms and through that data and through that AI, even if you don\\'t realize you are, so me watching a video on YouTube, if I watch a video that has ethical issues, if I watch a video that it just is a very mean spirited video that can harm someone else by watching it I\\'m promoting it. And then while it might be, \"well, I didn\\'t tell anyone about it.\" It\\'s still me actually watching it needs an algorithm system that then promotes it to especially people in my geographic region or people with somewhat profiles to me. So it\\'s this idea that, I think it\\'s important because it also important and ultimately I think it\\'s the fault of the company that is the case, but I don\\'t trust them to fix that. So I think it\\'s more important to educate the individual on how that\\'s going to work. And then I think that\\'s just, once again your data literacy, understanding your digital footprint and also understanding your worth, humans are worth more as data to people than they are as humans. So I think understanding just how much your data is worth contextualize, you should just be giving it out for free, it is something fairly important and also the other adage of nothing is free in life where it\\'s, \"yeah, all these things are free.\" TikTok is free, but you\\'re getting a lot of data and you\\'re getting advertised to a lot.']",
          "['I met a guy in person, he used to help make movies. He\\'s like, \"yeah, my job used to take a team. And now it\\'s just one dude at one program.\" And so he\\'s changing careers into learning how to code, and so there\\'s that. Consumers might not be aware that they\\'re interacting with AI when they\\'re shopping online, except it might not be aware to the extent that they are and that can have harms to principles like consent, being aware enough of what\\'s going on to be able to fully make a decision and not doing that you\\'re interacting with AI or that visual. Invisible changes are happening on the screen behind the scenes. It might be bad depending on that context. And it\\'s fucking hard to regulate AI.']",
          "['Yeah. That makes sense. I am still learning how to code. So I\\'ll get there one day. | Oh yeah. I think that\\'s more important, but it\\'s mostly because I think it\\'s more important because you as an individual interact with society through those platforms and through that data and through that AI, even if you don\\'t realize you are, so me watching a video on YouTube, if I watch a video that has ethical issues, if I watch a video that it just is a very mean spirited video that can harm someone else by watching it I\\'m promoting it. And then while it might be, \"well, I didn\\'t tell anyone about it.\" It\\'s still me actually watching it needs an algorithm system that then promotes it to especially people in my geographic region or people with somewhat profiles to me. So it\\'s this idea that, I think it\\'s important because it also important and ultimately I think it\\'s the fault of the company that is the case, but I don\\'t trust them to fix that. So I think it\\'s more important to educate the individual on how that\\'s going to work. And then I think that\\'s just, once again your data literacy, understanding your digital footprint and also understanding your worth, humans are worth more as data to people than they are as humans. So I think understanding just how much your data is worth contextualize, you should just be giving it out for free, it is something fairly important and also the other adage of nothing is free in life where it\\'s, \"yeah, all these things are free.\" TikTok is free, but you\\'re getting a lot of data and you\\'re getting advertised to a lot.']",
          "[\"That's awesome. I mean we need more people to understand coding in general. So I think it's great that you do that work. Has any of the work you've done with those type of groups ever bridged into AI before or machine learning? | Yes, it's a data ethics course. We talk about AI a fuck ton. So besides camps, teaching people how to code, I'm a PA right now with YJ Kim here at the Wisconsin Center for Ed Research and I know David Shaffer. And through him I've heard about goal like everyone else in this community. So I hear about Clemson all the time. But where was I going? So I have a PA ship here. I've TAed here, TA for code and power a lot with Dr. Royston and that's kind of your classic critical theory approach to AI and a lot of... but like meritocracy, Google image search results for black women showing images of gorillas, those kinds of cases. Yeah, it's kind of fucked up. So talk about how code and power get gets embedded. And it's very critical around institutions and getting students to think about that stuff and learn about implicit biases and things like that. We actually have them take the implicit bias test and then think about the limitations of that test and try and reason about what data says. But-\"]",
          "[\"Yeah, I'm part of the teaching faculty, so most of my focus is on teaching, but another responsibility I have is I serve as the executive director for Clemson's AI Research Institute for Science and Engineering, which is a new institute that came online in the summer of 2020, so right in the middle of COVID, and is only now sort of getting going. It was founded by Dr. Feng Luo, who is a professor in the school of computing. I'm helping him realize his vision for AI RISE, is what we call it, and it really is a combination of providing educational opportunities across Clemson to train faculty and researchers and students. It's to bring in the community, the upstate and the entire state, to help educate everyone really, on AI, and what AI is because it's such an overloaded, overused term and everybody maybe thinks they know what it is, but I think everybody has probably a different idea of what it is. AI always comes up in whatever subject I'm teaching. One of the classes I'm teaching this semester is on computing, ethics, and society. So obviously, talk a lot about AI and the moral issues associated with the use of AI and the bias that is proven to be in a lot of systems that employ AI today, and it has certainly lots of positive impacts, but also a lot of negative impacts. We talked a lot about that in that class. It always comes up because it's everywhere, honestly. | Thank you. We appreciate those examples. I think most people have been encountering the AI that makes decisions or suggest things for them. Amazon, Netflix, they all have that. Now the reverse of that question, how has AI or machine learning harmed us and who have they harmed?\"]",
          "['Yeah, I\\'m actually familiar with that one, because it\\'s the CU-TLP program. We have some people in the learning sciences that are working on that. So I\\'m familiar with this one. Would you mind giving me a little bit more detail about your project that involves AI ethics? What were some of the results that came out of that? | Yeah. So on the day-to-day, once again, if I talk about it from a project to project standpoint, there are some easy ways to look at it, which are the TLP project, that\\'s coding and expert system, which means I\\'m looking at how experts look at things and I\\'m creating rules for a system to make judgements based on those rules. So, on a day-to-day, what I do is, I check those rules. I look at them, but most of what I do is just make sure it\\'s still working. Make sure nothing\\'s gone wrong, make sure I look at the results. I\\'m like, \"okay, these results came out, they don\\'t look bad.\" A lot of it, a lot of time goes into those, a lot, at a concentrated period and then a lot of it\\'s just watching it happen. So that\\'s the one. And then for my dissertation work, I actually use machine intelligent systems that were designed by other groups. So I got them from a group that the platform I chose to use, they had created these bots for that platform. And so what I actually do is just, I found these bots made by that platform that are open source and I slightly tweaked them. So similar to before it\\'s less of a like I need to change and develop so many things every day and more like, \"okay, I just need to keep this up and running, make sure everything\\'s fine. Make sure nothing\\'s changing.\" So just a lot of oversight.']",
          "['Technology in general.\\n | I met a guy in person, he used to help make movies. He\\'s like, \"yeah, my job used to take a team. And now it\\'s just one dude at one program.\" And so he\\'s changing careers into learning how to code, and so there\\'s that. Consumers might not be aware that they\\'re interacting with AI when they\\'re shopping online, except it might not be aware to the extent that they are and that can have harms to principles like consent, being aware enough of what\\'s going on to be able to fully make a decision and not doing that you\\'re interacting with AI or that visual. Invisible changes are happening on the screen behind the scenes. It might be bad depending on that context. And it\\'s fucking hard to regulate AI.']",
          "[\"Yeah, there's a really thought provoking... Hang on, just let me see. It's a great thought provoking website. What is it called? Oh yeah. moralmachine.net. If you go to moralmachine.net and then click the judge link, what this is, is this research that's done by MIT and a handful of other universities, and it asks you some interesting questions because everybody's talking about self-driving cars, and how they're going to happen. They're already happening. It's going to happen, and the reason behind it is, humans are not good at driving. People always think they're good at driving, but they're not. So, if we get the self-driving stuff done correctly, even if it's not entirely self-driving, even if you're sitting in the driver's seat and have to take over, there are these interesting moral decisions to be made by the technology in the event of something unexpected. This website poses all these interesting questions about, if you were programming this self-driving car, what would you do in the situation? Your brakes fail. If you continue on your path, you're going to destroy three children in the pedestrian path. If you swerve, you're going to kill everybody in your car because you're going to crash. It's an interesting reveal of how we think and they, MIT and these other universities collect this data. I think for young people, gaming, just taking advantage of gaming technology to educate in a way that's maybe not right in your face, I think is a good way to learn. So much of learning is caught rather than taught. You just never know.\"]",
          "['So the data ethics course is a combi credit. There\\'s a combi version and a non-combi version. When I taught it over summer, it was eight weeks online, a hundred-ish students, some combi, some non-combi. I had two TAs and also I was the TA for it twice after. So we designed it. I was a TA, I was a TA, and then I taught it and I\\'m praying to teach it again this summer. But it\\'s a lot of writing because it\\'s a big combi component. And Alan Ruble was the instructor who made it, who\\'s my advisor. And when teachers design courses, they use the pedagogy they were taught with is the most number one thing I learned from talking with all these AI ethics teachers and all these different backgrounds is they might have similar motivation, similar goals, similar big picture aims where all we give a shit about AI ethics. But then what they actually do in the classroom is just what they were taught with. And I\\'m just like, \"Okay, that\\'s not very critical of you, but good.\" But it\\'s good to see all these things and these different views besides my own too because again, I teach the way that I\\'ve been teaching. I teach drawing on my English background, not my computer science background, even though I teach computer science style courses because in English, they have you take pedagogy classes. So I\\'m drawing on that. I have weekly journals that I make my students do because I found that useful in a class I took. It\\'s like, \"I like that so I\\'m going to make my students do it.\" But I\\'ve adapted to make sense for coding. So in that class, it\\'s a lot of writing and reading because that\\'s what the instructor who designed did a lot because he\\'s a philosopher. And in philosophy you do a lot of reading and writing. I added in because I like them the studio discussions. The students read an article or watch... do some kind of prep for the discussion. They meet with their group on Microsoft Teams or similar tool. They record it. They post the recording to the discussion board and then they watch X number of recordings and respond to them. I did a pre-post set up for that for the course where they read the first and second house at the time of the Pasco PD case that is happening in Florida. Pasco police department made an algorithm to predict who would commit crime or would be likely offenders and then went out and targeted people based on it. It\\'s currently going under investigation by the Department of Justice, which means you\\'re going to have a lot of fact finding about this case. So that\\'s a good case to know when you\\'re teaching data ethics.']",
          "['I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI.  | No that\\'s great, thank you. So this might go more into some of your ethics background, but how do some advances in AI or machine learning help humans?']",
          "[\"How can we engage youth in learning about AI and machine learning and ethics? Have you ever used any activities? Do you know of any resources out there of [inaudible 00:24:53] or any ideas of how we can? | Yeah, there's a really thought provoking... Hang on, just let me see. It's a great thought provoking website. What is it called? Oh yeah. moralmachine.net. If you go to moralmachine.net and then click the judge link, what this is, is this research that's done by MIT and a handful of other universities, and it asks you some interesting questions because everybody's talking about self-driving cars, and how they're going to happen. They're already happening. It's going to happen, and the reason behind it is, humans are not good at driving. People always think they're good at driving, but they're not. So, if we get the self-driving stuff done correctly, even if it's not entirely self-driving, even if you're sitting in the driver's seat and have to take over, there are these interesting moral decisions to be made by the technology in the event of something unexpected. This website poses all these interesting questions about, if you were programming this self-driving car, what would you do in the situation? Your brakes fail. If you continue on your path, you're going to destroy three children in the pedestrian path. If you swerve, you're going to kill everybody in your car because you're going to crash. It's an interesting reveal of how we think and they, MIT and these other universities collect this data. I think for young people, gaming, just taking advantage of gaming technology to educate in a way that's maybe not right in your face, I think is a good way to learn. So much of learning is caught rather than taught. You just never know.\"]",
          "['I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI. ']",
          "['Oh yeah. I think that\\'s more important, but it\\'s mostly because I think it\\'s more important because you as an individual interact with society through those platforms and through that data and through that AI, even if you don\\'t realize you are, so me watching a video on YouTube, if I watch a video that has ethical issues, if I watch a video that it just is a very mean spirited video that can harm someone else by watching it I\\'m promoting it. And then while it might be, \"well, I didn\\'t tell anyone about it.\" It\\'s still me actually watching it needs an algorithm system that then promotes it to especially people in my geographic region or people with somewhat profiles to me. So it\\'s this idea that, I think it\\'s important because it also important and ultimately I think it\\'s the fault of the company that is the case, but I don\\'t trust them to fix that. So I think it\\'s more important to educate the individual on how that\\'s going to work. And then I think that\\'s just, once again your data literacy, understanding your digital footprint and also understanding your worth, humans are worth more as data to people than they are as humans. So I think understanding just how much your data is worth contextualize, you should just be giving it out for free, it is something fairly important and also the other adage of nothing is free in life where it\\'s, \"yeah, all these things are free.\" TikTok is free, but you\\'re getting a lot of data and you\\'re getting advertised to a lot. | Yeah. So, I said what happens to your data is the most important thing to me. And also what do you produce as a person in terms of data? So what do you produce and then what happens to it? That\\'s the biggest thing for me, because I think that also goes hand in hand with that literacy angle. But I also think the most important thing is how easy it is. Because I think reducing a mythos around a technology, there\\'s a quote about \"tech big magic hammer,\" but reducing the mythos around it, I think is also an extremely important aspect of it because computing has been plagued with people who brag about how hard it is and try to make it seem extra hard. And it isn\\'t. Especially if you don\\'t go into it with respective of it being extra hard. If you go into it with an encouragement angle and with an interest in learning something, it\\'s a lot easier. So I think the second most important thing is you need to learn just how easy it is to get started. There are so many things out there just to get started and they\\'re really easy to get started. They\\'re really easy to get just a little bit interested in it and then that\\'s enough to know if you\\'re going to like it or not. So I think that\\'s the two things. It\\'s from the computing side, what happens to your data? And then from that side, what are the resources you can do to get started and how easy is it to get started?']",
          "['I think ultimately AI systems themselves are tools. So, an ethical dilemma existing in AI is simply the reflection of that ethical dilemma existing in a person. What I think AI makes it easier is AI is a tool that makes a lot of things easier and I think that also means AI could be a tool that makes being unethical a lot easier. There\\'s a lot of things you can do with AI systems, with them being black boxes and completely hidden. And also then just being a machine system, if I think of a really modern example, Google is extremely adamant that they will never share the AI algorithm side that determines whether or not a video on YouTube should be monetized or demonetized and the reason they say they don\\'t want to do that is because of bad actors. They\\'re like, \"oh, if we tell you that, then they\\'re a bad actor come in and do something,\" but the issue with this, of them coming in and so saying like, \"oh, we won\\'t do this,\" is there\\'s no oversight now to know if that\\'s discriminatory or not, we can\\'t even tell. And then we say like, \"well shouldn\\'t you provide something?\" No, we can\\'t even provide the oversight because that\\'s too much information, and you be like, \"you just have to trust us.\" And I think the challenge right now is that with AI systems, people are way too willing to give them that trust. If a company comes out and says, \"oh, we can\\'t divulge the AI secrets,\" because they\\'ll be, \"oh they\\'re just trade secrets, you can\\'t,\" it\\'s like a normal trade secret, but it\\'s a trade secret that heavily impacts a lot of other people. And it doesn\\'t have that oversight right now. So, I think that\\'s where the complication of ethics comes in is that it makes it easier especially with the current state of it to do something unethical because there\\'s that lack of oversight coming from both a computational side and a social side where people are just like, \"oh, it\\'s an AI system. We can just trust it,\" when in reality, it allows the bad motivations of individuals to almost be hidden because they\\'re exercising those bad motivations through a system. | I think it\\'s important, but I think it\\'s important from two perspectives. The thing I think it\\'s more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it\\'s very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society\\'s going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there\\'s it\\'s good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it\\'s a two front benefit from it.']",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "2_ai_ethics_moral",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "2_ai_ethics_moral"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          11.463852882385254,
          10.320557594299316,
          10.590439796447754,
          11.172551155090332,
          11.181780815124512,
          11.508947372436523,
          10.36925983428955,
          11.194672584533691,
          10.03564453125,
          9.924834251403809,
          10.017583847045898,
          11.44275951385498,
          11.478002548217773,
          10.696988105773926,
          9.97885799407959,
          10.636080741882324,
          11.1288423538208,
          10.427628517150879,
          10.678956985473633,
          10.447714805603027,
          9.996514320373535,
          9.349238395690918,
          10.638259887695312
         ],
         "y": [
          10.56596565246582,
          10.002336502075195,
          9.974644660949707,
          10.805746078491211,
          10.78097152709961,
          10.538457870483398,
          10.318617820739746,
          10.803068161010742,
          10.172172546386719,
          10.262565612792969,
          10.188843727111816,
          10.597421646118164,
          10.58753490447998,
          10.449501037597656,
          10.232440948486328,
          10.02379035949707,
          10.822654724121094,
          10.25313949584961,
          10.050712585449219,
          10.256778717041016,
          10.169724464416504,
          9.724411964416504,
          10.344614028930664
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"No, I actually feel that because I recently joined Golnaz's lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations?\"]",
          "[\"It'll be interesting to see what type of advances the AI will add to our medical system. | Well, I do a lot of presentations that cover these basics to get people on the same page. It's really the ability of something to mimic the capabilities of the human mind by learning from data and experiencing things. This general definition, the ability again, to have a system mimic the capabilities that we have, but what we have, in reality, is something quite different. In general, I think about that. And then within that, there're subsets, like machine learning and deep learning. I think when people think of AI, they think of robots killing all the humans.\"]",
          "[\"Yeah. And even just the tendency, like one thing I was discussing in a research project I'm working on right now is that a lot of people didn't trust an AI to work in a risky decision be because they thought it'd be too logical. Like it's going to make a decision based off what the exact probability of something happening is versus what a human in this scenario is probably going to start thinking of the worst case as possible and they may be tangentially probable, but it might be so bad that they're not going to make a decision. And that's part of a human emotional factor that's not going to be replicated by a machine.\"]",
          "[\"Yeah, I don't. It's been a while. I don't know...do something with tick tock? | Are you talking about just learning AI and ML in general or the ethical issues?\"]",
          "[\"That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way? | That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning?\"]",
          "['Technology in general.\\n']",
          "[\"I mean, I would say it's broadly sort of the study of how to get computers and technologies to do things that humans would regard as intelligent. And I don't know. I say it like that, because I think it's sometimes a shifting boundary. Sometimes AI will accomplish something in AI and then people are like, wow, maybe that wasn't actually that intelligent after all. But I would say murkily, it's getting computers to do things that human humans would see as intelligent.\"]",
          "[\"Yeah. And even just the tendency, like one thing I was discussing in a research project I'm working on right now is that a lot of people didn't trust an AI to work in a risky decision be because they thought it'd be too logical. Like it's going to make a decision based off what the exact probability of something happening is versus what a human in this scenario is probably going to start thinking of the worst case as possible and they may be tangentially probable, but it might be so bad that they're not going to make a decision. And that's part of a human emotional factor that's not going to be replicated by a machine. | Yeah. That's great. Like simulating that environment a little bit closer to what we had in person potentially.\"]",
          "[\"I would say my machine learning to me, my understanding is it's kind of the algorithm behind the scene. I consider artificial intelligence as kind of like with, how to say, a subject being there. It could be virtual, it could be a robot being there, but machine learning is more like the algorithm behind it. Kind of like its mind or core. That kind of feeling. It's kind of like if I use human as an example, it's just like artificial intelligence is a human and machine learning is kind of his brain to think, to help it to learn and make predictions.\"]",
          "[\"Well, I do a lot of presentations that cover these basics to get people on the same page. It's really the ability of something to mimic the capabilities of the human mind by learning from data and experiencing things. This general definition, the ability again, to have a system mimic the capabilities that we have, but what we have, in reality, is something quite different. In general, I think about that. And then within that, there're subsets, like machine learning and deep learning. I think when people think of AI, they think of robots killing all the humans.\"]",
          "['One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system.']",
          "[\"One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system. | It'll be interesting to see what type of advances the AI will add to our medical system.\"]",
          "['Yeah. I mean, it\\'s essentially... So it has to appear intelligent, I think, right? So I don\\'t know if it has to actually be intelligent. And so that\\'s where I\\'d classify some of the things that I do that are... If you really dig down into it, it would probably be classified in algorithms, right? It\\'s an algorithm. So I did a book selection algorithm. But I think it\\'s important to think about the user\\'s perspective on these things. And so I have a pretty broad idea about AI in that if it appears intelligent, if it comes across as intelligent... I mean, maybe if it even wasn\\'t the intent that it comes across as intelligent, but it appears that way, I think we have to treat it as AI because of the impact it would have on the person that it\\'s interacting with, right? So my book selection algorithm is not complicated at all. It\\'s essentially like a sorting algorithm and we add in a couple of inputs about what the kids\\' book preferences are like, their reading skill level, the amount of time they read, that kind of stuff. And then we tag books for all those features, and we just make a priority queue out of it. Really simple, early CS stuff. But to the kids, it came across as intelligent. And I think that\\'s the key factor, that when we interviewed them afterwards, kids felt that the robot was paying attention to them and that the suggestions the robot made for books were personal, that they were about them. And so that to me is now where you\\'re in artificial intelligence and you really have to then take that seriously, because if the person believes that they\\'re working with an intelligent machine, then you have to treat that carefully. | Yeah. In that sense, that I think, I would fall in line with a little bit more traditional perspective, because machine learning doesn\\'t necessarily always have to... It\\'s not is a user-facing part of what\\'s happening. So it\\'s essentially how to take a bunch of inputs, teaching a machine how to interpret those inputs, and to organize, categorize, or plan actions based on those inputs. So it\\'s different levels of black boxiness that go along with it. But yeah, it\\'s essentially the training machines to have a space in between input and output that is nonlinear, I guess. So I mean, the traditional perspective is you have this set of data that\\'s coded with these sets. And so you train that way. And then a new set of data that isn\\'t coded, the machine should take what it learned from this first one, apply it there, and come up with the same codes. Those codes could then be actions to do. Those codes could be categories. Those codes could be things like emotions, right? So, \"Here\\'s 10,000 pictures of people who look angry. Here\\'s 10,000 more. Which of these are angry?\" kind of thing. So that, I think. And now that I\\'m thinking about it, that\\'s almost sneaky or more problematic sometimes because you don\\'t necessarily always have the user interacting with it while you\\'re developing these things and testing them. And it can be to such a scale sometimes that the errors and the problems in there are easy to miss, right? That, \"Hey, we got 99% accuracy,\" but that means if there\\'s 100,000 images in that set that you\\'re classifying on, 1% is actually a lot. And if that 1% impacts me and you\\'re just going to take this thing off the shelf that\\'s 99% accurate, and you\\'re going to take it off the shelf, and it\\'s going to make a medical diagnosis, and I get the 1% problem, that\\'s pretty impactful. So again, that\\'s one of the things I talk a lot with graduate students who are like, \"Oh, I\\'ll just grab the thing and we\\'ll just figure it out. It\\'ll tell us what to do.\" No, that\\'s not safe in a lot of the things that we\\'re doing. So, yeah. So I guess, I don\\'t know. I mean, that\\'s a too-long explanation of what machine learning is. ']",
          "[\"Yeah. Yeah. It's cool. And what's interesting is it's almost like a broad computer science department that just happens to be at Microsoft. So there's a lot of us doing AI stuff, but it's kind of the whole spectrum of computer science research. | That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way?\"]",
          "[\"Yeah. I mean, it's essentially... So it has to appear intelligent, I think, right? So I don't know if it has to actually be intelligent. And so that's where I'd classify some of the things that I do that are... If you really dig down into it, it would probably be classified in algorithms, right? It's an algorithm. So I did a book selection algorithm. But I think it's important to think about the user's perspective on these things. And so I have a pretty broad idea about AI in that if it appears intelligent, if it comes across as intelligent... I mean, maybe if it even wasn't the intent that it comes across as intelligent, but it appears that way, I think we have to treat it as AI because of the impact it would have on the person that it's interacting with, right? So my book selection algorithm is not complicated at all. It's essentially like a sorting algorithm and we add in a couple of inputs about what the kids' book preferences are like, their reading skill level, the amount of time they read, that kind of stuff. And then we tag books for all those features, and we just make a priority queue out of it. Really simple, early CS stuff. But to the kids, it came across as intelligent. And I think that's the key factor, that when we interviewed them afterwards, kids felt that the robot was paying attention to them and that the suggestions the robot made for books were personal, that they were about them. And so that to me is now where you're in artificial intelligence and you really have to then take that seriously, because if the person believes that they're working with an intelligent machine, then you have to treat that carefully.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "3_ai_intelligence_intelligent",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "3_ai_intelligence_intelligent"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          6.246002674102783,
          6.02422571182251,
          6.105388164520264,
          6.125729084014893,
          5.853436470031738,
          6.241237640380859,
          5.901906967163086,
          6.085046768188477,
          5.749626636505127,
          5.854104995727539,
          6.137455940246582,
          6.094892978668213,
          6.364406108856201,
          5.951918125152588,
          6.268695831298828,
          6.066938400268555
         ],
         "y": [
          9.172074317932129,
          8.09530258178711,
          7.603143692016602,
          8.923491477966309,
          8.322120666503906,
          7.992392063140869,
          8.237245559692383,
          7.571958065032959,
          8.377275466918945,
          8.196070671081543,
          7.903256893157959,
          8.055797576904297,
          8.493020057678223,
          8.242029190063477,
          8.397936820983887,
          8.238873481750488
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah, I mean, I understand that so going off of what you said, what do you think it would be important for you to know about AI and machine learning like what are some key things that you think they should take or learn about not necessarily going into the ethical issues if you don't want to. | I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids. | I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"I mean, I know privacy is always a big issue, especially when you're talking about collecting major data, in order for something to apply or use machine learning. It's got to be collecting a lot of data about environment and people it's working with. So you have people who are comfortable sharing different levels of information and different levels of being information collected about them. And then also if you have an agent like that constantly collecting data, wherever it's working or interacting, there's the concept of like, okay, at what point do you require people to be like, oh, where this is going on and happening and require some sort of consent versus like it's just, it's so ubiquitous that everybody just knows it's going on. There's probably a tipping point somewhere there, but I think that are long ways off from that. So I think the privacy concerns are going to be pretty, pretty important. | Yeah. As, so as someone who works so closely with AI, can you imagine an elementary school student or a middle schooler or even younger than that, what sort of, how could you break that down? Like AI or machine learning to get them kind of exposed to that?\"]",
          "[\"Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"Thank you for those responses. Can you think of any tools, activities, or resources that could be used to help young people to start thinking about AI and machine learning and kind of the algorithms behind them? | I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids.\"]",
          "[\"Yeah, I mean, I understand that so going off of what you said, what do you think it would be important for you to know about AI and machine learning like what are some key things that you think they should take or learn about not necessarily going into the ethical issues if you don't want to. | I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids.\"]",
          "[\"I mean, I know privacy is always a big issue, especially when you're talking about collecting major data, in order for something to apply or use machine learning. It's got to be collecting a lot of data about environment and people it's working with. So you have people who are comfortable sharing different levels of information and different levels of being information collected about them. And then also if you have an agent like that constantly collecting data, wherever it's working or interacting, there's the concept of like, okay, at what point do you require people to be like, oh, where this is going on and happening and require some sort of consent versus like it's just, it's so ubiquitous that everybody just knows it's going on. There's probably a tipping point somewhere there, but I think that are long ways off from that. So I think the privacy concerns are going to be pretty, pretty important. | I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids.\"]",
          "['Yeah. We have thought of that, integrating Snapchat filters or something like that. Something that they would be interested in. | Are you talking about just learning AI and ML in general or the ethical issues?']",
          "[\"I mean, I understand that. So going off of what you said, what do you think it would be important for youth to know about AI and machine learning? What are some key things that you think they should take or learn about not necessarily going into the ethical issues, if you don't want to think about that? | I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"Yeah, that's fascinating. And I think that's a really good example of the power of AI and how it can be so helpful, especially, like you're taking the person's strengths, you're taking the AI strengths and bringing them together to make this really functional system even more functional. So what are some examples then kind of on the opposite end of the spectrum of how potentially AI or machine learning can harm us? And who in particular do you think it harms? | Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"I mean, I know privacy is always a big issue, especially when you're talking about collecting major data, in order for something to apply or use machine learning. It's got to be collecting a lot of data about environment and people it's working with. So you have people who are comfortable sharing different levels of information and different levels of being information collected about them. And then also if you have an agent like that constantly collecting data, wherever it's working or interacting, there's the concept of like, okay, at what point do you require people to be like, oh, where this is going on and happening and require some sort of consent versus like it's just, it's so ubiquitous that everybody just knows it's going on. There's probably a tipping point somewhere there, but I think that are long ways off from that. So I think the privacy concerns are going to be pretty, pretty important. | Definitely. Yeah. Okay. So just shifting a little bit. This is more directly related to some of the work that we do in my lab. What are your thoughts about youth learning about artificial intelligence or machine learning even as young as elementary or middle school?\"]",
          "[\"I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids. | I mean, I understand that. So going off of what you said, what do you think it would be important for youth to know about AI and machine learning? What are some key things that you think they should take or learn about not necessarily going into the ethical issues, if you don't want to think about that?\"]",
          "[\"I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "4_ai_privacy_kids",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "4_ai_privacy_kids"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          8.606433868408203,
          8.730952262878418,
          8.819485664367676,
          8.951194763183594,
          8.50910758972168,
          8.661848068237305,
          8.706643104553223,
          8.913516998291016,
          7.910544395446777,
          8.579483985900879,
          8.9974365234375,
          8.90297794342041,
          8.705126762390137,
          8.71381664276123,
          8.693469047546387
         ],
         "y": [
          10.592357635498047,
          10.457908630371094,
          10.573555946350098,
          9.137646675109863,
          10.691069602966309,
          10.332066535949707,
          10.382264137268066,
          10.645217895507812,
          10.774395942687988,
          10.633694648742676,
          9.196148872375488,
          10.701116561889648,
          10.320844650268555,
          10.418957710266113,
          10.346946716308594
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "['That\\'s interesting. How did you actually... I know you said you had a psychology degree from Clemson and then you kind of moved into the human-centered computing program. What made you interested in that? How did you become interested in working with AI and machine learning? | I pretty much always wanted to be more in the realm of technology and computer science, but also just really working with people. I was a little intimidated by computer science as a field at first. So, I went with psych. And then, I always knew I wanted to go to grad school. And so, I was originally thinking a good marriage between the two would be human factor psychology, because that is also very involved with technology and design and development. But I didn\\'t even know about the human-centered computing program until my current advisor, I found some papers by him that were about human-AI teaming and team cognition in those teams. And so, I reached out to him and the lab that I was working at the time, I did a collaboration with Nathan and he basically just reached out at me and was like, \"Hey, I\\'d like to have you in the lab. What do you think?\" And I\\'m thinking to myself, \"Oh, this is an opportunity to get to learn computer science and be a little bit more involved in that and still retain the usefulness of my undergrad.\" So, I jumped right at that opportunity.']",
          "[\"Yeah. So pretty much been an academic in training for a very long time. My dad was a dean at Penn State, so I grew up in academia. I knew about academia. I never wanted to be in academia, this is the funny part of the story. I remember telling both of my parents, why would I ever, ever want to do what my dad does? It sounds like the most boring thing in the world. And then I have ended up mimicking his career in many ways. So my professional career is very interdisciplinary. I have a degree in psychology, I have a degree in information science, and then a postdoc basically on computer science. And then I'm a professor of computer science, essentially. So I expand the spectrum from social and hard computational perspectives, and I think that's really important when you talk about AI. It can't just be one or the other. It can't just be a bunch of psychologists ruminating about what they think is important or sociologists thinking what they think is important about AI. You need real computer scientists in the room as well to understand the feasibility of how these things are going to happen. So both of these entities need to be talking to each other. I kind of planned my career that way, that I knew from a very early age that I was interested in the intersection of just people and technology. So I started off learning, I built my foundation with people, with the psychology, understanding that. And throughout my career, I've kept that foundation. It's what grounds me as a researcher and from my worldview of things. I've kept that humanistic foundation, but I built on top of that through additional degrees, more of a technology flare from information sciences and computer science. And now you kind of get whatever the heck I am nowadays. It's like a morphous blob of social and computer science. But I think it's important. And that's how I train my students to think about things is, if we're going to make sure that AI is beneficial for people and inclusive to many people, you better be taking into account the human side of things. But you also need equally know, from my perspective, as people that build AI, you need to know how to do that as well. So it's a big ask, but I think it's what the next generation of people studying, and implementing and developing AI need. They need both of these perspectives. Collaboration amongst both of those is great.But if we can start training people for this early on, to your point in some of your studies that you look at, if we can start instilling this mindset in kids from an early age of AI is not just computer science, it's human science as well, it's both of these things together, instilling that from an early age and having actual interdisciplinary training and degrees for that is going to be really important. So I went all over the place. I'll do this throughout the interview, I'll ramble- | Yeah. So what we study in my research group, almost everything we study nowadays is on this concept known as human AI teaming or human autonomy teaming. So it's the idea of humans teaming with AIs or autonomous teammates for the completion of a shared goal or a shared task. So everything we study nowadays is related to this concept, and there's a lot of different perspectives and paradigms and ways to look at that concept. And we really try to run the gamut on this. So the thing I talk about all the time when we talk about human AI teaming is that there's a bidirectional quality to it. You have to check off the boxes for the human. You have to check off the boxes for the AI. So the human needs to know how to interact with the AI. It needs to have an expectation of what the AI is. But also the AI needs to know what the heck humans are. It needs to know what teaming is, what matters. And this is where we have to get better. We have to develop autonomous teammates that actually know how to interact with humans, because it can't just be a one sided paradigm. And that's what it is right now. We stick people into a human AI team and we say, go work with this AI. But the AI has no clue how to work with you as a human. So what happens is that the human has to take on this brunt of dealing with basically a bad teammate. So what we're trying to do is number one, understand perceptions that humans have of AI as teammates, and reverse engineer those perceptions so we can build more effective, good AI teammates. So like I was talking about before, you can clearly see how there's the psychology point of view with the perceptions of humans and AI as teammates, but then on the other side, it's the computer science side of things. How do we actually build AI's that understand communication, coordination, awareness, things like team cognition, really critical aspects of teaming that have to be built into human AI teams?\"]",
          "[\"Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at. | Yeah, I'm actually familiar with that one, because it's the CU-TLP program. We have some people in the learning sciences that are working on that. So I'm familiar with this one. Would you mind giving me a little bit more detail about your project that involves AI ethics? What were some of the results that came out of that?\"]",
          "[\"I mean, I got into computer science generally. I was interested in computer games, video games. I was kind of interested in the AI that goes into all the games and the enemies that you fight against or whatever. But I started out more interested in graphics and animation and my undergrad was more, I took more classes that are about computer graphics and animation with a little bit of AI, just kind of out of interest. But then in grad school, it was really just in talking with some of the potential professors that were there that I could work with. So my main advisor [inaudible 00:23:30] he kind of really started to introduce me to some of the ideas of, HCI and just thinking about interaction and thinking about how to... Because I was still interested in animation and animated characters, but thinking about how could you actually design animated characters that interact with people? And so I just got super interested in thinking about how does interaction work and then more and more interested in how does humans' social interaction work in the first place? It's so complex. So my passion has become understanding as much as I can about human social interaction and how to computationalize it enough that you can start to develop intelligence systems that participate in that.\"]",
          "[\"Yeah. So pretty much been an academic in training for a very long time. My dad was a dean at Penn State, so I grew up in academia. I knew about academia. I never wanted to be in academia, this is the funny part of the story. I remember telling both of my parents, why would I ever, ever want to do what my dad does? It sounds like the most boring thing in the world. And then I have ended up mimicking his career in many ways. So my professional career is very interdisciplinary. I have a degree in psychology, I have a degree in information science, and then a postdoc basically on computer science. And then I'm a professor of computer science, essentially. So I expand the spectrum from social and hard computational perspectives, and I think that's really important when you talk about AI. It can't just be one or the other. It can't just be a bunch of psychologists ruminating about what they think is important or sociologists thinking what they think is important about AI. You need real computer scientists in the room as well to understand the feasibility of how these things are going to happen. So both of these entities need to be talking to each other. I kind of planned my career that way, that I knew from a very early age that I was interested in the intersection of just people and technology. So I started off learning, I built my foundation with people, with the psychology, understanding that. And throughout my career, I've kept that foundation. It's what grounds me as a researcher and from my worldview of things. I've kept that humanistic foundation, but I built on top of that through additional degrees, more of a technology flare from information sciences and computer science. And now you kind of get whatever the heck I am nowadays. It's like a morphous blob of social and computer science. But I think it's important. And that's how I train my students to think about things is, if we're going to make sure that AI is beneficial for people and inclusive to many people, you better be taking into account the human side of things. But you also need equally know, from my perspective, as people that build AI, you need to know how to do that as well. So it's a big ask, but I think it's what the next generation of people studying, and implementing and developing AI need. They need both of these perspectives. Collaboration amongst both of those is great.But if we can start training people for this early on, to your point in some of your studies that you look at, if we can start instilling this mindset in kids from an early age of AI is not just computer science, it's human science as well, it's both of these things together, instilling that from an early age and having actual interdisciplinary training and degrees for that is going to be really important. So I went all over the place. I'll do this throughout the interview, I'll ramble- | Can you give us an example of a real world situation where that research could be applied, where AI and humans are working together?\"]",
          "[\"Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at.\"]",
          "['I pretty much always wanted to be more in the realm of technology and computer science, but also just really working with people. I was a little intimidated by computer science as a field at first. So, I went with psych. And then, I always knew I wanted to go to grad school. And so, I was originally thinking a good marriage between the two would be human factor psychology, because that is also very involved with technology and design and development. But I didn\\'t even know about the human-centered computing program until my current advisor, I found some papers by him that were about human-AI teaming and team cognition in those teams. And so, I reached out to him and the lab that I was working at the time, I did a collaboration with Nathan and he basically just reached out at me and was like, \"Hey, I\\'d like to have you in the lab. What do you think?\" And I\\'m thinking to myself, \"Oh, this is an opportunity to get to learn computer science and be a little bit more involved in that and still retain the usefulness of my undergrad.\" So, I jumped right at that opportunity.']",
          "[\"Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess. | I mean, I got into computer science generally. I was interested in computer games, video games. I was kind of interested in the AI that goes into all the games and the enemies that you fight against or whatever. But I started out more interested in graphics and animation and my undergrad was more, I took more classes that are about computer graphics and animation with a little bit of AI, just kind of out of interest. But then in grad school, it was really just in talking with some of the potential professors that were there that I could work with. So my main advisor [inaudible 00:23:30] he kind of really started to introduce me to some of the ideas of, HCI and just thinking about interaction and thinking about how to... Because I was still interested in animation and animated characters, but thinking about how could you actually design animated characters that interact with people? And so I just got super interested in thinking about how does interaction work and then more and more interested in how does humans' social interaction work in the first place? It's so complex. So my passion has become understanding as much as I can about human social interaction and how to computationalize it enough that you can start to develop intelligence systems that participate in that.\"]",
          "[\"Yeah, absolutely. I'm a researcher at Microsoft Research in Redmond. I've been here for about five years. And before that I was a PhD student at University of Wisconsin studying computer science, probably, but I was working kind of on human robot interaction and doing a lot that was kind of based on understanding social gaze behavior of people and how to design social gaze mechanisms for robots. And so at Microsoft Research, I'm a research scientist and I'm kind of in a group that focuses on broad problems in AI. My focus is more on still on interaction. So I work in an area that my group calls situated interaction or situated intelligence, where we're very interested in understanding how to design AI systems that can interact with people in kind of everyday settings. So it encompasses human robot interaction. I've done stuff with human virtual agent interaction, stuff in mixed reality. But the idea is how do you combine multiple AI technologies and actually build real systems that can interact with people? So there's an element of studying interaction and studying people and studying social science and then studying the AI and how to actually build systems that can interact with people effectively.\"]",
          "['I pretty much always wanted to be more in the realm of technology and computer science, but also just really working with people. I was a little intimidated by computer science as a field at first. So, I went with psych. And then, I always knew I wanted to go to grad school. And so, I was originally thinking a good marriage between the two would be human factor psychology, because that is also very involved with technology and design and development. But I didn\\'t even know about the human-centered computing program until my current advisor, I found some papers by him that were about human-AI teaming and team cognition in those teams. And so, I reached out to him and the lab that I was working at the time, I did a collaboration with Nathan and he basically just reached out at me and was like, \"Hey, I\\'d like to have you in the lab. What do you think?\" And I\\'m thinking to myself, \"Oh, this is an opportunity to get to learn computer science and be a little bit more involved in that and still retain the usefulness of my undergrad.\" So, I jumped right at that opportunity. | It\\'s definitely different than undergrad. I got my undergrad at Clemson too, in genetics. So, I understand. I\\'m doing a similar thing as you. Would you tell me a little bit more about your AI teaming and cognition, your research, what you\\'re studying?']",
          "[\"Yeah. So pretty much been an academic in training for a very long time. My dad was a dean at Penn State, so I grew up in academia. I knew about academia. I never wanted to be in academia, this is the funny part of the story. I remember telling both of my parents, why would I ever, ever want to do what my dad does? It sounds like the most boring thing in the world. And then I have ended up mimicking his career in many ways. So my professional career is very interdisciplinary. I have a degree in psychology, I have a degree in information science, and then a postdoc basically on computer science. And then I'm a professor of computer science, essentially. So I expand the spectrum from social and hard computational perspectives, and I think that's really important when you talk about AI. It can't just be one or the other. It can't just be a bunch of psychologists ruminating about what they think is important or sociologists thinking what they think is important about AI. You need real computer scientists in the room as well to understand the feasibility of how these things are going to happen. So both of these entities need to be talking to each other. I kind of planned my career that way, that I knew from a very early age that I was interested in the intersection of just people and technology. So I started off learning, I built my foundation with people, with the psychology, understanding that. And throughout my career, I've kept that foundation. It's what grounds me as a researcher and from my worldview of things. I've kept that humanistic foundation, but I built on top of that through additional degrees, more of a technology flare from information sciences and computer science. And now you kind of get whatever the heck I am nowadays. It's like a morphous blob of social and computer science. But I think it's important. And that's how I train my students to think about things is, if we're going to make sure that AI is beneficial for people and inclusive to many people, you better be taking into account the human side of things. But you also need equally know, from my perspective, as people that build AI, you need to know how to do that as well. So it's a big ask, but I think it's what the next generation of people studying, and implementing and developing AI need. They need both of these perspectives. Collaboration amongst both of those is great.But if we can start training people for this early on, to your point in some of your studies that you look at, if we can start instilling this mindset in kids from an early age of AI is not just computer science, it's human science as well, it's both of these things together, instilling that from an early age and having actual interdisciplinary training and degrees for that is going to be really important. So I went all over the place. I'll do this throughout the interview, I'll ramble-\"]",
          "[\"Yeah, so I got a computer science degree, and then during my computer science degree, I worked at a few different jobs using it. So I worked, like, IT management in ceilings working IT, I then had two stints doing software development work, one for a manufacturing company and then another for a larger tech company. And then around the time of graduating, I decided to go into grad school instead. And now that I'm in grad school, most of what I do is research with AI systems and humans using AI systems.\"]",
          "[\"Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at. | Yeah, so work that I've done has been outlining how there are different camps I've worked in. Number one is outlining how humans and AI interact with each other, specifically my dissertation work is on how humans and AI systems can impact and influence each other in a task. So how a human could be susceptible to letting a robot or an AI system tell it what to do and take commands from it or vice versa. So looking at what comprises that, how humans should lead AI or how an AI should lead a human, things like that. So that trade off and then I've also done work looking at how ethics in AI systems can be implemented, created and how it interacts with humans. So, it's fairly big part is the ethical implications of AI systems and how those ethical implications ultimately impact the utility of AI. And then the last, there are other smaller things that I've had to do. But then the last main one I worked on is AI. There's a grant that I work on fairly often that is a grant that looks at using machine intelligence and recommender systems to provide recommendations to teachers for professional development. So it's a lot of providing recommendations for their professional development and I work on the side of that where I work on building this system and outline in that aspect.\"]",
          "[\"Yeah, so I got a computer science degree, and then during my computer science degree, I worked at a few different jobs using it. So I worked, like, IT management in ceilings working IT, I then had two stints doing software development work, one for a manufacturing company and then another for a larger tech company. And then around the time of graduating, I decided to go into grad school instead. And now that I'm in grad school, most of what I do is research with AI systems and humans using AI systems. | Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "5_academia_ai_grad",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "5_academia_ai_grad"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          4.008605003356934,
          4.078652381896973,
          3.528125524520874,
          4.1949591636657715,
          4.042124271392822,
          3.488996982574463,
          3.9460809230804443,
          4.222592830657959,
          4.4773268699646,
          3.992950677871704,
          4.123745918273926,
          3.5362484455108643,
          3.5429999828338623,
          3.469778537750244,
          3.903798818588257
         ],
         "y": [
          10.796599388122559,
          10.808152198791504,
          10.89428424835205,
          10.881651878356934,
          10.789339065551758,
          10.92903995513916,
          10.7570219039917,
          10.882048606872559,
          10.90406608581543,
          10.720453262329102,
          10.85517406463623,
          10.853536605834961,
          10.879063606262207,
          10.953694343566895,
          10.85029411315918
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"I think one of the topics that keeps coming up is this issue of facial recognition, and a lot of research in that area where the accuracy of facial recognition is certainly in doubt, especially for people of color. There's a lot of research and certainly a lot of improvements being made in those areas, but still, wrongfully identifying somebody because of a failure in a AI system for facial and image recognition is devastating for the people. When you're not one of those people who are flagged, you're like, well, what's the deal? That doesn't affect me, but if you're that person, it's life changing in a bad way. It could be. | One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system.\"]",
          "['Yeah. With bias, the there\\'s a famous Facebook, one where they\\'re serving ads for higher paying jobs to more men than women. There\\'s other ones where they\\'re predictive policing. The algorithm AI will tell officers to target more communities of color and not the communities of... it\\'s the inverse of minority and majority. And then there\\'s another one. Oh. That they have AI algorithms that try and identify candidates for parole. And it\\'ll say that African American individuals are 40% more likely to repeat offend, even though the human will look at the data and be like, \"That\\'s not my experience. That\\'s not the case. That\\'s not true.\" Things like that.']",
          "[\"Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight. | It depends on the context. So if it's a Google image search engine, many kids say, yeah, I'm not represented here, but that's how the world is. The consequence isn't huge. I search for computer science professor and I don't see myself as a African American woman. Okay, that hurts. But I don't think Google should mess with that. Then when it comes to hiring algorithms, yeah, they see that as problematic. So the context really matters and the effect of the consequence as they see it matters to them.\"]",
          "['Well, I mean, so there\\'s definitely documentations of bias that are in there for essentially, right, the people who are designing and making these things cook in their own biases into there. And so if 98% of the data that we trained on are a bunch of white males, then anybody not in that category is going to be on the margins of the data set. And that means they have not been properly trained into that data. And so we\\'re going to be misclassified at a much higher rate. So I mean, I think there\\'s good efforts that are out there now to actually really think about representation and training sets. I don\\'t know how that gets reported. I haven\\'t seen anyone who has openly reported their training statistics that way, that, \"Here\\'s how we balance gender and all the other demographic things that would go into impacting these outcomes.\" So in a healthcare setting, I mean, it\\'s happened before machine learning, is doing these diagnostic things, the reports of people where there\\'s no black people represented in medical textbooks, right? That\\'s not about a machine learning thing. That\\'s about a human learning thing, where if you\\'ve never actually practiced or considered examining a person\\'s body who\\'s different than cliche US Western standards, then you\\'re going to make errors. And so I think we\\'re making the same mistake that we ignored before machine learning. We\\'re making the same mistake. And yeah, I think right now, the biggest thing for me would be transparency on these things. How are these systems trained? What do we know about those data sets? So yeah, they\\'re much more likely to impact marginalized communities because those are going to be the data that\\'s on the margins in the training sets. But I do think there\\'s repercussions for everyone as well. And that\\'s part of my worry too, that definitely some populations are going to be overrepresented in how they\\'re negatively impacted. But I think there\\'s negative impacts across the board. And so again, emotion recognition kind of things, I think those are problematic. Medical examination kind of things, I think are really problematic. So I\\'m skeptical across the board, but yeah. [crosstalk 00:28:37]-']",
          "[\"Yeah, that's fascinating. And I think that's a really good example of the power of AI and how it can be so helpful, especially, like you're taking the person's strengths, you're taking the AI strengths and bringing them together to make this really functional system even more functional. So what are some examples then kind of on the opposite end of the spectrum of how potentially AI or machine learning can harm us? And who in particular do you think it harms? | Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight.\"]",
          "['Well, I mean, so there\\'s definitely documentations of bias that are in there for essentially, right, the people who are designing and making these things cook in their own biases into there. And so if 98% of the data that we trained on are a bunch of white males, then anybody not in that category is going to be on the margins of the data set. And that means they have not been properly trained into that data. And so we\\'re going to be misclassified at a much higher rate. So I mean, I think there\\'s good efforts that are out there now to actually really think about representation and training sets. I don\\'t know how that gets reported. I haven\\'t seen anyone who has openly reported their training statistics that way, that, \"Here\\'s how we balance gender and all the other demographic things that would go into impacting these outcomes.\" So in a healthcare setting, I mean, it\\'s happened before machine learning, is doing these diagnostic things, the reports of people where there\\'s no black people represented in medical textbooks, right? That\\'s not about a machine learning thing. That\\'s about a human learning thing, where if you\\'ve never actually practiced or considered examining a person\\'s body who\\'s different than cliche US Western standards, then you\\'re going to make errors. And so I think we\\'re making the same mistake that we ignored before machine learning. We\\'re making the same mistake. And yeah, I think right now, the biggest thing for me would be transparency on these things. How are these systems trained? What do we know about those data sets? So yeah, they\\'re much more likely to impact marginalized communities because those are going to be the data that\\'s on the margins in the training sets. But I do think there\\'s repercussions for everyone as well. And that\\'s part of my worry too, that definitely some populations are going to be overrepresented in how they\\'re negatively impacted. But I think there\\'s negative impacts across the board. And so again, emotion recognition kind of things, I think those are problematic. Medical examination kind of things, I think are really problematic. So I\\'m skeptical across the board, but yeah. [crosstalk 00:28:37]- | Yeah. I love that, Joe. And I can see that in your work, the way you\\'ve described it. That\\'s great. Okay. So let\\'s switch. So talking a little bit about why we\\'re doing this interview, right? We\\'re taking what people are saying and trying to apply it for learning experiences for young people. So what are your thoughts just generally about elementary school, middle school-age kids learning about either AI machine learning and the social and ethical impacts or both? [crosstalk 00:29:56] ideas around that, what they should learn, what\\'s important for them to know? Can they [crosstalk 00:30:00] in those issues']",
          "[\"Yeah. That's great. Like simulating that environment a little bit closer to what we had in person potentially. | I mean, the aspect is so, AI is only as good as the data we give it, things like that. And I think there's a lot of discussion and research right now about a lot of AI being really biased to majorities because that's the data that they have access to. And so there might not be the ability to accurately assess or communicate with minority populations. And it could, I think there's some discussion about it kind of reinforcing biases and stereotypes because it's just operating off of the set of data that it's given.\"]",
          "[\"Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight. | Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"I mean, the aspect is so, AI is only as good as the data we give it, things like that. And I think there's a lot of discussion and research right now about a lot of AI being really biased to majorities because that's the data that they have access to. And so there might not be the ability to accurately assess or communicate with minority populations. And it could, I think there's some discussion about it kind of reinforcing biases and stereotypes because it's just operating off of the set of data that it's given. | We were talking about it a lot in recognition software and things like that, where the data sets that it's usually using to recognize and communicate with people is generally very Western white male and wrongly classifies people and communicates with them as if they were that group, which makes it very difficult for people to not only work but connect to it and get the benefits as so much to some other populations.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "6_bias_ai_biased",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "6_bias_ai_biased"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          6.449155330657959,
          7.578571796417236,
          7.683675765991211,
          7.5993428230285645,
          7.683804512023926,
          7.578761577606201,
          7.444912910461426,
          7.683236122131348,
          7.562704563140869,
          7.473796367645264
         ],
         "y": [
          7.796512603759766,
          8.230463981628418,
          7.674612998962402,
          8.302094459533691,
          7.67273473739624,
          8.299556732177734,
          8.29338264465332,
          7.674150466918945,
          8.252017974853516,
          8.021724700927734
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "['Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it. | Yeah. One that just comes to mind that didn\\'t come up is self-driving car technology. And that is dangerous in a much more salient way, right? And I think getting people to understand, when I see a car and I look over, I evaluate what the person behind that wheel is attending to, right? Before I cross the street, I like to make sure that I see that person look at me so that I know that they\\'re sensing me, right? So if and when it comes to be that I look over and there\\'s nobody at that wheel, there really needs to be an acclimation to understanding, \"Okay, the situation is that there\\'s a machine driving that car. How does it make mistakes?\" I know how people make mistakes, right? I know what it is. If they didn\\'t look at me and I start to walk out and they just roll even at a red light, that could catch me. So I pay attention to that. What are the types of errors that machines make in that scenario are important, that we know that sometimes, it might not recognize a stop sign at all. People are typically better at it. Maybe not typically, but we learn to understand the types of errors that can be common in these systems, or even uncommon, if they\\'re going to be catastrophic, and are aware enough in a way that we can respond in reasonable actions to it. I mean, right now, if I saw a driverless car, I would go nowhere near the street because they\\'re prone to weird accidents that I don\\'t understand right now. So I can\\'t figure them out well enough to do it. But eventually, if they become commonplace, we\\'ll have to know how that works.']",
          "['Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences.']",
          "[\"If you sit around in a room and you hear a bunch of people just talking, you learn a lot. They're not teaching you, you're just learning a lot. You're just kind of catching it. I think by employing things like gaming and some of these thought workshops, like what would you do in this case? It's interesting. That's why I really like teaching the class I'm teaching this semester, which is computing, ethics, and global society. Because some things that seem simple, are not. They're tough decisions. They may benefit some people. They're going to hurt some people. Is it the right thing to do? Even though you can do it, should you do it? It's-\"]",
          "['Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it. | Yeah. And that interaction between human and the self-driving car, the human and the technology, is really interesting. If you can\\'t look at them in the eye, how do you interact with them to ensure to minimize risk, right? That\\'s a whole other dimension of designing these technologies and testing them. And also-']",
          "['Yeah. I love that, Joe. And I can see that in your work, the way you\\'ve described it. That\\'s great. Okay. So let\\'s switch. So talking a little bit about why we\\'re doing this interview, right? We\\'re taking what people are saying and trying to apply it for learning experiences for young people. So what are your thoughts just generally about elementary school, middle school-age kids learning about either AI machine learning and the social and ethical impacts or both? [crosstalk 00:29:56] ideas around that, what they should learn, what\\'s important for them to know? Can they [crosstalk 00:30:00] in those issues | Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "['Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences. | Yeah, so from two perspectives, in terms of full formalized teaching, I\\'ve taught the second one is about stuff I do with my mother, but I\\'ve taught coding courses at the collegiate level, like lab courses at collegiate style. And so there is less of a gap there in terms of age. I was pretty much two years older than everyone I was teaching. And so yeah, you do get interesting. You do have to work down in terms of their understanding, but also you can still stay at a very high level, but even when I was doing that, the best instructors I\\'ve ever had are the ones that pulled up their code and did it with you. So, that\\'s just how I did it. Like I said, I think what changes is the modality you do it through. So for college kids, you don\\'t pull up a YouTube and talk about how their data is produced there. You just pull up data and you work with them through a data set. So, I think the teaching principle, the pedagogy principle, [inaudible 00:38:56] whatever the word is, principle still stays the same, which that demonstration aspect, the difference is how abstract that demonstration gets at the collegiate level. We don\\'t need that. But then my mother works with students who have gotten interested in stem and I\\'m like, \"yeah, I can talk to them about this.\" And it\\'s more about like once again, the education side for that younger side is, I don\\'t want to just sit up there and tell them about those things. I want them to go find what they find interesting. So I give them the website of, \"here\\'s is all, here\\'s scratch.\" Just play around with it for 10 minutes. I was like, \"I don\\'t want to touch it.\" I\\'m not going to talk to you about it. You just go play with it for 10 minutes because that\\'s like how I not only learned it, but I got interested in it. Because I think it\\'s less interesting for a student to be told what to do. And it\\'s way more interesting if they were, \"here\\'s a fun thing. Here\\'s 10 minutes.\" Instead of doing homework in class, go do this for 10 minutes. And that was always my favorite part of doing computer stuff when I was younger.']",
          "['Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "['Yeah. Absolutely. You answered everything. I\\'m looking at my follow-ups. I\\'m like, \"Oh, you got that. You got that.\" Yeah. I mean, if you want to expand a little bit, if you were to take Jules for example, right, your daughter, and she\\'s six, and what would you want her to know about machine learning or about at that age, anything or about how harmful it can be? Would you talk to her about privacy? Would you talk to her about misrepresentation or discrimination? Where would you go? | Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "['Yeah I mean that is you just gave us a lot of good information Thank you so, can you think of any ways to help us connect these topics like thinking about data machine learning AI to their everyday lives and make it meaningful for them. | Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences.']",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "7_consumer_teaching_do",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "7_consumer_teaching_do"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          9.302505493164062,
          9.425544738769531,
          11.127968788146973,
          9.310688018798828,
          9.27819538116455,
          9.444637298583984,
          9.28552532196045,
          9.208793640136719,
          9.479153633117676,
          9.540334701538086
         ],
         "y": [
          12.856029510498047,
          12.159490585327148,
          10.955851554870605,
          12.852926254272461,
          12.949044227600098,
          12.124953269958496,
          12.876852035522461,
          12.898571014404297,
          12.145522117614746,
          12.424359321594238
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "D1",
          "x": -0.06762518882751464,
          "y": 5.338542878627777,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "D2",
          "x": 6.583832144737244,
          "xshift": 10,
          "y": 15.902326202392578
         }
        ],
        "height": 750,
        "shapes": [
         {
          "line": {
           "color": "#CFD8DC",
           "width": 2
          },
          "type": "line",
          "x0": 6.583832144737244,
          "x1": 6.583832144737244,
          "y0": -5.225240445137024,
          "y1": 15.902326202392578
         },
         {
          "line": {
           "color": "#9E9E9E",
           "width": 2
          },
          "type": "line",
          "x0": -0.06762518882751464,
          "x1": 13.235289478302002,
          "y0": 5.338542878627777,
          "y1": 5.338542878627777
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Documents and Topics</b>",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1200,
        "xaxis": {
         "visible": false
        },
        "yaxis": {
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "key_topic_model.visualize_documents(docs_clo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning the model with MMR \n",
    "mmr_representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "mmr_topic_model = BERTopic(representation_model=mmr_representation_model,umap_model=umap_model, hdbscan_model=hdbscan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and fit MMR\n",
    "mmr_topics, mmr_probs = mmr_topic_model.fit_transform(docs_clo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>86</td>\n",
       "      <td>-1_so_with_do_its</td>\n",
       "      <td>[so, with, do, its, they, about, think, like, ...</td>\n",
       "      <td>[['Yeah. I mean, very much the way I would han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0_ai_human_we_lot</td>\n",
       "      <td>[ai, human, we, lot, be, you, really, team, da...</td>\n",
       "      <td>[[\"Yeah. And even just the tendency, like one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1_in_data_ai_so</td>\n",
       "      <td>[in, data, ai, so, be, are, algorithm, think, ...</td>\n",
       "      <td>[[\"Yeah. You look at AI used to make decisions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2_to_ai_we_people</td>\n",
       "      <td>[to, ai, we, people, about, you, because, thin...</td>\n",
       "      <td>[['Okay, here\\'s how I\\'m going to think about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>3_think_it_start_like</td>\n",
       "      <td>[think, it, start, like, understand, should, y...</td>\n",
       "      <td>[[\"I think to not be fearful of it, but to und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>4_that_information_kids_think</td>\n",
       "      <td>[that, information, kids, think, like, learnin...</td>\n",
       "      <td>[[\"I mean, I understand that. So going off of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>5_to_robot_we_work</td>\n",
       "      <td>[to, robot, we, work, needs, be, do, say, spee...</td>\n",
       "      <td>[['A bit. Yeah. So the primary ones that I\\'d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>6_computer_so_science_ai</td>\n",
       "      <td>[computer, so, science, ai, at, school, tech, ...</td>\n",
       "      <td>[[\"Yeah. So pretty much been an academic in tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>7_think_alexa_kids_she</td>\n",
       "      <td>[think, alexa, kids, she, have, learn, logic, ...</td>\n",
       "      <td>[[\"For kids, especially younger, the more tact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8_to_about_like_they</td>\n",
       "      <td>[to, about, like, they, what, thats, netflix, ...</td>\n",
       "      <td>[['Yeah. Absolutely. You answered everything. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                           Name  \\\n",
       "0     -1     86              -1_so_with_do_its   \n",
       "1      0     27              0_ai_human_we_lot   \n",
       "2      1     27                1_in_data_ai_so   \n",
       "3      2     27              2_to_ai_we_people   \n",
       "4      3     23          3_think_it_start_like   \n",
       "5      4     13  4_that_information_kids_think   \n",
       "6      5     13             5_to_robot_we_work   \n",
       "7      6     13       6_computer_so_science_ai   \n",
       "8      7     10         7_think_alexa_kids_she   \n",
       "9      8      9           8_to_about_like_they   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [so, with, do, its, they, about, think, like, ...   \n",
       "1  [ai, human, we, lot, be, you, really, team, da...   \n",
       "2  [in, data, ai, so, be, are, algorithm, think, ...   \n",
       "3  [to, ai, we, people, about, you, because, thin...   \n",
       "4  [think, it, start, like, understand, should, y...   \n",
       "5  [that, information, kids, think, like, learnin...   \n",
       "6  [to, robot, we, work, needs, be, do, say, spee...   \n",
       "7  [computer, so, science, ai, at, school, tech, ...   \n",
       "8  [think, alexa, kids, she, have, learn, logic, ...   \n",
       "9  [to, about, like, they, what, thats, netflix, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [['Yeah. I mean, very much the way I would han...  \n",
       "1  [[\"Yeah. And even just the tendency, like one ...  \n",
       "2  [[\"Yeah. You look at AI used to make decisions...  \n",
       "3  [['Okay, here\\'s how I\\'m going to think about...  \n",
       "4  [[\"I think to not be fearful of it, but to und...  \n",
       "5  [[\"I mean, I understand that. So going off of ...  \n",
       "6  [['A bit. Yeah. So the primary ones that I\\'d ...  \n",
       "7  [[\"Yeah. So pretty much been an academic in tr...  \n",
       "8  [[\"For kids, especially younger, the more tact...  \n",
       "9  [['Yeah. Absolutely. You answered everything. ...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequent topic using MMR\n",
    "mmr_topic_model.get_topic_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.027837430532092585,
          0.02853900686931092,
          0.037918213830990394,
          0.04034433080308994,
          0.058980731958969024
         ],
         "xaxis": "x",
         "y": [
          "be  ",
          "lot  ",
          "we  ",
          "human  ",
          "ai  "
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#0072B2"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.03086495246090259,
          0.03201434185069841,
          0.0350828774393521,
          0.03639391135691038,
          0.04171045896360776
         ],
         "xaxis": "x2",
         "y": [
          "be  ",
          "so  ",
          "ai  ",
          "data  ",
          "in  "
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "color": "#CC79A7"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.02404761767282257,
          0.024191646833710258,
          0.025193069525932872,
          0.030254763502572377,
          0.04711059853239149
         ],
         "xaxis": "x3",
         "y": [
          "about  ",
          "people  ",
          "we  ",
          "ai  ",
          "to  "
         ],
         "yaxis": "y3"
        },
        {
         "marker": {
          "color": "#E69F00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.026887887660553726,
          0.02951813381004908,
          0.0352976588753499,
          0.03731255224051866,
          0.06214695150032777
         ],
         "xaxis": "x4",
         "y": [
          "understand  ",
          "like  ",
          "start  ",
          "it  ",
          "think  "
         ],
         "yaxis": "y4"
        },
        {
         "marker": {
          "color": "#56B4E9"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.03473297633733259,
          0.03559859980639634,
          0.04058592530231335,
          0.04579406292730118,
          0.06324591351003414
         ],
         "xaxis": "x5",
         "y": [
          "like  ",
          "think  ",
          "kids  ",
          "information  ",
          "that  "
         ],
         "yaxis": "y5"
        },
        {
         "marker": {
          "color": "#009E73"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.02433887410715171,
          0.025997552486789824,
          0.028853782551482982,
          0.034648743326606404,
          0.04776111181443943
         ],
         "xaxis": "x6",
         "y": [
          "needs  ",
          "work  ",
          "we  ",
          "robot  ",
          "to  "
         ],
         "yaxis": "y6"
        },
        {
         "marker": {
          "color": "#F0E442"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.028470610976630478,
          0.032414322843911436,
          0.03980991731922995,
          0.042937538623433906,
          0.04364618256802211
         ],
         "xaxis": "x7",
         "y": [
          "at  ",
          "ai  ",
          "science  ",
          "so  ",
          "computer  "
         ],
         "yaxis": "y7"
        },
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.03090369486859391,
          0.03273831748163206,
          0.034612345745401084,
          0.040595325661753924,
          0.04063143094508965
         ],
         "xaxis": "x8",
         "y": [
          "have  ",
          "she  ",
          "kids  ",
          "alexa  ",
          "think  "
         ],
         "yaxis": "y8"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 0",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 1",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 2",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 3",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 4",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 5",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 6",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 7",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 500,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "Topic Word Scores",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis7": {
         "anchor": "y7",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis8": {
         "anchor": "y8",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis8": {
         "anchor": "x8",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mmr_topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that.\"]",
          "[\"Actually, I was surprised. I remember when I learned programming, I think I started learning programming after I went to college. The first year freshman, I feel like currently kids, they learn. They actually have the channels to interact with different technologies really early. They probably have a Google home or Siri on their mom or dad's phone or like their iPad or so. So I feel they're exposed to those advanced technologies, including machine learning or artificial intelligence a lot. And I feel like, I'm not sure, I know high school kids definitely learn programming. I'm not totally sure about middle school kids, but I would say even if it's not in school, they still have a lot of chance to interact with technologies. Not necessarily to learn how it works, but more just to get to know it. And I currently don't see, at least I haven't really thought about the harm of they learning machine learning at a really young age. There might be [inaudible 00:17:12] issues. I'm not sure, but I think they might trust more than we do. I don't really trust the Google, not trust, it's more like, I just don't feel very comfortable having Google home listening to my voice, but they probably got the young generation because they interacted with technologies much earlier than we did. So that might have built the trust in machine learning or artificial intelligence.\"]",
          "['Yeah. That makes a lot of sense. What do you think is the most important things that you should know about with AI and machine learning? | Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "[\"Sure. Yeah. Currently an assistant professor in computer science and learning sciences at University of Illinois, Chicago. The work I do is in social robots for educational purposes that are designed as learning companions, where they work with or around kids to help them learn in lots of different scenarios. My training with it is... My PhD's in learning sciences from University of Wisconsin, but I have a PhD minor in computer science where I did a lot of human robot interaction work. So I'm trained on the HR, the design of the robots and the interactions, and on learning and learning theory. And so the combination of those two is to design learning interactions for kids to really enhance their learning experiences. | And what sort of technology or programming do you use for these robots or have you been using? Do you use artificial intelligence or machine learning?\"]",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay. | Well, okay. So you\\'ve already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything?']",
          "['Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "[\"I mean, I'm a cynic in this regard. I think everything in this country and this world is motivated by money and financial interest, and corporate entities maintain that at the highest level. And depending on the governments, they either promote that or try to curb that back. But at the end of the day, it's all about money. So I think when you talk about AI and dealing with the problems that are dealt with, with AI, you have to understand that people are making money off of AI. Even if it's terrible AI in terms of being this most biased, terrible agent in the world, someone's going to make money off of it potentially. And I'm a cynic in that regard. People will put money above much of their own ethics in some cases. | Yeah. Yeah. I think so too. And they're so creative, the kids that ... All kids really, but the kids we're working with are just phenomenal. And once they get into it, they design such amazing things to help people. They're really interested in designing robots to help and robots for social good. So they're really understanding this stuff. And even if the goal is not for them to go be computer scientists or to go build these, but to have that fundamental understanding so they can be critical consumers, so they stop and say, wait, this is wrong. I need to say something. That's what we're hoping, obviously. I mean, we're not going to follow them, but we're hoping that what we do has a little bit of that impact anyway.\"]",
          "['Yeah, a weird one. I was looking at job applications recently. I was just on LinkedIn, saw a few and this is a very interesting one. There\\'s a weird and this is where my dissertation\\'s about acceptance and over-reliance and things like that. And there\\'s a weird over-reliance for some of these things that creates someone missing the point almost. So I was looking at data science positions and they were, \"oh yeah, so data science work, you need to be able to do machine learning stuff for data science and predictive analysis.\" And so for that, they\\'re something called random force generation, which is a machine learning technique where you just put a bunch of data in and then say what you want to predict. And then it predicts the stuff with a bunch of complicated math and that\\'s all fine and dandy and it works out great. But you can also do that with basic statistical analysis. So it\\'s almost one of the concerns I run into is, people use it when they don\\'t need to like, don\\'t use a tool that isn\\'t needed or don\\'t use a tool that\\'s too much for the issue almost, which is an issue I\\'ve seen. And something I run into as a larger issue is over designing and over implementing some of that tech. | No, I actually feel that because I recently joined Golnaz\\'s lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations?']",
          "['Yeah. Absolutely. You answered everything. I\\'m looking at my follow-ups. I\\'m like, \"Oh, you got that. You got that.\" Yeah. I mean, if you want to expand a little bit, if you were to take Jules for example, right, your daughter, and she\\'s six, and what would you want her to know about machine learning or about at that age, anything or about how harmful it can be? Would you talk to her about privacy? Would you talk to her about misrepresentation or discrimination? Where would you go? | Yeah. One that just comes to mind that didn\\'t come up is self-driving car technology. And that is dangerous in a much more salient way, right? And I think getting people to understand, when I see a car and I look over, I evaluate what the person behind that wheel is attending to, right? Before I cross the street, I like to make sure that I see that person look at me so that I know that they\\'re sensing me, right? So if and when it comes to be that I look over and there\\'s nobody at that wheel, there really needs to be an acclimation to understanding, \"Okay, the situation is that there\\'s a machine driving that car. How does it make mistakes?\" I know how people make mistakes, right? I know what it is. If they didn\\'t look at me and I start to walk out and they just roll even at a red light, that could catch me. So I pay attention to that. What are the types of errors that machines make in that scenario are important, that we know that sometimes, it might not recognize a stop sign at all. People are typically better at it. Maybe not typically, but we learn to understand the types of errors that can be common in these systems, or even uncommon, if they\\'re going to be catastrophic, and are aware enough in a way that we can respond in reasonable actions to it. I mean, right now, if I saw a driverless car, I would go nowhere near the street because they\\'re prone to weird accidents that I don\\'t understand right now. So I can\\'t figure them out well enough to do it. But eventually, if they become commonplace, we\\'ll have to know how that works.']",
          "[\"Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "['Trying to think. The first thing that jumps to my mind is that any of these algorithms is responding to you and your digital choices, I guess. The importance of their data, and I know the erosion of privacy or the apathy towards the erosion of our privacy is continuing. I\\'m trying to think. I\\'m not sure how to teach people about things that are going to... or how to teach the youth about issues of ethics and bias because being involved in that conversation, I don\\'t know. It requires, like we were talking about... I don\\'t know, it\\'s just hard to teach kids, I feel like. Those are hard topics to broach. And so, I\\'m thinking that might be something restricted to like high school, but I don\\'t know. The sooner you start teaching things to kids the better, because they have continual reinforcement throughout all of their years. I don\\'t know. What do you think? How do we teach kids concepts like that? | Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "[\"I think it's going to be a lot easier than we think just because they're so ingrained in the technology itself. My daughter is just about to turn one and she can already swipe on a tablet because she's watches her brother swipe on a tablet. And I think just those simple movement to think, they're going to be so more used to it than we are getting used to it. I don't think it'll be that much of a stretch just relating it to the devices they already use or have , I mean, even Alexa is a type of AI, because it does learn things about you, it learns to pick up your voice and your inflections and specific words and what you like. So, I mean just being able to talk about the everyday devices that they use is going to be easy for them. | Yeah, I mean, I just have to be broad as close to them as they can like little things like, hey, you can't turn on this device in your sister's room and listen, because there's a privacy implication there. Just don't make it about huge asylum impact. It's got to be close ecosystems to themselves.\"]",
          "[\"Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these- | It depends on the context. So if it's a Google image search engine, many kids say, yeah, I'm not represented here, but that's how the world is. The consequence isn't huge. I search for computer science professor and I don't see myself as a African American woman. Okay, that hurts. But I don't think Google should mess with that. Then when it comes to hiring algorithms, yeah, they see that as problematic. So the context really matters and the effect of the consequence as they see it matters to them.\"]",
          "[\"I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that. | Oh, I think it's super important. And I think that should be right with lesson number one. I mean, especially with machine learning, because again machine learning is algorithms that are trained on data and the data has to come from somewhere. And so there's the ethical questions around where does the data come from? And there are ethical questions of when someone's going to use this algorithm, this model. And so where are they going to use it? Who's going to use it? What are they going to use it for? So I think those are really important just to talk about early, I would say.\"]",
          "['I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic.']",
          "[\"No, I actually feel that because I recently joined Golnaz's lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations? | Yeah. I mean, we've actually already seen that with some of like there's AI robots that Ford uses in their factories and stuff. So we've already seen that happening and I'm sure it's going to grow from here. So same kind of question, but in reverse. So how do advances in AI or machine learning harm us and are there any particular populations that it harms? Oh, oh no. Okay. All right. I'm going to go ahead and ask the question again just in case. So how do advances in AI or machine learning harm us, harm humans and who in particular is it harming?\"]",
          "[\"Yeah. Yeah. I think so too. And they're so creative, the kids that ... All kids really, but the kids we're working with are just phenomenal. And once they get into it, they design such amazing things to help people. They're really interested in designing robots to help and robots for social good. So they're really understanding this stuff. And even if the goal is not for them to go be computer scientists or to go build these, but to have that fundamental understanding so they can be critical consumers, so they stop and say, wait, this is wrong. I need to say something. That's what we're hoping, obviously. I mean, we're not going to follow them, but we're hoping that what we do has a little bit of that impact anyway.\"]",
          "[\"It's the same bucket. If it's a random person on the street, one, why the fuck are they talking to you about it, machine learning. Usually when we have that discussion, the technical discussion of AI versus machine learning, machine learning might be seen as a sub-case of AI a broad category and machine learning being a set of particular techniques for how we optimize AI. So having the machine learn from data, again, a big metaphor about comparing the human brain to computers, which is a bad thing to do for your own wellbeing. We aren't machines. Machines don't think like us. We call it... this is from Ellen Ullman's Close to the Machine, fantastic book. It's a memoir. I have it with me. And she talks about how we call it the machine. We call it a memory, but it's not right. So one, I don't like the name machine learning when people use it too much to compare to how babies. I'm like no, fuck, that's not it. But a way of optimizing around data, and so it's just really cool statistics. And it's really close to data science, where you're like you have data, but what meaning can you get out of this for a social reason? What's blind on this? I don't really care about the difference between, but then AI a more general label. If I have a bunch of if/then statements. Is that AI? Well, yeah, it used to be. We were trying to solve like chess originally or checkers. We had some pretty simple sequence of steps because that's how we thought the human brain worked or at least that area thought things worked. So I don't know. I use them broadly because I don't think the specifics matter. It's when we get to the ethics side of things. | No that's great, thank you. So this might go more into some of your ethics background, but how do some advances in AI or machine learning help humans?\"]",
          "[\"But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam.\"]",
          "[\"Potentially everyone. I think there's a lot of promise and therapy and education and lots of places. | Awesome.  I'm kind of switching gears a little bit, what are your thoughts about youth learning about AI and machine learning?\"]",
          "['Well, that\\'s one of the reason we\\'re interviewing all of you, is because we\\'re trying to create a tool to help teach young children or young people about machine learning and AI and hopefully integrate some social and ethical issues into that. That\\'s our goal, is that we\\'re trying to figure out how to do that. Do you have any ideas of games or resources that you\\'ve seen that engage youth in these kinds of conversations or just conversations about AI or machine learning? | Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty.']",
          "['Well, as we said before we started recording I\\'m a tired, stressed grad student. I do a lot. I have a bajillion hats. I\\'m from Kentucky, before here and before that I lived in Georgia. In Kentucky, I got a Master\\'s in Computer Science and a Master\\'s in English. I worked at Fruit of the Loom, whose headquarters was in Bowling Green, Kentucky, where I was. And I did database stuff for them for six months. And that team, it was kind of rite of passage to accidentally lose the company a million dollars because they make enough that that\\'s laughable. And we\\'re just like, \"Holy shit, I\\'m going to get fired.\" And they\\'re like, \"No, no, no, we\\'ve all done it. Let\\'s go fix this.\" So I quit that job because I wanted to teach, and a full-time teaching position opened at the community college where I was an adjunct. So I taught for three years while I did my English masters and I moved here. So I do summer camps with kids and stuff, something I started doing in Kentucky that I still do here with [Wickedy 00:01:53] here. It was called VAMPY in Kentucky. I don\\'t know what it is about making fun five letter acronym names for these things. So I work with Wickedy here and [Bedra 00:02:03] pre-college, doing stuff with high school students. The youngest I\\'ve done is fourth graders all the way up to 60 year old guy changing careers for the 12th time in the community college, teaching them how to program. Or last summer I was just fucking bored from the pandemics. Well, I get to pick the topics. I\\'m going to have fun with this. We made art in class, but we wrote code to generate art for us. And these kids from Korea are fucking phenomenal, just putting that out there. They blew us all away with the stuff they did. Let\\'s see...']",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything? | Yeah. I mean, that's definitely related because a lot of AI and ML technologies surveil us, right, or take that data and then do something with it. So in that this case, it's about the data collection and how comfortable we are.\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that.\"]",
          "['Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick. | Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "[\"Well, that's one of the reason we're interviewing all of you, is because we're trying to create a tool to help teach young children or young people about machine learning and AI and hopefully integrate some social and ethical issues into that. That's our goal, is that we're trying to figure out how to do that. Do you have any ideas of games or resources that you've seen that engage youth in these kinds of conversations or just conversations about AI or machine learning? | I mean, just pull anything out. I mean, a phone. This Google Home I have here, AI is everywhere. There is not a thing... I'm sure there's an AI processing algorithm in my camera right now that is doing something with my face. Any browser, your email. I mean, it's everywhere. Getting them to connect that training data and all this data that AI uses to continually teach itself and learn is coming from you and it's coming from everywhere. That might be a little scary for them to think about. But I think it's important to know.\"]",
          "['I can only steal ideas for that because I usually don\\'t work with kids that young. I try to avoid compulsory schooling age stuff because I\\'m just like there\\'s so much more to consider and so many more standards you have to meet, which is why summer camps are good because you can do whatever you want within reason. But there is this book. I wish this had the fun cover version of it. It\\'s Living in data by Jer Thorp. And he\\'s an installation artist. It\\'s fascinating fucking reading. You should really find a recording of it if he has any book talks. It\\'s the same stuff in the book. But he says it and it has pictures.He\\'s an installation artist. Him and his group of his grad students, one thing they did is they did this installation in New York, right on a city street. And it\\'s this heart. You look at it, it\\'s a heart. But it\\'s all these pipes that are taller or shorter or whatever. It makes this heart shape. And people are like, \"Cool, we\\'ll take selfies in front of it.\" And they go look at it closer. And it\\'s a bar chart. And it labels on there where the population in New York has come from, from around the world.So then he sees people taking selfies with the bar, finding where their family is from. And then they\\'ve caught two people having weddings in front of it. And he\\'s like, \"This is the world\\'s only bar chart that has also been a wedding venue.\" And it\\'s just fascinating. He has all these different ways of getting people to experience data differently as a way to get out of their head and think about things in a really cool way. So I asked what his views on data ethics were. He said, \"Well, I have a 300 page answer to it. It\\'s called the book.\" So I had to get the book and fuck, there was another one. There is this fun activity that I saw someone do and post results of where they had real young kids. I can\\'t remember what age he said, but it had them draw what they thought Alexa looks like. Just take this disembodied voice that we as adults might take for granted and say, \"Well, kids, how do you think this looks?\" Something else that Jer Thorp did is working with kids, I just remembered this, is he had the students draw on big printouts of maps of their city, things that were important to them or things that they had noticed, like where are broken sidewalks? Where is the good place to get food? All these things that have meaning to the kids as they\\'ve experienced their own city. And they take this giant kid map and overlay it on old voting things. And you can see, well, here\\'s the red lining that happened. Here\\'s the history that you can see in the voting record aligning from forever ago with what you could see in your day to day life. And it was just this fascinating moment of overlaying and you go, \"Oh fuck.\" You don\\'t know that that\\'s what you\\'re drawing, but it\\'s what you\\'re drawing. And then they overlay it. It\\'s like, well, there you go. This data has all a long history and you can see it kids. So that I thought was really cool.  | I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI. ']",
          "[\"Yeah. In my work, not really. Not in an AI/ML way, one of the things we do is we'll set up auto video recording when kids are reading with a robot in the home. And so that was when we spent a lot of time thinking about and working with families and what's going to make them comfortable. And most of them were pretty fine with it. They just wanted it to be really, really clear when recording was happening. So it's actually impacts the choice of robots that I use. So the Misty robot I have has a little LED on its advisor that we can make really bright. And so it's there because when it's video recording, and it actually has a nice scene, but it's also really good indicator so that we train families to know that if you see that light, it's video recording. And we also train the families to know exactly how to shut that off in a one-touch thing. There's a spot on the robot that if you touch it there, the video recording will shut down.We also give them free license if they don't want to do that and just want to snap the whole thing off, that's okay too. So it's not in an AI/ML way, but in a privacy ethics issue, that's probably the number one thing I think we run into with my work. | Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "[\"But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam. | Those are even still around. Yeah. Those are really fun. And that's-by high school coding class we did Java programming for Mindstorms and it was really interesting, but there's so much more you could do with it after that. So if you had kids start with it and grow up with it, the potential they have there is pretty cool I think.\"]",
          "[\"So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful. | Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.\"]",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything? | Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "['Those are even still around. Yeah. Those are really fun. And that\\'s-by high school coding class we did Java programming for Mindstorms and it was really interesting, but there\\'s so much more you could do with it after that. So if you had kids start with it and grow up with it, the potential they have there is pretty cool I think. | Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences.']",
          "[\"I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that. | Yeah. Just teach the basics. That makes a lot of sense. So what are your thoughts about youth learning or youth being introduced to some of the ethical or social issues around AI and machine learning?\"]",
          "[\"Okay, great. And then, so related to what we were talking about, what about ethical or social issues that we discussed or even beyond that maybe we haven't discussed, how would you, what are your thoughts about youth learning about that in conjunction with AI and ML? | Yeah, absolutely. Kids are getting Chromebooks in second grade now. The schools are distributing. We just talked about our children, interacting with Alexa that's a intelligent system, so.\"]",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything? | Yeah. In my work, not really. Not in an AI/ML way, one of the things we do is we'll set up auto video recording when kids are reading with a robot in the home. And so that was when we spent a lot of time thinking about and working with families and what's going to make them comfortable. And most of them were pretty fine with it. They just wanted it to be really, really clear when recording was happening. So it's actually impacts the choice of robots that I use. So the Misty robot I have has a little LED on its advisor that we can make really bright. And so it's there because when it's video recording, and it actually has a nice scene, but it's also really good indicator so that we train families to know that if you see that light, it's video recording. And we also train the families to know exactly how to shut that off in a one-touch thing. There's a spot on the robot that if you touch it there, the video recording will shut down.We also give them free license if they don't want to do that and just want to snap the whole thing off, that's okay too. So it's not in an AI/ML way, but in a privacy ethics issue, that's probably the number one thing I think we run into with my work.\"]",
          "['Sure. Yeah. Currently an assistant professor in computer science and learning sciences at University of Illinois, Chicago. The work I do is in social robots for educational purposes that are designed as learning companions, where they work with or around kids to help them learn in lots of different scenarios. My training with it is... My PhD\\'s in learning sciences from University of Wisconsin, but I have a PhD minor in computer science where I did a lot of human robot interaction work. So I\\'m trained on the HR, the design of the robots and the interactions, and on learning and learning theory. And so the combination of those two is to design learning interactions for kids to really enhance their learning experiences. | A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on.']",
          "['Yeah. In that sense, that I think, I would fall in line with a little bit more traditional perspective, because machine learning doesn\\'t necessarily always have to... It\\'s not is a user-facing part of what\\'s happening. So it\\'s essentially how to take a bunch of inputs, teaching a machine how to interpret those inputs, and to organize, categorize, or plan actions based on those inputs. So it\\'s different levels of black boxiness that go along with it. But yeah, it\\'s essentially the training machines to have a space in between input and output that is nonlinear, I guess. So I mean, the traditional perspective is you have this set of data that\\'s coded with these sets. And so you train that way. And then a new set of data that isn\\'t coded, the machine should take what it learned from this first one, apply it there, and come up with the same codes. Those codes could then be actions to do. Those codes could be categories. Those codes could be things like emotions, right? So, \"Here\\'s 10,000 pictures of people who look angry. Here\\'s 10,000 more. Which of these are angry?\" kind of thing. So that, I think. And now that I\\'m thinking about it, that\\'s almost sneaky or more problematic sometimes because you don\\'t necessarily always have the user interacting with it while you\\'re developing these things and testing them. And it can be to such a scale sometimes that the errors and the problems in there are easy to miss, right? That, \"Hey, we got 99% accuracy,\" but that means if there\\'s 100,000 images in that set that you\\'re classifying on, 1% is actually a lot. And if that 1% impacts me and you\\'re just going to take this thing off the shelf that\\'s 99% accurate, and you\\'re going to take it off the shelf, and it\\'s going to make a medical diagnosis, and I get the 1% problem, that\\'s pretty impactful. So again, that\\'s one of the things I talk a lot with graduate students who are like, \"Oh, I\\'ll just grab the thing and we\\'ll just figure it out. It\\'ll tell us what to do.\" No, that\\'s not safe in a lot of the things that we\\'re doing. So, yeah. So I guess, I don\\'t know. I mean, that\\'s a too-long explanation of what machine learning is. ']",
          "[\"No, I actually feel that because I recently joined Golnaz's lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations? | Yeah, so I think piggybacking off of my last answer, I think the most optimistic thing is work future and enabling a work future. But that is also simultaneously the worst aspect of it if done incorrectly. So generally speaking, I like the idea of relieving human workload, but I think the larger concern is in actual phasing out of them from a work perspective. So in other words, an over-reliance on a machine system for the sake of using a machine system as opposed to a human I think is a main issue. And I think ultimately it affects lower level, blue collar workers more and it affects a larger population that has received less expertise, education, throughout their lifestyle, throughout their entire life. So for instance, a PhD individual would probably not have a larger issue with that, given that the specialized area of knowledge that they received, but someone who has a more general work area where the knowledge application is not as deep, but it's a task based knowledge. I think that's the person that has the greatest ability to impact. And I think ultimately it comes down to who we let use that system because there's easy ways to mitigate that, creating systems where for instance, there's the idea of creating head to access for robots, whereas the utilization of a robotic employee doesn't allow you to just skirt the idea of paying for that employee. It still should be something that CRE, it is something you use in generating revenues, therefore it's something that needs to also be accounted for. So I think that's the main issue, the work future I want is also a good and a bad thing, depending on who's in charge of it.\"]",
          "[\"Yeah, I mean, I just have to be broad as close to them as they can like little things like, hey, you can't turn on this device in your sister's room and listen, because there's a privacy implication there. Just don't make it about huge asylum impact. It's got to be close ecosystems to themselves. | Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas?\"]",
          "[\"So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful.\"]",
          "[\"Well, okay. So you've already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything?\"]",
          "[\"So I'm currently a fourth year PhD student in human centered computing, where we basically just study how humans interact with different technologies. And in our lab, our research focus is mainly human AI teams, human AI teaming. So we basically study how humans interact with AI teammates in a given environment like gaming, where AI is pretty common to see. And before that I did my bachelor and master in engineering. So basically it's kind of like the algorithm behind the thing. That's basically my background.\"]",
          "[\"Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right? | Oh yeah. My son figured out, I guess about a month ago, how to actually tell Alexa how to do things. And now it's hilarious because I have the app on my phone where I can see all the devices and I can see what Alexa, hears him saying.\"]",
          "['Oh yeah. Well, so for the most part, I have a relatively Wisconsin accent. And so the models that we used were... we tried to do was out of Carnegie Mellon. I forget the name of the program that did it, but it was one that could run on board. And that did relatively standard US dialect, was generally really successful. The student who was running that was from India, though, and her accent was completely inaccessible to the robot. And then she worked with another student who was Korean and also really struggled with her accent as well. So between the two of them, they were just really frustrated trying to get... And we were just trying to set up simple keyword kind of things like, \"Hey, Alexa,\" kind of stuff. We were just trying to do simple keywords with the robot, and it got nowhere. So that was the Carnegie Mellon one. And then we\\'ve been a little bit more successful. I haven\\'t used it, but the folks I worked with at Wisconsin who followed up with some of this, they did add some keywords to our last field-based one over the summer. And I don\\'t know if they tested it much as far as accents go. I know one of the researchers on the team is Turkish, and she didn\\'t seem to have much trouble. But again, we really constrained the word list for that. And the cloud-based ones I think are a little bit better now than the ones we were... Sync, that\\'s the CMU one that we were using. So I think they\\'re a little bit better now. And so that\\'s one that has been starting to get cooked into the robots I have. I just haven\\'t sent them out in the field doing that yet, but just simple, yes, no kind of stuff seems to work, and that hasn\\'t... We haven\\'t seen any evidence that that level has been impacted by any accent or dialects.  | Yeah. I mean, it\\'s essentially... So it has to appear intelligent, I think, right? So I don\\'t know if it has to actually be intelligent. And so that\\'s where I\\'d classify some of the things that I do that are... If you really dig down into it, it would probably be classified in algorithms, right? It\\'s an algorithm. So I did a book selection algorithm. But I think it\\'s important to think about the user\\'s perspective on these things. And so I have a pretty broad idea about AI in that if it appears intelligent, if it comes across as intelligent... I mean, maybe if it even wasn\\'t the intent that it comes across as intelligent, but it appears that way, I think we have to treat it as AI because of the impact it would have on the person that it\\'s interacting with, right? So my book selection algorithm is not complicated at all. It\\'s essentially like a sorting algorithm and we add in a couple of inputs about what the kids\\' book preferences are like, their reading skill level, the amount of time they read, that kind of stuff. And then we tag books for all those features, and we just make a priority queue out of it. Really simple, early CS stuff. But to the kids, it came across as intelligent. And I think that\\'s the key factor, that when we interviewed them afterwards, kids felt that the robot was paying attention to them and that the suggestions the robot made for books were personal, that they were about them. And so that to me is now where you\\'re in artificial intelligence and you really have to then take that seriously, because if the person believes that they\\'re working with an intelligent machine, then you have to treat that carefully.']",
          "['Hmm. I think kids would probably always like to use... I remember when I was a kid, a chat bot was something that was super cool to me. And that is something, I\\'m sure when I was a kid probably not even close to being actually, it was probably just a self-programmed expert system or... oh gosh, I don\\'t even remember what to call it. It\\'s a certain type of algorithm. But OpenAI has a program called GTP-3 that you can play around with. It\\'s really cool. It\\'s the most advanced national language processing algorithm in the world. And that is something that I\\'m sure kids would have a lot of fun exploring, honestly, because you can look at the different models that they have and they.-you can go as depth into detail as you want. I think that would be really cool. I think teaching kids how to... I think reinforcement learning programming is also another good avenue. I did a educational component thing for this fellowship that I was on, and I basically made this video to teach middle schoolers how to make a reinforcement learning AI or it\\'s really just a reinforcement learning model, and the programming language was Python, but the library that it was implemented on was Tensorforce. And it\\'s really not that hard. It\\'s so much easier than you\\'d think. I basically coded everything up and just added it to where they would enter one or two things and be able to see how it changes. I think that, obviously somebody can do it better than I can. They probably still would\\'ve had a very hard time figuring out how to do it. But I think especially as these libraries continue to develop and become even easier, you can do, especially with machine learning models, you can do some of those in two lines of code. So, I think that would be a really good way. It might be something for only middle school and high school, but you might be able to bring it down to the elementary level and just hold their hand or just show them on the screen, getting those kids to actually see the process of how these models get made and how they can be implemented for useful features. Or there you go. There\\'s a way to show them bias. You can do two models based on two different sets of training data and you can show them, \"Hey, do you see how this one outputs basically bias and incorrect answers, whereas this one with the more representative training set does make better decisions that more humans would make?\" And then, it also shows them that AI aren\\'t perfect impartial beings. They are subject to the same flaws that we are, especially if we\\'re giving them training data that we make. But yeah, I think that\\'s probably the best way to get down into the nitty-gritty. | I mean, just pull anything out. I mean, a phone. This Google Home I have here, AI is everywhere. There is not a thing... I\\'m sure there\\'s an AI processing algorithm in my camera right now that is doing something with my face. Any browser, your email. I mean, it\\'s everywhere. Getting them to connect that training data and all this data that AI uses to continually teach itself and learn is coming from you and it\\'s coming from everywhere. That might be a little scary for them to think about. But I think it\\'s important to know.']",
          "[\"I mean, just pull anything out. I mean, a phone. This Google Home I have here, AI is everywhere. There is not a thing... I'm sure there's an AI processing algorithm in my camera right now that is doing something with my face. Any browser, your email. I mean, it's everywhere. Getting them to connect that training data and all this data that AI uses to continually teach itself and learn is coming from you and it's coming from everywhere. That might be a little scary for them to think about. But I think it's important to know.\"]",
          "[\"Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n | Yeah. And you haven't had any problems recognizing faces. It recognizes kids, all different kinds of kids?\"]",
          "['That\\'s awesome. I mean we need more people to understand coding in general. So I think it\\'s great that you do that work. Has any of the work you\\'ve done with those type of groups ever bridged into AI before or machine learning? | So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]-']",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that. | Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say.\"]",
          "['Yeah. I love that, Joe. And I can see that in your work, the way you\\'ve described it. That\\'s great. Okay. So let\\'s switch. So talking a little bit about why we\\'re doing this interview, right? We\\'re taking what people are saying and trying to apply it for learning experiences for young people. So what are your thoughts just generally about elementary school, middle school-age kids learning about either AI machine learning and the social and ethical impacts or both? [crosstalk 00:29:56] ideas around that, what they should learn, what\\'s important for them to know? Can they [crosstalk 00:30:00] in those issues | Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick.']",
          "[\"Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right? | Yeah. Yeah, absolutely. It's getting a little better though. I have a young son too and Alexa is starting to understand him.\"]",
          "[\"Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important.\"]",
          "[\"I have to think. And I can maybe send you some links, but I know there's research going on, for example, because Microsoft owns Minecraft. There's research going on. I think they've released it as... But there's really research going on like how do you do reinforcement learning in Minecraft? How do you use Minecraft to create intelligent agents? And I think they've released tools and things that people can explore that on their own with Minecraft. So that's one thing. There's another project that comes to mind. I think it's called Make Code. I don't know. Is that a thing? Let me look really quick. I thought that has some element of... Oh this also has something to do with Minecraft, I guess. Microsoft free online learn to code platform. Yeah. So let me send you this link.\"]",
          "['So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]-']",
          "[\"Yeah, so work that I've done has been outlining how there are different camps I've worked in. Number one is outlining how humans and AI interact with each other, specifically my dissertation work is on how humans and AI systems can impact and influence each other in a task. So how a human could be susceptible to letting a robot or an AI system tell it what to do and take commands from it or vice versa. So looking at what comprises that, how humans should lead AI or how an AI should lead a human, things like that. So that trade off and then I've also done work looking at how ethics in AI systems can be implemented, created and how it interacts with humans. So, it's fairly big part is the ethical implications of AI systems and how those ethical implications ultimately impact the utility of AI. And then the last, there are other smaller things that I've had to do. But then the last main one I worked on is AI. There's a grant that I work on fairly often that is a grant that looks at using machine intelligence and recommender systems to provide recommendations to teachers for professional development. So it's a lot of providing recommendations for their professional development and I work on the side of that where I work on building this system and outline in that aspect. | Yeah, I'm actually familiar with that one, because it's the CU-TLP program. We have some people in the learning sciences that are working on that. So I'm familiar with this one. Would you mind giving me a little bit more detail about your project that involves AI ethics? What were some of the results that came out of that?\"]",
          "[\"We were talking about it a lot in recognition software and things like that, where the data sets that it's usually using to recognize and communicate with people is generally very Western white male and wrongly classifies people and communicates with them as if they were that group, which makes it very difficult for people to not only work but connect to it and get the benefits as so much to some other populations. | Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right?\"]",
          "[\"Yeah, I mean, I just have to be broad as close to them as they can like little things like, hey, you can't turn on this device in your sister's room and listen, because there's a privacy implication there. Just don't make it about huge asylum impact. It's got to be close ecosystems to themselves.\"]",
          "[\"Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess.\"]",
          "['So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]- | Yes, it\\'s a data ethics course. We talk about AI a fuck ton. So besides camps, teaching people how to code, I\\'m a PA right now with YJ Kim here at the Wisconsin Center for Ed Research and I know David Shaffer. And through him I\\'ve heard about goal like everyone else in this community. So I hear about Clemson all the time. But where was I going? So I have a PA ship here. I\\'ve TAed here, TA for code and power a lot with Dr. Royston and that\\'s kind of your classic critical theory approach to AI and a lot of... but like meritocracy, Google image search results for black women showing images of gorillas, those kinds of cases. Yeah, it\\'s kind of fucked up. So talk about how code and power get gets embedded. And it\\'s very critical around institutions and getting students to think about that stuff and learn about implicit biases and things like that. We actually have them take the implicit bias test and then think about the limitations of that test and try and reason about what data says. But-']",
          "['And what sort of technology or programming do you use for these robots or have you been using? Do you use artificial intelligence or machine learning? | Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay.']",
          "[\"Good. I mean that's fascinating. That's great. So you touched on the fact that you had some experience teaching young people about computer science. If you were going to teach, just think about what you could do if you were going to teach some young people about machine learning or AI, what type of activities or resources would you maybe use in order to do that? | I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that.\"]",
          "[\"Yeah. So it's like you start with that data set that's not very representative. And then you test it on a not very representative sample because those who aren't representative or represented in the data in the machine don't want to use it. So then you don't end up teaching it to be more inclusive, I guess. Is that right? | Yeah. It's like everyone's got an Alexa or something like that in their house. And it is vocal recognition software, but if it can't understand you, it can't understand you. So it already has some of that bias in it or it can't understand certain dialects and accents and things like that.\"]",
          "[\"Yeah. I mean, that's definitely related because a lot of AI and ML technologies surveil us, right, or take that data and then do something with it. So in that this case, it's about the data collection and how comfortable we are. | Yeah. It's part of the decision we've done for facial recognition, part privacy, but also part practicality. We only do frame sampling... I think it's every... Might even be every minute now. We used to do it about 20 seconds, but even that was a little bit tough on the processing, and it ended up capturing a lot of information from the family home. So we set it somewhere between 30 and 60 seconds now where instead of trying to real-time video or a lot of people do three frames, a second kind of stuff, we take a really slow approach to it. In parts, we collect less data. It means that the facial recognition stuff is a little bit slower to respond. But in our user testing, it hasn't been too much of a big deal. So again, in that trade-off of how precise we can get the face tracking stuff versus how much information we're collecting from a home, we balance those two things out. And again, we're really clear with families about how that works and what's going on with it and how they can disable it if they need to.\\n\"]",
          "[\"I have to think. And I can maybe send you some links, but I know there's research going on, for example, because Microsoft owns Minecraft. There's research going on. I think they've released it as... But there's really research going on like how do you do reinforcement learning in Minecraft? How do you use Minecraft to create intelligent agents? And I think they've released tools and things that people can explore that on their own with Minecraft. So that's one thing. There's another project that comes to mind. I think it's called Make Code. I don't know. Is that a thing? Let me look really quick. I thought that has some element of... Oh this also has something to do with Minecraft, I guess. Microsoft free online learn to code platform. Yeah. So let me send you this link. | So this is another kind of project that I remember seeing out of Microsoft Research. And it's about coding in general, I think and computer science, but I think it gets into some things that have some AI. So yeah, I think that those are the places I'd start.\"]",
          "['I can only steal ideas for that because I usually don\\'t work with kids that young. I try to avoid compulsory schooling age stuff because I\\'m just like there\\'s so much more to consider and so many more standards you have to meet, which is why summer camps are good because you can do whatever you want within reason. But there is this book. I wish this had the fun cover version of it. It\\'s Living in data by Jer Thorp. And he\\'s an installation artist. It\\'s fascinating fucking reading. You should really find a recording of it if he has any book talks. It\\'s the same stuff in the book. But he says it and it has pictures.He\\'s an installation artist. Him and his group of his grad students, one thing they did is they did this installation in New York, right on a city street. And it\\'s this heart. You look at it, it\\'s a heart. But it\\'s all these pipes that are taller or shorter or whatever. It makes this heart shape. And people are like, \"Cool, we\\'ll take selfies in front of it.\" And they go look at it closer. And it\\'s a bar chart. And it labels on there where the population in New York has come from, from around the world.So then he sees people taking selfies with the bar, finding where their family is from. And then they\\'ve caught two people having weddings in front of it. And he\\'s like, \"This is the world\\'s only bar chart that has also been a wedding venue.\" And it\\'s just fascinating. He has all these different ways of getting people to experience data differently as a way to get out of their head and think about things in a really cool way. So I asked what his views on data ethics were. He said, \"Well, I have a 300 page answer to it. It\\'s called the book.\" So I had to get the book and fuck, there was another one. There is this fun activity that I saw someone do and post results of where they had real young kids. I can\\'t remember what age he said, but it had them draw what they thought Alexa looks like. Just take this disembodied voice that we as adults might take for granted and say, \"Well, kids, how do you think this looks?\" Something else that Jer Thorp did is working with kids, I just remembered this, is he had the students draw on big printouts of maps of their city, things that were important to them or things that they had noticed, like where are broken sidewalks? Where is the good place to get food? All these things that have meaning to the kids as they\\'ve experienced their own city. And they take this giant kid map and overlay it on old voting things. And you can see, well, here\\'s the red lining that happened. Here\\'s the history that you can see in the voting record aligning from forever ago with what you could see in your day to day life. And it was just this fascinating moment of overlaying and you go, \"Oh fuck.\" You don\\'t know that that\\'s what you\\'re drawing, but it\\'s what you\\'re drawing. And then they overlay it. It\\'s like, well, there you go. This data has all a long history and you can see it kids. So that I thought was really cool. ']",
          "[\"Geez. Hopefully they help everyone. I don't know. I mean, I just see it as, it's like any technology, I mean it has the potential to increase human productivity. It has the potential to increase human collaboration, creativity. I don't know. It's hard to answer because I think there's so many different kinds of technology that you could lump under AI and it's almost like who's helped by computers? | Awesome.  I'm kind of switching gears a little bit, what are your thoughts about youth learning about AI and machine learning?\"]",
          "[\"Yeah. We have thought of that, integrating Snapchat filters or something like that. Something that they would be interested in. | Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess.\"]",
          "[\"Sure. Yeah. Currently an assistant professor in computer science and learning sciences at University of Illinois, Chicago. The work I do is in social robots for educational purposes that are designed as learning companions, where they work with or around kids to help them learn in lots of different scenarios. My training with it is... My PhD's in learning sciences from University of Wisconsin, but I have a PhD minor in computer science where I did a lot of human robot interaction work. So I'm trained on the HR, the design of the robots and the interactions, and on learning and learning theory. And so the combination of those two is to design learning interactions for kids to really enhance their learning experiences.\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that. | That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning?\"]",
          "['Yeah. And that also makes it to where it creates a mythos around it, that makes it really intimidating for some people like, \"is this new fancy amazing,\". Like that job application, I was like, \"yeah, it\\'s not hard to use, machine learning though.\" My brother learned it in a week and does it for his job, and he goes to his job and he tells his job, \"oh, I implement machine learning for this.\" And they\\'re just amazed by it. He learned that in two days. So it creates this weird thing where it is simultaneous, over-engineering the problem, but also intimidating new people from getting involved in it because it is considered magic. | No, I actually feel that because I recently joined Golnaz\\'s lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations?']",
          "['AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay. | Yeah. I mean, that\\'s definitely related because a lot of AI and ML technologies surveil us, right, or take that data and then do something with it. So in that this case, it\\'s about the data collection and how comfortable we are.']",
          "['Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick. | Yeah. Absolutely. You answered everything. I\\'m looking at my follow-ups. I\\'m like, \"Oh, you got that. You got that.\" Yeah. I mean, if you want to expand a little bit, if you were to take Jules for example, right, your daughter, and she\\'s six, and what would you want her to know about machine learning or about at that age, anything or about how harmful it can be? Would you talk to her about privacy? Would you talk to her about misrepresentation or discrimination? Where would you go?']",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay. | Yeah. In my work, not really. Not in an AI/ML way, one of the things we do is we\\'ll set up auto video recording when kids are reading with a robot in the home. And so that was when we spent a lot of time thinking about and working with families and what\\'s going to make them comfortable. And most of them were pretty fine with it. They just wanted it to be really, really clear when recording was happening. So it\\'s actually impacts the choice of robots that I use. So the Misty robot I have has a little LED on its advisor that we can make really bright. And so it\\'s there because when it\\'s video recording, and it actually has a nice scene, but it\\'s also really good indicator so that we train families to know that if you see that light, it\\'s video recording. And we also train the families to know exactly how to shut that off in a one-touch thing. There\\'s a spot on the robot that if you touch it there, the video recording will shut down.We also give them free license if they don\\'t want to do that and just want to snap the whole thing off, that\\'s okay too. So it\\'s not in an AI/ML way, but in a privacy ethics issue, that\\'s probably the number one thing I think we run into with my work.']",
          "[\"I would say it's a computer or an agent that can take information and learn from it and make prediction or decision based on their ease. I don't know, learning process. | I would say my machine learning to me, my understanding is it's kind of the algorithm behind the scene. I consider artificial intelligence as kind of like with, how to say, a subject being there. It could be virtual, it could be a robot being there, but machine learning is more like the algorithm behind it. Kind of like its mind or core. That kind of feeling. It's kind of like if I use human as an example, it's just like artificial intelligence is a human and machine learning is kind of his brain to think, to help it to learn and make predictions.\"]",
          "[\"Yeah, absolutely. I'm a researcher at Microsoft Research in Redmond. I've been here for about five years. And before that I was a PhD student at University of Wisconsin studying computer science, probably, but I was working kind of on human robot interaction and doing a lot that was kind of based on understanding social gaze behavior of people and how to design social gaze mechanisms for robots. And so at Microsoft Research, I'm a research scientist and I'm kind of in a group that focuses on broad problems in AI. My focus is more on still on interaction. So I work in an area that my group calls situated interaction or situated intelligence, where we're very interested in understanding how to design AI systems that can interact with people in kind of everyday settings. So it encompasses human robot interaction. I've done stuff with human virtual agent interaction, stuff in mixed reality. But the idea is how do you combine multiple AI technologies and actually build real systems that can interact with people? So there's an element of studying interaction and studying people and studying social science and then studying the AI and how to actually build systems that can interact with people effectively.\"]",
          "[\"I'm trying to remember. I sent it to my mom a while ago. If I ever find it, I will send you again. There was a really good group I found a while ago, that did coding education for individuals, but they split it up by grade and it was research group I think I have Kentucky or something, but they do second grade should learn this and this. And second grade is when they start them on scratch | But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam.\"]",
          "[\"But then after that is when they get higher and higher. If I'm trying to think about things that I mean, block coding is obviously the first thing that always comes to mind given its prevalence. But I could also think that something from a higher level that would be interesting is introducing them to something like GitHub, but not from the perspective of getting them to use it. But from the perspective getting them to look at things they might find interesting. GitHub is also a social media platform. It is tech repositories, there's so many things and looking at open source websites, source forage, websites that have cool projects, even something like Indestructible, where it's just looking at something that has a project associated with it. I love Kiwi Kits as well. Kiwi Kits are really cool.  Those are really fun things for me. I grew up on Lego Mindstorms. So I also like heavily push the concept. Because Lego Mindstorms are really cool because they start out with block coding and then after block coding, they have a full Java implementation. So you can do the full path and then if it has Java, that means it can have AI. So you can do the whole gambit, going from start to finish. So I think that could be, for me that is where it's at. Yeah, like scratch is my biggest one for early stuff. And then all Lego Mindstorms. Those are my jam. | Yeah I mean that is you just gave us a lot of good information Thank you so, can you think of any ways to help us connect these topics like thinking about data machine learning AI to their everyday lives and make it meaningful for them.\"]",
          "['That\\'s great, that\\'s a good perspective. So what do you think about youth learning about the ethical issues of AI? | AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "[\"Are you talking about just learning AI and ML in general or the ethical issues? | Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess.\"]",
          "[\"It depends on the context. So if it's a Google image search engine, many kids say, yeah, I'm not represented here, but that's how the world is. The consequence isn't huge. I search for computer science professor and I don't see myself as a African American woman. Okay, that hurts. But I don't think Google should mess with that. Then when it comes to hiring algorithms, yeah, they see that as problematic. So the context really matters and the effect of the consequence as they see it matters to them.\"]",
          "['Machine learning is an AI that essentially learns by itself and it improves by ingesting more data to perform a task with better efficiency and accuracy. That\\'s kind of what I think of it. That\\'s why when people start talking about training data, their eyes glass over, but it is about that. You\\'re just educating this AI, helping it to learn, like what a cat looks like. So that, when you say, \"Is this a cat?\", it knows what a cat looks like, and that\\'s what we see in image recognition and stuff like that. That\\'s why you can look for cats on the internet. It\\'s just that, more and more data, but the downside of it is, you\\'re assuming that what you\\'re feeding it is accurate and complete. | Yeah. It\\'ll definitely change the whole algorithm. Switching gears a little bit, we\\'re going to start talking about youth. What are your thoughts about youth learning about AI or machine learning?']",
          "['No, I actually feel that because I recently joined Golnaz\\'s lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations? | Yeah, I think there\\'s a couple things with that in terms of, if I\\'m thinking about helping humans, I think it\\'s more about something that\\'s really cool about how humans work is that a lot of times they find easy small problems and they figure out how to fix it. And then it turns out that problem is a really big problem because it affects a lot of people. And so AI systems create this interesting idea where in my opinion, AI systems have a very low barrier of entry or at least including machine learning, you can use machine learning stuff if you can take two weekend classes and I think get pretty up to speed with it if you have the right education and teaching resources behind it. And the key to that is that, you can start solving really small problems in your life really quickly with really simple systems. And eventually what that does is it enables people on a micro level to make their life a bit easier, but also understand technology from this smaller perspective. And then I have a second way as well, but in my head there\\'s a small piece there where it\\'s the toolbox for people to get involved in computing stuff, I think actually lowers with that. So it helps people make things easier. Because I would say back in the day, coding on C and Fortran and stuff was awful and computers were millions of dollars and now it\\'s, \"oh you can do very easy things with very little.\" So that\\'s that The second one I have is, more of on the large scale meta-perspective is I think one of the reasons I also got into AI and find it really interesting is I like to see it from a perspective like aiding humans in reducing workload and replacing a human systems within a group and not having humans pick up that work. So more of the idea of freeing up humans to do things that matter to them, but also freeing up humans to work on stuff that they find important. I think that\\'s the 10 years from now potential is, I think in terms of workload and work future it has the most potential to impact and improve people.']",
          "['Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay.']",
          "['Yeah. I mean, very much the way I would handle a science classroom or... I\\'ve never taught history, but a history classroom, so that we really need to make sure that part of the training is not just how to use it, but when to use it, why to use it, where it came from, where it\\'s going. All the other pieces around it are really crucial, so that it\\'s not just seen as a tool that you take off the shelf and you go do it, right? This isn\\'t just a ruler that you put down and you draw a line. It\\'s so much more complicated than that. Without really thinking about it, it\\'s easy to use it, though like I said, with graduate students, the conversation I have all the time like, \"Careful with that,\" right? And so these are dangerous tools. And so understanding that is part of the training. The first step, I think, is really understanding, particularly for machine learning, how these things are built, where they come from. And if I were designing something for elementary kids, it would be to demonstrate how impactful the representation in the training set impacts that output set. That\\'s really where I would start. So that\\'s the basis of all their knowledge about this, is how machine learning systems are created so that every time they do use it, they should hopefully be there thinking about how this was put together. So that would be the first step, the ontology part of it. How do these things come to be? And then the output part of it, what are the potential impacts that again, it reminds me of working in labs with science kids. \"Okay. So here\\'s this thing. What could go wrong and how are you going to mitigate that? And if it does go wrong, what do we do, right?\" Those are the questions that have to happen before you start even using basic glassware in a science lab. And so I think that those are the conversations that need to happen, that these need to be treated as complex and potentially damaging tools. And so even at a young age, kids got it. Kids totally get it, right? They have no problem. My six-year-old gets it with some of this stuff. She knows the tools around the house that are potentially dangerous, and she understands why and how they\\'re dangerous. So I know that we can impart that to human elementary kids around this stuff. Because again, I think machine learning, you can do simple versions of it that really illustrate how it\\'s working. And that\\'s it. You can\\'t just say like, \"Okay,\" right? I think illustrating it, showing them that and letting them manipulate those inputs and see what happens to the output kind of stuff. I think that\\'s going to make some really powerful ways to demonstrate that and get it to really stick.']",
          null
         ],
         "marker": {
          "color": "#CFD8DC",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "other",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          8.30334758758545,
          10.672567367553711,
          6.880512237548828,
          8.820889472961426,
          2.0837743282318115,
          2.999366044998169,
          8.814050674438477,
          7.696227550506592,
          9.2811861038208,
          9.072664260864258,
          7.45142936706543,
          8.719647407531738,
          6.726388454437256,
          8.33940315246582,
          10.683069229125977,
          5.893824577331543,
          5.2440056800842285,
          6.431481838226318,
          6.322505474090576,
          11.093803405761719,
          5.784865379333496,
          8.769771575927734,
          9.029179573059082,
          7.418632984161377,
          7.0848259925842285,
          9.634891510009766,
          7.908900737762451,
          9.243196487426758,
          7.049928188323975,
          11.095014572143555,
          6.68290901184082,
          7.435838222503662,
          9.984062194824219,
          10.683548927307129,
          7.286442279815674,
          7.346311092376709,
          2.072577953338623,
          9.411843299865723,
          5.245518207550049,
          6.733590126037598,
          6.687638282775879,
          7.4236016273498535,
          3.3440825939178467,
          5.569645881652832,
          6.600658416748047,
          8.876775741577148,
          7.795421600341797,
          7.448235034942627,
          9.236739158630371,
          7.07369327545166,
          9.63189697265625,
          5.447390556335449,
          6.390172481536865,
          11.023752212524414,
          9.241345405578613,
          7.716599941253662,
          5.269401550292969,
          7.027801036834717,
          7.891671657562256,
          9.229870796203613,
          2.978712320327759,
          10.683761596679688,
          5.480374336242676,
          7.447204113006592,
          11.041799545288086,
          9.226120948791504,
          5.731091022491455,
          7.937491416931152,
          2.097865343093872,
          7.097933292388916,
          5.30872106552124,
          6.653461933135986,
          3.0286617279052734,
          9.6124267578125,
          3.022081136703491,
          4.244752883911133,
          1.9221999645233154,
          11.13224983215332,
          11.092264175415039,
          6.807523250579834,
          7.922499656677246,
          8.403836250305176,
          6.354024410247803,
          5.2245378494262695,
          3.0027785301208496,
          9.585103988647461,
          7.318578720092773
         ],
         "y": [
          3.8474671840667725,
          6.873153209686279,
          3.6073362827301025,
          3.2429444789886475,
          7.406252384185791,
          -7.171593189239502,
          3.261103868484497,
          5.350307941436768,
          7.176059722900391,
          5.16933536529541,
          1.5547150373458862,
          3.3424487113952637,
          2.441559076309204,
          3.845832109451294,
          6.864263534545898,
          4.9893951416015625,
          5.6767377853393555,
          2.804546356201172,
          5.783078670501709,
          4.8417463302612305,
          4.862163543701172,
          3.287019729614258,
          7.864201068878174,
          1.905847191810608,
          0.19449184834957123,
          7.221290588378906,
          4.0416178703308105,
          8.078080177307129,
          2.4286582469940186,
          4.84279203414917,
          3.175370216369629,
          1.7054213285446167,
          4.800753593444824,
          6.863765716552734,
          4.1527533531188965,
          1.9781306982040405,
          7.400355815887451,
          7.133143901824951,
          5.691493511199951,
          2.563145160675049,
          3.161475896835327,
          2.0049681663513184,
          5.946204662322998,
          2.7337141036987305,
          2.5004091262817383,
          3.1978933811187744,
          3.691977024078369,
          1.5495742559432983,
          8.057341575622559,
          0.18018393218517303,
          7.24891996383667,
          2.8028573989868164,
          2.807860851287842,
          4.792452335357666,
          8.067337989807129,
          5.351757049560547,
          2.9325923919677734,
          2.593200445175171,
          3.5099387168884277,
          8.055217742919922,
          -7.151195526123047,
          6.863317012786865,
          2.767807960510254,
          1.5193119049072266,
          4.812906265258789,
          8.060545921325684,
          5.068331241607666,
          3.5584330558776855,
          7.43864107131958,
          0.20582737028598785,
          5.703573226928711,
          5.622555732727051,
          -7.201111793518066,
          7.210273742675781,
          -7.194463729858398,
          5.692896842956543,
          7.404966354370117,
          4.886722564697266,
          4.843388557434082,
          5.584732532501221,
          3.502239942550659,
          3.7851176261901855,
          5.755256652832031,
          5.644479274749756,
          -7.1751933097839355,
          7.237351417541504,
          3.9154624938964844
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah. It's still, I think, very early for AI, but at small companies, if done properly, small companies can leverage AI to be bigger, and for a big company, you can definitely leverage AI to keep growing and to be more competitive. So, certainly at places like Morningstar, where there is so much data to be processed, we applied AI in a wide variety of fashions. We really focus on providing independent investment data and research. Mostly, that analysis is done by human experts, but there are times where there's so many possible investments to cover that you have to consider something like AI to look at all the information about a particular investment that may not be that well known or very popular, but for completeness, a company like Morningstar wants to provide some insights. Oftentimes, we will experiment with AI to consume all that available information and generate some information that a human analyst could also generate, but maybe doesn't have the time to. There's lots of applications for AI all over the place.\"]",
          "[\"And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there.\"]",
          "[\"Yeah. So there's code blue resuscitations in hospitals. I think you guys are probably familiar with that, when somebody goes into cardiac arrest. Whenever that happens, an alarm goes off in the hospital and everybody that is assigned to that code blue that day is supposed to, in theory, stop what they're doing and run to that room and help triage a patient in real time. And it's one of the best instances that I've ever studied of human-human teamwork. It's an absolute chaotic environment where you have extreme time pressures. People don't have, they have information disparities, inaccuracies, they don't know what's going on really so they're trying to understand at an individual level, what do I do? What do my team members do? Leadership is needed. One of the biggest problems that you see within those is what I was just saying, is information disparities and information needs not being able to access that very quickly. So in the communities I've been talking to regarding this, there's the development of utilizing robots and intelligent agents to aid in that collaborative decision making process, where the intelligent agent is providing, it's looking at all the data on the human, because as a human, we give a lot of biophysical data and can't possibly look at all of that in real time. It takes us a long time to go through all that. So an AI agent that has been trained on an algorithm to look at peaks and valleys of all of that, and then flag it can be trained to provide real time relevant information to that team when they need it. They don't have time to dig through the data, where the AI can dig through all that real time data and alert you of multiple things that are going wrong so you can better understand the medical operational environment. That's an example of where we're heading towards. | Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight.\"]",
          "[\"It's definitely different than undergrad. I got my undergrad at Clemson too, in genetics. So, I understand. I'm doing a similar thing as you. Would you tell me a little bit more about your AI teaming and cognition, your research, what you're studying? | Sure. All the research I've done up to this point has been looking at basically what does team cognition currently look in human-AI teams? I did a study on the role of spatial awareness or the availability of spatial information. And then I did another study that was essentially just looking at, it was pretty exploratory in the fact that I just studied or I just collected measures of shared mental models, along with trust and a bunch of qualitative data. So, for the objective measure, I say objective, but it's hard to actually measure a construct like that objectively. We didn't get anything on the objective measures, but the qualitative data that we got was really interesting. The perceived team cognition too is lower when you're working with AI, which I thought was pretty interesting. And then, there was lower trust as well. And it gets worse when you work with two AIS and you're the only human. When you become a minority member of the team, versus when it's two humans and one AI. So, that's where my research has been. And basically what I want to move into is how to develop, or figuring out what qualities an AI can have that is going to best support shared understanding between humans in a human-AI team. And then going that extra step and creating basically a shared mental model of the AI teammate and its operation. And that sounds a lot like AI explainability and transparency, but I think you take it a step further in making sure that both human teammates have a shared understanding of that. You know, I can go deep into the weeds in terms of the theory behind it, because you got task mental models and team mental models, and I'm kind proposing that there should be a AI teammate mental model that the humans can share, but basically developing AI that are going to support the more traditional aspects of shared understanding for humans. And then also developing them in a way that makes it easy for the humans to develop a mental model of the AI teammate, because you know, working with AI is very different. They are very smart and very stupid at the same time.\"]",
          "[\"Sure. Basically, I have not been officially in the working world yet. I did my undergrad in psychology in Clemson and graduated December of 2018. Then my current grad program, which is in human-centered computing, did not start until the fall. So, I was a substitute teacher at a middle school for that whole semester. And then over the summer, I just kind of take a break, went to Europe, all that good stuff. Then I started in the fall and I've been a research assistant ever since, working on human-AI teaming projects. Specifically, my research is on team cognition in human-AI teams.  I don't know, we can get more into the weeds with that, but the only other professional work experience I have is working at a light during internship my junior year of undergrad. And they did benefits management stuff. To be honest with you, I know this is a bad AI. Basically, their entire company could go away because they... I don't know, my job, it was just so much data management and it's crazy because they hire people with four-year degrees to do these jobs that you could basically do a Python script for. | That's interesting. How did you actually... I know you said you had a psychology degree from Clemson and then you kind of moved into the human-centered computing program. What made you interested in that? How did you become interested in working with AI and machine learning?\"]",
          "[\"So one job I had, when I was on a team, we would go out to different sites of critical infrastructure and basically assess their security posture and detect if they had any breaches or things like that and that all I've also done instant response missions, where a corporation may detect that they've had a breach of their security defenses and we come in and figure out where it came from, how to fix it and get rid of all the traces that are still on their systems or the networks. | Okay. Got it. Got it. So then how did you become interested in joining the research lab that you're in now and exploring more AI applications?\"]",
          "[\"It's definitely different than undergrad. I got my undergrad at Clemson too, in genetics. So, I understand. I'm doing a similar thing as you. Would you tell me a little bit more about your AI teaming and cognition, your research, what you're studying?\"]",
          "[\"Yeah. So there's code blue resuscitations in hospitals. I think you guys are probably familiar with that, when somebody goes into cardiac arrest. Whenever that happens, an alarm goes off in the hospital and everybody that is assigned to that code blue that day is supposed to, in theory, stop what they're doing and run to that room and help triage a patient in real time. And it's one of the best instances that I've ever studied of human-human teamwork. It's an absolute chaotic environment where you have extreme time pressures. People don't have, they have information disparities, inaccuracies, they don't know what's going on really so they're trying to understand at an individual level, what do I do? What do my team members do? Leadership is needed. One of the biggest problems that you see within those is what I was just saying, is information disparities and information needs not being able to access that very quickly. So in the communities I've been talking to regarding this, there's the development of utilizing robots and intelligent agents to aid in that collaborative decision making process, where the intelligent agent is providing, it's looking at all the data on the human, because as a human, we give a lot of biophysical data and can't possibly look at all of that in real time. It takes us a long time to go through all that. So an AI agent that has been trained on an algorithm to look at peaks and valleys of all of that, and then flag it can be trained to provide real time relevant information to that team when they need it. They don't have time to dig through the data, where the AI can dig through all that real time data and alert you of multiple things that are going wrong so you can better understand the medical operational environment. That's an example of where we're heading towards. | Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"Okay, great. So is human AI teaming a mix of AI agents and humans working together to complete a task? | Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools.\"]",
          "[\"So I'm currently a fourth year PhD student in human centered computing, where we basically just study how humans interact with different technologies. And in our lab, our research focus is mainly human AI teams, human AI teaming. So we basically study how humans interact with AI teammates in a given environment like gaming, where AI is pretty common to see. And before that I did my bachelor and master in engineering. So basically it's kind of like the algorithm behind the thing. That's basically my background. | Would you mind telling me a little bit more about AI teaming?\"]",
          "[\"So I'm currently a fourth year PhD student in human centered computing, where we basically just study how humans interact with different technologies. And in our lab, our research focus is mainly human AI teams, human AI teaming. So we basically study how humans interact with AI teammates in a given environment like gaming, where AI is pretty common to see. And before that I did my bachelor and master in engineering. So basically it's kind of like the algorithm behind the thing. That's basically my background. | So basically the human AI teaming concept is that human and AI teammates coordinate and collaborate to finish a set of goals basically as [inaudible 00:01:32] goes. And what we have done previously is given environment norm in games and give them a series of team tasks that they need to either share information with AI or share with them and also receive information that shared by the AI teammates and finish the task. Or we have also done research where the AI has their own responsibility, but this you need to coordinate their tasks like connect with each other. So that they collaborate and finish the team go. So that's basically how we define human AI teams.\"]",
          "[\"Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools. | I mean, right now it's a lot of just research and what their capabilities are and then trying to model what they might be able to do. So I'm designing a new experiment right now and we're going to fake the AI. We're not going to make the real system, but we have to be able to make it seem real to the person doing the experiments. There's a lot of research there about what the AI should be capable of and what the person would see and interact with the AI.\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate. | Yeah, that makes sense. That makes sense. Interesting. So I noticed you just mentioned maybe using AI to help humans. Do you know of ways that AI have helped humans and like what groups of humans have they been able to help?\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate.\"]",
          "[\"So one job I had, when I was on a team, we would go out to different sites of critical infrastructure and basically assess their security posture and detect if they had any breaches or things like that and that all I've also done instant response missions, where a corporation may detect that they've had a breach of their security defenses and we come in and figure out where it came from, how to fix it and get rid of all the traces that are still on their systems or the networks. | So something, we noticed that a lot of the tools, so to speak that we use in computer security are very automated and are going towards the trend of what you would consider AI autonomous agents and I started to become interested in Dr. Nathan McNeese work on human AI teaming because those systems are almost getting to the point in computer security where they would have a full team role. And just trying, I wanted to start looking into the factors that would make those teams more successful as those tools become more and more autonomous.\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate. | So there are several examples I think I can talk about the first one is Tesla, is a kind of AI driving. You can consider as a teamwork because their goal is to get to the destination safely or so, or you can consider it as AI, just easy to use as a tool to drive. So that could be an example of humans using AI to help them. And another one theory we use every day, Google home, that type of thing is also. We ask them okay, turn on the light or share what's the weather today. So that's another example of humans using AI to get the information and save their time. And I think also healthcare, I think I read papers before, but I don't really know examples in my life, but I think I have read that healthcare use machine learning, especially when diagnosing some images of humans. What is it called that kind of scanning pictures and can help them to diagnose whether it's benign or a bad cancer or so, and in addition to that that's most of the examples. And also I mentioned before data scientists that would use different models to help them predict things.\"]",
          "[\"And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there. | Yeah. That's great. Like simulating that environment a little bit closer to what we had in person potentially.\"]",
          "[\"So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate. | Yeah, definitely. Those are great examples of ways that AI have helped us. So kind of on the reverse, what are ways that maybe AI has harmed us and who in particular have they harmed?\"]",
          "[\"I know you provided an example with games because that's probably what you work on, but do you know of any other examples with AI teaming that's outside of the gaming world? | So a lot of times human... we would call it human AI collaboration when we don't really view AI as teammates. So we have a lot of actual human AI collaboration, examples, especially in data science that we use different models. Those actually are also machine learning algorithms, with those we basically consider is a tool. So it's not really human AI teaming but more like human AI collaboration. So currently not a lot. And that's actually why we choose game because you know, games, we have a lot of bots, but also I'm not sure whether you have heard of Open AI. It's basically a company, they created very intelligent AI teammates, but basically what they did was a team of AI and compete with humans instead of human and AI in a team. So I would say outside of games, not a lot, most of them AIs are just used as a tool to provide some prediction results or be used to help humans, but not really collaborate at the same level as a teammate.\"]",
          "[\"Sure. Basically, I have not been officially in the working world yet. I did my undergrad in psychology in Clemson and graduated December of 2018. Then my current grad program, which is in human-centered computing, did not start until the fall. So, I was a substitute teacher at a middle school for that whole semester. And then over the summer, I just kind of take a break, went to Europe, all that good stuff. Then I started in the fall and I've been a research assistant ever since, working on human-AI teaming projects. Specifically, my research is on team cognition in human-AI teams.  I don't know, we can get more into the weeds with that, but the only other professional work experience I have is working at a light during internship my junior year of undergrad. And they did benefits management stuff. To be honest with you, I know this is a bad AI. Basically, their entire company could go away because they... I don't know, my job, it was just so much data management and it's crazy because they hire people with four-year degrees to do these jobs that you could basically do a Python script for.\"]",
          "[\"So one job I had, when I was on a team, we would go out to different sites of critical infrastructure and basically assess their security posture and detect if they had any breaches or things like that and that all I've also done instant response missions, where a corporation may detect that they've had a breach of their security defenses and we come in and figure out where it came from, how to fix it and get rid of all the traces that are still on their systems or the networks. | Okay, great. So is human AI teaming a mix of AI agents and humans working together to complete a task?\"]",
          "[\"Yeah. And even just the tendency, like one thing I was discussing in a research project I'm working on right now is that a lot of people didn't trust an AI to work in a risky decision be because they thought it'd be too logical. Like it's going to make a decision based off what the exact probability of something happening is versus what a human in this scenario is probably going to start thinking of the worst case as possible and they may be tangentially probable, but it might be so bad that they're not going to make a decision. And that's part of a human emotional factor that's not going to be replicated by a machine. | And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there.\"]",
          "[\"Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools. | That's so interesting, like this hybrid task force and like what the responsibilities should be of the AI and what the responsibilities should be the human. I think that's fascinating. Okay. So more about you personally. So what sort of activities related to AI would you do in a typical week in your position now?\"]",
          "[\"So in work, we mainly test actually how humans react to AI with different attributes, kind of like human, maybe talk a lot or maybe do a lot or different features. And sometimes actually we would use a wizard [inaudible 00:08:08] just because AI teammate means they need to coordinate at a higher level instead of just like, okay, I get the data and analyze it. I give you a prediction result. So it's more than that. And that's why a lot of times we would use wizard of OZ to try to make sure it's kind of controlled, all the variables, the confounding variables are controlled, especially in experiments. And so in addition to that, in terms of [inaudible 00:08:41] we have use. Normally it's the machine learning algorithm that's used to kind of give them an instruction in terms of how they react in different situations. And in terms of AI in personal life, I actually don't really trust AI that much. Siri on my phone is always off. I don't really use it and I don't have a Google home. I just don't feel very comfortable that it listens to my voice all the time. Although I'm not saying anything that cannot be heard, it's kind of a privacy issue, I guess. For instance, if my family member really wants to have one, I would be fine with it. Although I personally would not choose to have one, if that makes sense?\"]",
          "[\"Sure. So the big differentiator for what would be a teammate versus a tool is that it's sort of an interdependent relationship. I'm not just using a system. I need stuff from it. It needs stuff for me. And there's a symbiotic relationship there. And I saw that as becoming more and more important, and I'm really interested in it because a lot of the tools that we're using on like computer security and a lot of other professional organizations are getting so sophisticated that they have capabilities that we're not able to do, that we really need to be able to leverage better. So from my perspective and a security standpoint, that's a lot of data analysis. AI can tackle data sets that we can't even fathom at a much faster rate. And we need to be able to give it a certain amount of autonomy to do that. But then there's also the human side of, I still need human reasoning to be involved in making a decision, especially when the risks or the consequences are high. So there's a lot of just interesting research areas for figuring out what the right balance is of that and how we work together with those tools.\"]",
          "[\"Yeah. It's still, I think, very early for AI, but at small companies, if done properly, small companies can leverage AI to be bigger, and for a big company, you can definitely leverage AI to keep growing and to be more competitive. So, certainly at places like Morningstar, where there is so much data to be processed, we applied AI in a wide variety of fashions. We really focus on providing independent investment data and research. Mostly, that analysis is done by human experts, but there are times where there's so many possible investments to cover that you have to consider something like AI to look at all the information about a particular investment that may not be that well known or very popular, but for completeness, a company like Morningstar wants to provide some insights. Oftentimes, we will experiment with AI to consume all that available information and generate some information that a human analyst could also generate, but maybe doesn't have the time to. There's lots of applications for AI all over the place. | Yeah, I'm part of the teaching faculty, so most of my focus is on teaching, but another responsibility I have is I serve as the executive director for Clemson's AI Research Institute for Science and Engineering, which is a new institute that came online in the summer of 2020, so right in the middle of COVID, and is only now sort of getting going. It was founded by Dr. Feng Luo, who is a professor in the school of computing. I'm helping him realize his vision for AI RISE, is what we call it, and it really is a combination of providing educational opportunities across Clemson to train faculty and researchers and students. It's to bring in the community, the upstate and the entire state, to help educate everyone really, on AI, and what AI is because it's such an overloaded, overused term and everybody maybe thinks they know what it is, but I think everybody has probably a different idea of what it is. AI always comes up in whatever subject I'm teaching. One of the classes I'm teaching this semester is on computing, ethics, and society. So obviously, talk a lot about AI and the moral issues associated with the use of AI and the bias that is proven to be in a lot of systems that employ AI today, and it has certainly lots of positive impacts, but also a lot of negative impacts. We talked a lot about that in that class. It always comes up because it's everywhere, honestly.\"]",
          "[\"And the best thing about being able to utilize AI, especially from where my background is, it's just the ability to look and process so much more and faster than humans are capable of. And I think that'll help from the computer security standpoint immensely because our networks and just our vast space with internet of things and build your own devices is expanding so big that we need those systems in order to analyze the network and protect us from breaches, things like that. And then in other areas it's really important in human AI because we have all these abilities now on our social media or games to make avatars or realistic environments and all these other sort of things that we can really interact with. So a lot of people in my lab work are more in a gaming kind of area and human AI collaboration and that and you can make gaming and learning environments so realistic with AI agents being part of the system. I actually think it could be really beneficial if learning continues on the trend, it's going to be more and more virtual where maybe be there is an AI representation of other classmates or your teacher or things that might kind of bridge the gap between being physically in a building versus isolated in my own house. There might be a middle ground there. | Yeah. I mean, so like my young son, if we're doing something like virtual learning, he's got to be on virtual learning for that day or that week. It's very difficult for him to pay attention to just a screen for longer than like 10 to 15 minutes. But maybe if it was like a very immersive environment where you had some AI students that were almost collaborating with you, it might be something he could engage in and get a lot more from that environment.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "0_ai_human_we",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "0_ai_human_we"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          3.0408823490142822,
          3.3713088035583496,
          2.6043508052825928,
          2.726775646209717,
          2.0842318534851074,
          3.2785682678222656,
          2.8083605766296387,
          2.6042025089263916,
          2.5458104610443115,
          2.397536516189575,
          2.376014232635498,
          2.7909369468688965,
          2.1853229999542236,
          2.1776747703552246,
          2.8001866340637207,
          2.137773036956787,
          3.3862831592559814,
          2.179032325744629,
          2.126124382019043,
          2.0918867588043213,
          2.3388757705688477,
          3.5002520084381104,
          2.8154773712158203,
          3.530050754547119,
          2.7884809970855713,
          3.061432123184204,
          3.412203550338745,
          2.7096309661865234
         ],
         "y": [
          5.060207843780518,
          4.791630268096924,
          5.460826873779297,
          5.971532821655273,
          6.494410514831543,
          5.812232971191406,
          6.070035934448242,
          5.463797092437744,
          5.559497356414795,
          6.177581310272217,
          6.185857772827148,
          5.294456958770752,
          6.21248197555542,
          6.201810836791992,
          5.318221092224121,
          6.205957412719727,
          4.802485942840576,
          6.177083969116211,
          6.1673994064331055,
          6.47733211517334,
          6.1972336769104,
          4.848305702209473,
          5.273360252380371,
          4.992624759674072,
          5.297685146331787,
          5.019885063171387,
          4.779252529144287,
          5.641229152679443
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          null,
          "[\"Yeah. I mean, so like my young son, if we're doing something like virtual learning, he's got to be on virtual learning for that day or that week. It's very difficult for him to pay attention to just a screen for longer than like 10 to 15 minutes. But maybe if it was like a very immersive environment where you had some AI students that were almost collaborating with you, it might be something he could engage in and get a lot more from that environment. | I mean, the aspect is so, AI is only as good as the data we give it, things like that. And I think there's a lot of discussion and research right now about a lot of AI being really biased to majorities because that's the data that they have access to. And so there might not be the ability to accurately assess or communicate with minority populations. And it could, I think there's some discussion about it kind of reinforcing biases and stereotypes because it's just operating off of the set of data that it's given.\"]",
          "[\"Well, I do a lot of presentations that cover these basics to get people on the same page. It's really the ability of something to mimic the capabilities of the human mind by learning from data and experiencing things. This general definition, the ability again, to have a system mimic the capabilities that we have, but what we have, in reality, is something quite different. In general, I think about that. And then within that, there're subsets, like machine learning and deep learning. I think when people think of AI, they think of robots killing all the humans.\"]",
          "[\"I mean, the aspect is so, AI is only as good as the data we give it, things like that. And I think there's a lot of discussion and research right now about a lot of AI being really biased to majorities because that's the data that they have access to. And so there might not be the ability to accurately assess or communicate with minority populations. And it could, I think there's some discussion about it kind of reinforcing biases and stereotypes because it's just operating off of the set of data that it's given. | We were talking about it a lot in recognition software and things like that, where the data sets that it's usually using to recognize and communicate with people is generally very Western white male and wrongly classifies people and communicates with them as if they were that group, which makes it very difficult for people to not only work but connect to it and get the benefits as so much to some other populations.\"]",
          "['Technology in general.\\n']",
          "[\"It'll be interesting to see what type of advances the AI will add to our medical system. | Well, I do a lot of presentations that cover these basics to get people on the same page. It's really the ability of something to mimic the capabilities of the human mind by learning from data and experiencing things. This general definition, the ability again, to have a system mimic the capabilities that we have, but what we have, in reality, is something quite different. In general, I think about that. And then within that, there're subsets, like machine learning and deep learning. I think when people think of AI, they think of robots killing all the humans.\"]",
          "[\"Yeah. Yeah. It's cool. And what's interesting is it's almost like a broad computer science department that just happens to be at Microsoft. So there's a lot of us doing AI stuff, but it's kind of the whole spectrum of computer science research. | That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way?\"]",
          "['Yeah. I mean, it\\'s essentially... So it has to appear intelligent, I think, right? So I don\\'t know if it has to actually be intelligent. And so that\\'s where I\\'d classify some of the things that I do that are... If you really dig down into it, it would probably be classified in algorithms, right? It\\'s an algorithm. So I did a book selection algorithm. But I think it\\'s important to think about the user\\'s perspective on these things. And so I have a pretty broad idea about AI in that if it appears intelligent, if it comes across as intelligent... I mean, maybe if it even wasn\\'t the intent that it comes across as intelligent, but it appears that way, I think we have to treat it as AI because of the impact it would have on the person that it\\'s interacting with, right? So my book selection algorithm is not complicated at all. It\\'s essentially like a sorting algorithm and we add in a couple of inputs about what the kids\\' book preferences are like, their reading skill level, the amount of time they read, that kind of stuff. And then we tag books for all those features, and we just make a priority queue out of it. Really simple, early CS stuff. But to the kids, it came across as intelligent. And I think that\\'s the key factor, that when we interviewed them afterwards, kids felt that the robot was paying attention to them and that the suggestions the robot made for books were personal, that they were about them. And so that to me is now where you\\'re in artificial intelligence and you really have to then take that seriously, because if the person believes that they\\'re working with an intelligent machine, then you have to treat that carefully. | Yeah. In that sense, that I think, I would fall in line with a little bit more traditional perspective, because machine learning doesn\\'t necessarily always have to... It\\'s not is a user-facing part of what\\'s happening. So it\\'s essentially how to take a bunch of inputs, teaching a machine how to interpret those inputs, and to organize, categorize, or plan actions based on those inputs. So it\\'s different levels of black boxiness that go along with it. But yeah, it\\'s essentially the training machines to have a space in between input and output that is nonlinear, I guess. So I mean, the traditional perspective is you have this set of data that\\'s coded with these sets. And so you train that way. And then a new set of data that isn\\'t coded, the machine should take what it learned from this first one, apply it there, and come up with the same codes. Those codes could then be actions to do. Those codes could be categories. Those codes could be things like emotions, right? So, \"Here\\'s 10,000 pictures of people who look angry. Here\\'s 10,000 more. Which of these are angry?\" kind of thing. So that, I think. And now that I\\'m thinking about it, that\\'s almost sneaky or more problematic sometimes because you don\\'t necessarily always have the user interacting with it while you\\'re developing these things and testing them. And it can be to such a scale sometimes that the errors and the problems in there are easy to miss, right? That, \"Hey, we got 99% accuracy,\" but that means if there\\'s 100,000 images in that set that you\\'re classifying on, 1% is actually a lot. And if that 1% impacts me and you\\'re just going to take this thing off the shelf that\\'s 99% accurate, and you\\'re going to take it off the shelf, and it\\'s going to make a medical diagnosis, and I get the 1% problem, that\\'s pretty impactful. So again, that\\'s one of the things I talk a lot with graduate students who are like, \"Oh, I\\'ll just grab the thing and we\\'ll just figure it out. It\\'ll tell us what to do.\" No, that\\'s not safe in a lot of the things that we\\'re doing. So, yeah. So I guess, I don\\'t know. I mean, that\\'s a too-long explanation of what machine learning is. ']",
          "[\"Yeah. That's great. Like simulating that environment a little bit closer to what we had in person potentially. | I mean, the aspect is so, AI is only as good as the data we give it, things like that. And I think there's a lot of discussion and research right now about a lot of AI being really biased to majorities because that's the data that they have access to. And so there might not be the ability to accurately assess or communicate with minority populations. And it could, I think there's some discussion about it kind of reinforcing biases and stereotypes because it's just operating off of the set of data that it's given.\"]",
          "['One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system.']",
          "[\"That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way? | That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning?\"]",
          "[\"No, I actually feel that because I recently joined Golnaz's lab and she does work on AI and machine learning and I had never done that before, so I was intimidated to go and start working on it, but I understand completely. So, how do you see advances in AI or machine learning as a way to help human populations?\"]",
          "[\"Yeah. And even just the tendency, like one thing I was discussing in a research project I'm working on right now is that a lot of people didn't trust an AI to work in a risky decision be because they thought it'd be too logical. Like it's going to make a decision based off what the exact probability of something happening is versus what a human in this scenario is probably going to start thinking of the worst case as possible and they may be tangentially probable, but it might be so bad that they're not going to make a decision. And that's part of a human emotional factor that's not going to be replicated by a machine.\"]",
          "['Yeah. With bias, the there\\'s a famous Facebook, one where they\\'re serving ads for higher paying jobs to more men than women. There\\'s other ones where they\\'re predictive policing. The algorithm AI will tell officers to target more communities of color and not the communities of... it\\'s the inverse of minority and majority. And then there\\'s another one. Oh. That they have AI algorithms that try and identify candidates for parole. And it\\'ll say that African American individuals are 40% more likely to repeat offend, even though the human will look at the data and be like, \"That\\'s not my experience. That\\'s not the case. That\\'s not true.\" Things like that.']",
          "[\"Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight. | Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"I mean, I would say it's broadly sort of the study of how to get computers and technologies to do things that humans would regard as intelligent. And I don't know. I say it like that, because I think it's sometimes a shifting boundary. Sometimes AI will accomplish something in AI and then people are like, wow, maybe that wasn't actually that intelligent after all. But I would say murkily, it's getting computers to do things that human humans would see as intelligent.\"]",
          "[\"One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system. | It'll be interesting to see what type of advances the AI will add to our medical system.\"]",
          "[\"Yeah, I don't. It's been a while. I don't know...do something with tick tock? | Are you talking about just learning AI and ML in general or the ethical issues?\"]",
          "['Unfortunately, it can kind of amplify some of the systemic issues that are already happening. | One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system.']",
          "['Well, I mean, so there\\'s definitely documentations of bias that are in there for essentially, right, the people who are designing and making these things cook in their own biases into there. And so if 98% of the data that we trained on are a bunch of white males, then anybody not in that category is going to be on the margins of the data set. And that means they have not been properly trained into that data. And so we\\'re going to be misclassified at a much higher rate. So I mean, I think there\\'s good efforts that are out there now to actually really think about representation and training sets. I don\\'t know how that gets reported. I haven\\'t seen anyone who has openly reported their training statistics that way, that, \"Here\\'s how we balance gender and all the other demographic things that would go into impacting these outcomes.\" So in a healthcare setting, I mean, it\\'s happened before machine learning, is doing these diagnostic things, the reports of people where there\\'s no black people represented in medical textbooks, right? That\\'s not about a machine learning thing. That\\'s about a human learning thing, where if you\\'ve never actually practiced or considered examining a person\\'s body who\\'s different than cliche US Western standards, then you\\'re going to make errors. And so I think we\\'re making the same mistake that we ignored before machine learning. We\\'re making the same mistake. And yeah, I think right now, the biggest thing for me would be transparency on these things. How are these systems trained? What do we know about those data sets? So yeah, they\\'re much more likely to impact marginalized communities because those are going to be the data that\\'s on the margins in the training sets. But I do think there\\'s repercussions for everyone as well. And that\\'s part of my worry too, that definitely some populations are going to be overrepresented in how they\\'re negatively impacted. But I think there\\'s negative impacts across the board. And so again, emotion recognition kind of things, I think those are problematic. Medical examination kind of things, I think are really problematic. So I\\'m skeptical across the board, but yeah. [crosstalk 00:28:37]-']",
          "[\"Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight.\"]",
          "[\"Yeah. And even just the tendency, like one thing I was discussing in a research project I'm working on right now is that a lot of people didn't trust an AI to work in a risky decision be because they thought it'd be too logical. Like it's going to make a decision based off what the exact probability of something happening is versus what a human in this scenario is probably going to start thinking of the worst case as possible and they may be tangentially probable, but it might be so bad that they're not going to make a decision. And that's part of a human emotional factor that's not going to be replicated by a machine. | Yeah. That's great. Like simulating that environment a little bit closer to what we had in person potentially.\"]",
          "[\"Yeah, that's fascinating. And I think that's a really good example of the power of AI and how it can be so helpful, especially, like you're taking the person's strengths, you're taking the AI strengths and bringing them together to make this really functional system even more functional. So what are some examples then kind of on the opposite end of the spectrum of how potentially AI or machine learning can harm us? And who in particular do you think it harms? | Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight.\"]",
          "['Well, I mean, so there\\'s definitely documentations of bias that are in there for essentially, right, the people who are designing and making these things cook in their own biases into there. And so if 98% of the data that we trained on are a bunch of white males, then anybody not in that category is going to be on the margins of the data set. And that means they have not been properly trained into that data. And so we\\'re going to be misclassified at a much higher rate. So I mean, I think there\\'s good efforts that are out there now to actually really think about representation and training sets. I don\\'t know how that gets reported. I haven\\'t seen anyone who has openly reported their training statistics that way, that, \"Here\\'s how we balance gender and all the other demographic things that would go into impacting these outcomes.\" So in a healthcare setting, I mean, it\\'s happened before machine learning, is doing these diagnostic things, the reports of people where there\\'s no black people represented in medical textbooks, right? That\\'s not about a machine learning thing. That\\'s about a human learning thing, where if you\\'ve never actually practiced or considered examining a person\\'s body who\\'s different than cliche US Western standards, then you\\'re going to make errors. And so I think we\\'re making the same mistake that we ignored before machine learning. We\\'re making the same mistake. And yeah, I think right now, the biggest thing for me would be transparency on these things. How are these systems trained? What do we know about those data sets? So yeah, they\\'re much more likely to impact marginalized communities because those are going to be the data that\\'s on the margins in the training sets. But I do think there\\'s repercussions for everyone as well. And that\\'s part of my worry too, that definitely some populations are going to be overrepresented in how they\\'re negatively impacted. But I think there\\'s negative impacts across the board. And so again, emotion recognition kind of things, I think those are problematic. Medical examination kind of things, I think are really problematic. So I\\'m skeptical across the board, but yeah. [crosstalk 00:28:37]- | Yeah. I love that, Joe. And I can see that in your work, the way you\\'ve described it. That\\'s great. Okay. So let\\'s switch. So talking a little bit about why we\\'re doing this interview, right? We\\'re taking what people are saying and trying to apply it for learning experiences for young people. So what are your thoughts just generally about elementary school, middle school-age kids learning about either AI machine learning and the social and ethical impacts or both? [crosstalk 00:29:56] ideas around that, what they should learn, what\\'s important for them to know? Can they [crosstalk 00:30:00] in those issues']",
          "[\"I would say my machine learning to me, my understanding is it's kind of the algorithm behind the scene. I consider artificial intelligence as kind of like with, how to say, a subject being there. It could be virtual, it could be a robot being there, but machine learning is more like the algorithm behind it. Kind of like its mind or core. That kind of feeling. It's kind of like if I use human as an example, it's just like artificial intelligence is a human and machine learning is kind of his brain to think, to help it to learn and make predictions.\"]",
          "[\"I think one of the topics that keeps coming up is this issue of facial recognition, and a lot of research in that area where the accuracy of facial recognition is certainly in doubt, especially for people of color. There's a lot of research and certainly a lot of improvements being made in those areas, but still, wrongfully identifying somebody because of a failure in a AI system for facial and image recognition is devastating for the people. When you're not one of those people who are flagged, you're like, well, what's the deal? That doesn't affect me, but if you're that person, it's life changing in a bad way. It could be. | One other thing, just a quick thing. On the positive side, the nice thing about AI, if done correctly, those systems can identify things that humans cannot. Certain patterns. I just think about mostly in the area of medicine. They identify things that even a doctor, a skilled doctor will not recognize, because the pattern is not something that you can consume as a human, but certainly identifiable by an AI system.\"]",
          "[\"Yeah. You look at AI used to make decisions for hiring specifically. This is an issue we've seen this and then there's a lot. The problem with AI is not necessarily how the algorithm is aligned or built, humans, I believe are not malicious in nature, so for the most part they're not trying to build malicious AI algorithms. I'm of the belief that the problem starts with the data. The data that is often pulled is unrepresented. It's not inclusive. It's not diverse. So your starting point in your foundation is fundamentally flawed and biased. So we need to be very much more aware of the type of data that we're feeding into these algorithms, because if you have a bias in your data sample, it's only going to be more concentrated when it goes for an algorithm, because an algorithm basically at its core is just looking for concentrations and patterns of data. So if you have the slightest concentration or bias in your data, it's going to be amplified throughout the algorithm, and then the impacts of the algorithm with humans are going to be felt. So you see this in HR and recruiting all the time. When you have resumes that are given for, let's take the example of a professor. Unfortunately in many cases, most professors are white males and that's who's applying to be professors. So if you were training a theoretical AI algorithm to assess what a good candidate is for a professor and you feed them all a bunch of white male CVs, well, they're going to say that, oh, it seems like based on our data that the white male is the best candidate, because that's what the majority of the CVs are. So it's an inherent ... This is what's tricky, and I hope you guys are able to capture this in the whatever research you publish. AI's very tricky, because it's mimicking societal problems. It's not just the AI, that's the problem. It's in many ways, representative of real problems in society. And this is where I get really frustrated when I study, because I have grants on ethics and bias in AI. And I get really frustrated, because at some level I don't know how much we can fix these problems because they're deeply ingrained at a cultural and society level, because AI, and algorithms and data samples are in many ways mimicking the real world in some cases. It just becomes more amplified through the algorithm itself. I'm probably getting ahead of, but we have to build safeguards in to make sure that we're checking on the data that we're looking at, the algorithms that we're constantly having human eyes on the outputs. You have to make sure that human factors are directly tied to AI outcomes. You can't just let the AI run loose. It needs oversight. | It depends on the context. So if it's a Google image search engine, many kids say, yeah, I'm not represented here, but that's how the world is. The consequence isn't huge. I search for computer science professor and I don't see myself as a African American woman. Okay, that hurts. But I don't think Google should mess with that. Then when it comes to hiring algorithms, yeah, they see that as problematic. So the context really matters and the effect of the consequence as they see it matters to them.\"]"
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "1_in_data_ai",
         "text": [
          "1_in_data_ai",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          ""
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          4.410860538482666,
          4.926004886627197,
          4.311444282531738,
          4.8967390060424805,
          4.184469223022461,
          4.227740287780762,
          4.323271751403809,
          4.717228889465332,
          4.853777885437012,
          4.121769905090332,
          4.30543327331543,
          5.010893821716309,
          3.836831569671631,
          4.849238872528076,
          4.228579521179199,
          4.343080043792725,
          4.17301607131958,
          4.869852542877197,
          3.9475443363189697,
          4.905721664428711,
          4.26570987701416,
          3.784574270248413,
          4.177121639251709,
          4.893322944641113,
          4.192028999328613,
          4.017087459564209,
          4.230731964111328
         ],
         "y": [
          4.630914211273193,
          3.529252290725708,
          5.56336784362793,
          3.382826566696167,
          5.330361843109131,
          5.492588043212891,
          5.470030784606934,
          5.164454936981201,
          3.4945878982543945,
          5.266247272491455,
          5.504842758178711,
          5.582763195037842,
          4.904927730560303,
          3.3680756092071533,
          3.4674272537231445,
          5.55233097076416,
          5.3055338859558105,
          5.532582759857178,
          5.188937187194824,
          3.2885921001434326,
          3.429243564605713,
          4.864129543304443,
          3.5263586044311523,
          3.3076465129852295,
          5.679227828979492,
          5.067936420440674,
          3.4663619995117188
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah, so work that I've done has been outlining how there are different camps I've worked in. Number one is outlining how humans and AI interact with each other, specifically my dissertation work is on how humans and AI systems can impact and influence each other in a task. So how a human could be susceptible to letting a robot or an AI system tell it what to do and take commands from it or vice versa. So looking at what comprises that, how humans should lead AI or how an AI should lead a human, things like that. So that trade off and then I've also done work looking at how ethics in AI systems can be implemented, created and how it interacts with humans. So, it's fairly big part is the ethical implications of AI systems and how those ethical implications ultimately impact the utility of AI. And then the last, there are other smaller things that I've had to do. But then the last main one I worked on is AI. There's a grant that I work on fairly often that is a grant that looks at using machine intelligence and recommender systems to provide recommendations to teachers for professional development. So it's a lot of providing recommendations for their professional development and I work on the side of that where I work on building this system and outline in that aspect.\"]",
          "[\"Yeah, there's a really thought provoking... Hang on, just let me see. It's a great thought provoking website. What is it called? Oh yeah. moralmachine.net. If you go to moralmachine.net and then click the judge link, what this is, is this research that's done by MIT and a handful of other universities, and it asks you some interesting questions because everybody's talking about self-driving cars, and how they're going to happen. They're already happening. It's going to happen, and the reason behind it is, humans are not good at driving. People always think they're good at driving, but they're not. So, if we get the self-driving stuff done correctly, even if it's not entirely self-driving, even if you're sitting in the driver's seat and have to take over, there are these interesting moral decisions to be made by the technology in the event of something unexpected. This website poses all these interesting questions about, if you were programming this self-driving car, what would you do in the situation? Your brakes fail. If you continue on your path, you're going to destroy three children in the pedestrian path. If you swerve, you're going to kill everybody in your car because you're going to crash. It's an interesting reveal of how we think and they, MIT and these other universities collect this data. I think for young people, gaming, just taking advantage of gaming technology to educate in a way that's maybe not right in your face, I think is a good way to learn. So much of learning is caught rather than taught. You just never know.\"]",
          "[\"That's awesome. I mean we need more people to understand coding in general. So I think it's great that you do that work. Has any of the work you've done with those type of groups ever bridged into AI before or machine learning? | Yes, it's a data ethics course. We talk about AI a fuck ton. So besides camps, teaching people how to code, I'm a PA right now with YJ Kim here at the Wisconsin Center for Ed Research and I know David Shaffer. And through him I've heard about goal like everyone else in this community. So I hear about Clemson all the time. But where was I going? So I have a PA ship here. I've TAed here, TA for code and power a lot with Dr. Royston and that's kind of your classic critical theory approach to AI and a lot of... but like meritocracy, Google image search results for black women showing images of gorillas, those kinds of cases. Yeah, it's kind of fucked up. So talk about how code and power get gets embedded. And it's very critical around institutions and getting students to think about that stuff and learn about implicit biases and things like that. We actually have them take the implicit bias test and then think about the limitations of that test and try and reason about what data says. But-\"]",
          "['I think ultimately AI systems themselves are tools. So, an ethical dilemma existing in AI is simply the reflection of that ethical dilemma existing in a person. What I think AI makes it easier is AI is a tool that makes a lot of things easier and I think that also means AI could be a tool that makes being unethical a lot easier. There\\'s a lot of things you can do with AI systems, with them being black boxes and completely hidden. And also then just being a machine system, if I think of a really modern example, Google is extremely adamant that they will never share the AI algorithm side that determines whether or not a video on YouTube should be monetized or demonetized and the reason they say they don\\'t want to do that is because of bad actors. They\\'re like, \"oh, if we tell you that, then they\\'re a bad actor come in and do something,\" but the issue with this, of them coming in and so saying like, \"oh, we won\\'t do this,\" is there\\'s no oversight now to know if that\\'s discriminatory or not, we can\\'t even tell. And then we say like, \"well shouldn\\'t you provide something?\" No, we can\\'t even provide the oversight because that\\'s too much information, and you be like, \"you just have to trust us.\" And I think the challenge right now is that with AI systems, people are way too willing to give them that trust. If a company comes out and says, \"oh, we can\\'t divulge the AI secrets,\" because they\\'ll be, \"oh they\\'re just trade secrets, you can\\'t,\" it\\'s like a normal trade secret, but it\\'s a trade secret that heavily impacts a lot of other people. And it doesn\\'t have that oversight right now. So, I think that\\'s where the complication of ethics comes in is that it makes it easier especially with the current state of it to do something unethical because there\\'s that lack of oversight coming from both a computational side and a social side where people are just like, \"oh, it\\'s an AI system. We can just trust it,\" when in reality, it allows the bad motivations of individuals to almost be hidden because they\\'re exercising those bad motivations through a system. | I think it\\'s important, but I think it\\'s important from two perspectives. The thing I think it\\'s more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it\\'s very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society\\'s going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there\\'s it\\'s good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it\\'s a two front benefit from it.']",
          "['I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI.  | It\\'s the same bucket. If it\\'s a random person on the street, one, why the fuck are they talking to you about it, machine learning. Usually when we have that discussion, the technical discussion of AI versus machine learning, machine learning might be seen as a sub-case of AI a broad category and machine learning being a set of particular techniques for how we optimize AI. So having the machine learn from data, again, a big metaphor about comparing the human brain to computers, which is a bad thing to do for your own wellbeing. We aren\\'t machines. Machines don\\'t think like us. We call it... this is from Ellen Ullman\\'s Close to the Machine, fantastic book. It\\'s a memoir. I have it with me. And she talks about how we call it the machine. We call it a memory, but it\\'s not right. So one, I don\\'t like the name machine learning when people use it too much to compare to how babies. I\\'m like no, fuck, that\\'s not it. But a way of optimizing around data, and so it\\'s just really cool statistics. And it\\'s really close to data science, where you\\'re like you have data, but what meaning can you get out of this for a social reason? What\\'s blind on this? I don\\'t really care about the difference between, but then AI a more general label. If I have a bunch of if/then statements. Is that AI? Well, yeah, it used to be. We were trying to solve like chess originally or checkers. We had some pretty simple sequence of steps because that\\'s how we thought the human brain worked or at least that area thought things worked. So I don\\'t know. I use them broadly because I don\\'t think the specifics matter. It\\'s when we get to the ethics side of things.']",
          "['Oh yeah. I think that\\'s more important, but it\\'s mostly because I think it\\'s more important because you as an individual interact with society through those platforms and through that data and through that AI, even if you don\\'t realize you are, so me watching a video on YouTube, if I watch a video that has ethical issues, if I watch a video that it just is a very mean spirited video that can harm someone else by watching it I\\'m promoting it. And then while it might be, \"well, I didn\\'t tell anyone about it.\" It\\'s still me actually watching it needs an algorithm system that then promotes it to especially people in my geographic region or people with somewhat profiles to me. So it\\'s this idea that, I think it\\'s important because it also important and ultimately I think it\\'s the fault of the company that is the case, but I don\\'t trust them to fix that. So I think it\\'s more important to educate the individual on how that\\'s going to work. And then I think that\\'s just, once again your data literacy, understanding your digital footprint and also understanding your worth, humans are worth more as data to people than they are as humans. So I think understanding just how much your data is worth contextualize, you should just be giving it out for free, it is something fairly important and also the other adage of nothing is free in life where it\\'s, \"yeah, all these things are free.\" TikTok is free, but you\\'re getting a lot of data and you\\'re getting advertised to a lot. | Yeah. So, I said what happens to your data is the most important thing to me. And also what do you produce as a person in terms of data? So what do you produce and then what happens to it? That\\'s the biggest thing for me, because I think that also goes hand in hand with that literacy angle. But I also think the most important thing is how easy it is. Because I think reducing a mythos around a technology, there\\'s a quote about \"tech big magic hammer,\" but reducing the mythos around it, I think is also an extremely important aspect of it because computing has been plagued with people who brag about how hard it is and try to make it seem extra hard. And it isn\\'t. Especially if you don\\'t go into it with respective of it being extra hard. If you go into it with an encouragement angle and with an interest in learning something, it\\'s a lot easier. So I think the second most important thing is you need to learn just how easy it is to get started. There are so many things out there just to get started and they\\'re really easy to get started. They\\'re really easy to get just a little bit interested in it and then that\\'s enough to know if you\\'re going to like it or not. So I think that\\'s the two things. It\\'s from the computing side, what happens to your data? And then from that side, what are the resources you can do to get started and how easy is it to get started?']",
          "[\"How can we engage youth in learning about AI and machine learning and ethics? Have you ever used any activities? Do you know of any resources out there of [inaudible 00:24:53] or any ideas of how we can? | Yeah, there's a really thought provoking... Hang on, just let me see. It's a great thought provoking website. What is it called? Oh yeah. moralmachine.net. If you go to moralmachine.net and then click the judge link, what this is, is this research that's done by MIT and a handful of other universities, and it asks you some interesting questions because everybody's talking about self-driving cars, and how they're going to happen. They're already happening. It's going to happen, and the reason behind it is, humans are not good at driving. People always think they're good at driving, but they're not. So, if we get the self-driving stuff done correctly, even if it's not entirely self-driving, even if you're sitting in the driver's seat and have to take over, there are these interesting moral decisions to be made by the technology in the event of something unexpected. This website poses all these interesting questions about, if you were programming this self-driving car, what would you do in the situation? Your brakes fail. If you continue on your path, you're going to destroy three children in the pedestrian path. If you swerve, you're going to kill everybody in your car because you're going to crash. It's an interesting reveal of how we think and they, MIT and these other universities collect this data. I think for young people, gaming, just taking advantage of gaming technology to educate in a way that's maybe not right in your face, I think is a good way to learn. So much of learning is caught rather than taught. You just never know.\"]",
          "[\"Yes, it's a data ethics course. We talk about AI a fuck ton. So besides camps, teaching people how to code, I'm a PA right now with YJ Kim here at the Wisconsin Center for Ed Research and I know David Shaffer. And through him I've heard about goal like everyone else in this community. So I hear about Clemson all the time. But where was I going? So I have a PA ship here. I've TAed here, TA for code and power a lot with Dr. Royston and that's kind of your classic critical theory approach to AI and a lot of... but like meritocracy, Google image search results for black women showing images of gorillas, those kinds of cases. Yeah, it's kind of fucked up. So talk about how code and power get gets embedded. And it's very critical around institutions and getting students to think about that stuff and learn about implicit biases and things like that. We actually have them take the implicit bias test and then think about the limitations of that test and try and reason about what data says. But-\"]",
          "[\"Yeah, there's a really thought provoking... Hang on, just let me see. It's a great thought provoking website. What is it called? Oh yeah. moralmachine.net. If you go to moralmachine.net and then click the judge link, what this is, is this research that's done by MIT and a handful of other universities, and it asks you some interesting questions because everybody's talking about self-driving cars, and how they're going to happen. They're already happening. It's going to happen, and the reason behind it is, humans are not good at driving. People always think they're good at driving, but they're not. So, if we get the self-driving stuff done correctly, even if it's not entirely self-driving, even if you're sitting in the driver's seat and have to take over, there are these interesting moral decisions to be made by the technology in the event of something unexpected. This website poses all these interesting questions about, if you were programming this self-driving car, what would you do in the situation? Your brakes fail. If you continue on your path, you're going to destroy three children in the pedestrian path. If you swerve, you're going to kill everybody in your car because you're going to crash. It's an interesting reveal of how we think and they, MIT and these other universities collect this data. I think for young people, gaming, just taking advantage of gaming technology to educate in a way that's maybe not right in your face, I think is a good way to learn. So much of learning is caught rather than taught. You just never know. | If you sit around in a room and you hear a bunch of people just talking, you learn a lot. They're not teaching you, you're just learning a lot. You're just kind of catching it. I think by employing things like gaming and some of these thought workshops, like what would you do in this case? It's interesting. That's why I really like teaching the class I'm teaching this semester, which is computing, ethics, and global society. Because some things that seem simple, are not. They're tough decisions. They may benefit some people. They're going to hurt some people. Is it the right thing to do? Even though you can do it, should you do it? It's-\"]",
          "[\"Yeah, I'm part of the teaching faculty, so most of my focus is on teaching, but another responsibility I have is I serve as the executive director for Clemson's AI Research Institute for Science and Engineering, which is a new institute that came online in the summer of 2020, so right in the middle of COVID, and is only now sort of getting going. It was founded by Dr. Feng Luo, who is a professor in the school of computing. I'm helping him realize his vision for AI RISE, is what we call it, and it really is a combination of providing educational opportunities across Clemson to train faculty and researchers and students. It's to bring in the community, the upstate and the entire state, to help educate everyone really, on AI, and what AI is because it's such an overloaded, overused term and everybody maybe thinks they know what it is, but I think everybody has probably a different idea of what it is. AI always comes up in whatever subject I'm teaching. One of the classes I'm teaching this semester is on computing, ethics, and society. So obviously, talk a lot about AI and the moral issues associated with the use of AI and the bias that is proven to be in a lot of systems that employ AI today, and it has certainly lots of positive impacts, but also a lot of negative impacts. We talked a lot about that in that class. It always comes up because it's everywhere, honestly. | Yeah. I think maybe some of the obvious ones that people can relate to are benefits associated with recommendations. We rely on, and we always have, even before AI, rely on recommendations to help us make buying decisions that are best for us. You see it mostly in streaming media, for example. It's like, you watch this movie or you watch this genre, you might be interested in this series, and it is helpful, definitely is helpful, and then when we buy stuff, same kind of thing. People are looking at this, they ended up buying this. You were looking at this, maybe you'd be interested in this. It's subtle. It's kind of built in and it helps us make these important or maybe not so important decisions where we spend our money and things like that, and that affects so many people, and our reliance on recommendation systems has been around forever, and I think AI has really tapped into it. It's very freaky, and the other thing is, a lot of companies use AI just to speed up what they can do and increase their capacity. Screening resumes, looking for the right candidates to hire. There's so many opportunities out there. So much of the workforce is in motion right now and companies that are hiring to keep up, they employ different AI solutions that they probably bought from somebody else, who decided, hey, it's probably good business for me to do this. Those are just a couple examples.\"]",
          "[\"Yeah, I'm part of the teaching faculty, so most of my focus is on teaching, but another responsibility I have is I serve as the executive director for Clemson's AI Research Institute for Science and Engineering, which is a new institute that came online in the summer of 2020, so right in the middle of COVID, and is only now sort of getting going. It was founded by Dr. Feng Luo, who is a professor in the school of computing. I'm helping him realize his vision for AI RISE, is what we call it, and it really is a combination of providing educational opportunities across Clemson to train faculty and researchers and students. It's to bring in the community, the upstate and the entire state, to help educate everyone really, on AI, and what AI is because it's such an overloaded, overused term and everybody maybe thinks they know what it is, but I think everybody has probably a different idea of what it is. AI always comes up in whatever subject I'm teaching. One of the classes I'm teaching this semester is on computing, ethics, and society. So obviously, talk a lot about AI and the moral issues associated with the use of AI and the bias that is proven to be in a lot of systems that employ AI today, and it has certainly lots of positive impacts, but also a lot of negative impacts. We talked a lot about that in that class. It always comes up because it's everywhere, honestly. | Thank you. We appreciate those examples. I think most people have been encountering the AI that makes decisions or suggest things for them. Amazon, Netflix, they all have that. Now the reverse of that question, how has AI or machine learning harmed us and who have they harmed?\"]",
          "['I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI.  | No that\\'s great, thank you. So this might go more into some of your ethics background, but how do some advances in AI or machine learning help humans?']",
          "['All good, there\\'s too many schools. Summer 2019, it was in-person. Summer 2020 it wasn\\'t held for the obvious reason. But because it wasn\\'t held in 2020, we\\'d started doing during the semester some three Saturdays in a row camps instead of three weeks continuous. So I didn\\'t make as much money that year, because I didn\\'t have the... it pays a thousand a week, which is good money when you\\'re trying to pay fucking rent over the summer and your [inaudible 00:03:34] doesn\\'t get paid over the summer. So I taught instead 2020, I did remote. We were trying out a few different ways of running it. Right. And we kind of did some experiments with it and it was a class on... shoot, what the fuck was it called? Analysis Skills for College Success: Research and Data Analysis, something like that. It was essentially a research methods course for high schoolers. So they know that the word research doesn\\'t mean Googling shit. It means thinking through things in a certain way, but it had to work no matter what major the student might go into. So I got to teach this really, really mixed methods, fun little three week camp thing to students drawing on the fact that I have a crazy, weird, mixed background and I continue doing weird mixed stuff. But all the offerings of that, every time I\\'ve done it, students the first time had fun. They were really attentive and stuff. Last time I had one student who actually did anything. And so students have just been increasingly burnt out from that. But summer \\'20, this most recent summer, we did it online. So I came into my office on Zoom and just was on Zoom for nine to three, but breaks, trying to mirror as much as we could of the in-person experience from the two years prior. It was better because students were a bit more captive. They are honors kids with rich parents, honestly. And they were excited. They wanted to go to this camp. They know it was canceled the previous year. They\\'re big nerds. I never went to those camps, but I\\'m glad to teach the kids who go to those camp. So we\\'ve adapted with the pandemic with that. But it\\'s all just various kinds of teaching. That\\'s my one-line bio that I teach kids and adults how to code which is the [crosstalk 00:05:38].\\n | So I take this kind of data science approach to Python, where I have the students collect data on whatever they want. I\\'ve had a student, one of my favorites honestly, was someone analyzed Reddit posts about bourbon. And it was such a great project because he got to talk to his dad about it because it was kids in Kentucky. And there this huge value around family and stuff. So I\\'m like, \"Yeah, pick something that you can talk to your friends and family about. I don\\'t want you to come into this completely alone. It\\'s good to have someone who knows nothing about coding, but knows about bourbon to fucking talk to.\" So this guy gets to nerd out with his dad for this whole project, just talking about bourbon and why people were saying certain things in these posts and what they meant. So he was just doing fucking the same thing that you guys are going to be doing with this transcript, analyzing the codes and themes and explaining why they matter. I don\\'t know a fucking thing about bourbon. I used to live in Bardstown, the bourbon capital of the world, but I don\\'t drink. So he did that, but he used Python after he kind of came up with his codes, used Python to go through and scrape it to label the data and then count it, and then say why it matter. And it\\'s these 50 lines of code that he\\'s put a lot of thought into it and can reason about the data that comes out of it. So that\\'s the closest that I\\'ve gotten to doing AI stuff with coding because I tend to intro-level stuff. Here at UW, I have taught a data ethic course. So that\\'s not about coding [crosstalk 00:08:09]-']",
          "[\"I mean, I'm a cynic in this regard. I think everything in this country and this world is motivated by money and financial interest, and corporate entities maintain that at the highest level. And depending on the governments, they either promote that or try to curb that back. But at the end of the day, it's all about money. So I think when you talk about AI and dealing with the problems that are dealt with, with AI, you have to understand that people are making money off of AI. Even if it's terrible AI in terms of being this most biased, terrible agent in the world, someone's going to make money off of it potentially. And I'm a cynic in that regard. People will put money above much of their own ethics in some cases. | No. I think what you're doing is important. I think that AI, machine learning, reinforcement learning, neural networks aren't going anywhere. They're going to become a bigger entity and play a bigger role in society as time goes by. So the people that are really going to matter in terms of making sure that this works for other people, our youth, their kids, they're the ones that are going to actually have the big impact on being able to change this and understand it at a better level then people that are our age it. It starts with the education of the things that we're talking about, understanding how this can go wrong, understanding the errors so you're educated on that and you can avoid that. In many ways, I think what people like me and a lot of people that are similar to me, we just try to put bandaids on this because we're too far along on the road. And youth have the ability to not just put bandaids on things, they can actually fix things, I think.\"]",
          "['Okay, here\\'s how I\\'m going to think about it. In the Pasco case, I\\'ll use that example for both because it\\'s good to ground myself in a fucking case so I don\\'t go crazy. Looking at that case and thinking about how I speak to my students about AI ethics, which I speak to them the way I do because of my interviews of the 10 people I had. And they all had different views on things. It\\'s like, well, how can I do this in a way that\\'s kind of in agreement with all these different views, knowing that again, the course is going to be kind of pigeonholed the way that Allan did it. How can I introduce to things? And one of the things that people talk about a lot in the ethics, when you\\'re giving people who aren\\'t ethicists, they\\'re just a fucking programmer. You want them to think through things, is the stake stakeholders model. We just ask what\\'s at stake? But remember week one, I\\'m doing a pre-assignment where they don\\'t have any understanding what I\\'m talking about. So I can\\'t use the word stakes because they might take that in a too narrow sense. And so I say to them is, \"In this case, what are the things that we want to get right? Yes, there\\'s this really shitty thing that happened in Florida. The article ends on a very sad note, but there are things that we\\'re trying to get as humans in this. All the bad things that happened, we wanted those not to go that way, but we don\\'t want to just burn out everything to happen because again, we\\'re trying to get at things. So I have the students focus on this shared human endeavor of things we try to get right. And so in that case, the police department did a shitty thing, but they\\'re trying to get right reducing crime. They\\'re trying to right serving their community\\'s interest. They\\'re trying to get right distributing their resources well. Whether they actually got those things right in a way that still abides by fair terms is also preparation is a whole nother story, and we\\'ll let the Department of Justice... we\\'ll see what their investigation finds. But it has a lot of promise. And I try not to focus on promise because coming into viewing AI from the point of view of promise and predictions and it\\'s... you mention advancements. My first instinct there is no, let\\'s not talk about advancements first. There\\'s a huge history of technology here. There\\'s a huge history of institutions. Policing hasn\\'t always been the best and still isn\\'t always the best. And so if something\\'s being embedded there, we better really look at that history. We can\\'t look to advancement and promises of AI as a way to ignore how we got here. So I tried to avoid that framing. But we are trying to use it to get at good things. So I can\\'t say what is actually done well, but I can say we\\'re generally trying to get at good values with it. So let\\'s get that right, kind of be my call to action. And that involves also looking at the history of how we get to places. Anna Hoffman, AI ethics researcher from Northwest, I can\\'t remember where she\\'s from, but she\\'s awesome. You should look up her work. I have the idea of let\\'s not talk about entry points from promises, or futures, or potential of AI because of a talk she had. She\\'s like I\\'m in this pandemic and I\\'m tired and I\\'m sad. I\\'m a tired, stressed professor and I\\'m tired of looking at AI. So I\\'m going to come at this in a fun way, which is what does AI look like if we start from the position of infinite love for trans people? Let\\'s just take a fundamentally different approach to how we might think about AI. And it was a really fucking fun talk. But one of the things is looking at the medias around promises of AI and what gets left out when we focus too much on promises. So I\\'m doubly rambling at this point, but the things we\\'re trying to get right with AI, it\\'s good to enumerate those. And we kind of reground ourselves in the value of what we\\'re actually trying to do. And a lot of harms come out when we lose track of that and focus too much on it can just solve a lot of these things. When it\\'s put into context that it ought not be in, harms can come up. When it\\'s put into context too quickly, harms can come up. So I\\'m going to stop rambling and drink water.\\n | I\\'m going to bring up... I had this already. Nope, that\\'s my writing sample. I have just been writing all this down for applications to places. And I\\'m at the point where if I write something down, I\\'m going to forget it immediately, which is the opposite problem because now I\\'m not constantly thinking about it. So AI or AI ethics is a bundle of ethical issues. And so thinking about populations depends on your entry point to thinking what AI ethics is. And so I\\'m just reading my notes, honestly. We can\\'t violate public trust, is kind of one thing. That comes up a lot in the discussions around autonomous vehicles, that we want to have autonomous vehicles for X, Y, Z reason. But in order for that to happen, the public kind of has to trust in the system. We have to agree. We have to know that it\\'s not going to run over black people more than white people. If it gets down to that moment, we have to know that it isn\\'t going to confuse us, not know that a cyclist is there or the actual case of where Uber hit a woman. I don\\'t know if it was Uber or not, but there was an autonomous vehicle that hit a woman because she jaywalked. Well, jaywalking should result in a fine, if that, not in death from a car that didn\\'t see you. The idea that the road is owned by cars is a relatively new one in human history. Roads were owned by the people walking on them. And so that one, public needs trust in it and that we don\\'t want to violate that trust. So in a way we can harm everyone when we release things too early in that sense or have things that create harms. Also, the people who are literally hit by the car, I think get harmed the most. Let\\'s see, there\\'s a lot of talk around misinformation being amplified on social media. And there\\'s also talk on certain things when you have newsfeeds, like Facebook, that put the things at the top that they think you want to see or that they want you to see. That\\'s in their benefit for you to see. Or TikTok, as fun as it is to find your own extremely niche set of friends on TikTok, and ridiculously fast, what\\'s getting left out? Who\\'s getting pushed down? And so there\\'s a lot of harms that come up when misinformation gets brought up to the top and there\\'s a lot of harms that get caused when certain communities are just completely pushed down and systematically given lower scores in the algorithm. And so there\\'s a lot of talk on TikTok around trans communities and people of color, people who don\\'t look pretty getting rated lower by the algorithm, so they\\'re not going to have as much of a viewership. And so when your money is tied to being a content creator, it sucks. There\\'s also the whole thing that if you\\'re queer and online, you are subject to harassment. You always have to have a few backup accounts because one of them is going to get blocked out because people are going to mass... people who don\\'t like you are going to mass report everything you do until you get flagged by the algorithm as bad and systematically have your account removed, even though you didn\\'t do anything wrong. That happens all the fucking times with trans content creators. There\\'s like all these, \"Hey, my thing got deleted again.\" And it\\'s just happens. So there\\'s a lot of algorithms that play a part in that. But also the algorithm isn\\'t separate from the system of humans interacting with it. So fuck, I talked a long list, when it\\'s applied in places like policing, compulsory education, medicine, et cetera. Those already have a lot of scrutiny on them, legal and public scrutiny on them. So it\\'s very important that we get AI right in those cases because we\\'re constantly looking at them for whatever reason. Reason might be that we have a lot of public scrutiny on policing and education because we want a fair and just society. And we see those institutions that are very important to the function of a fair and just society. So if we fuck up there at all, everyone pays attention. Work, AI changes the nature of work, and when the nature of work changes, some people are benefited and some people are completely displaced.']",
          "['Yeah, I\\'m actually familiar with that one, because it\\'s the CU-TLP program. We have some people in the learning sciences that are working on that. So I\\'m familiar with this one. Would you mind giving me a little bit more detail about your project that involves AI ethics? What were some of the results that came out of that? | Yeah. So on the day-to-day, once again, if I talk about it from a project to project standpoint, there are some easy ways to look at it, which are the TLP project, that\\'s coding and expert system, which means I\\'m looking at how experts look at things and I\\'m creating rules for a system to make judgements based on those rules. So, on a day-to-day, what I do is, I check those rules. I look at them, but most of what I do is just make sure it\\'s still working. Make sure nothing\\'s gone wrong, make sure I look at the results. I\\'m like, \"okay, these results came out, they don\\'t look bad.\" A lot of it, a lot of time goes into those, a lot, at a concentrated period and then a lot of it\\'s just watching it happen. So that\\'s the one. And then for my dissertation work, I actually use machine intelligent systems that were designed by other groups. So I got them from a group that the platform I chose to use, they had created these bots for that platform. And so what I actually do is just, I found these bots made by that platform that are open source and I slightly tweaked them. So similar to before it\\'s less of a like I need to change and develop so many things every day and more like, \"okay, I just need to keep this up and running, make sure everything\\'s fine. Make sure nothing\\'s changing.\" So just a lot of oversight.']",
          "[\"How can we engage youth in learning about AI and machine learning and ethics? Have you ever used any activities? Do you know of any resources out there of [inaudible 00:24:53] or any ideas of how we can? | If you sit around in a room and you hear a bunch of people just talking, you learn a lot. They're not teaching you, you're just learning a lot. You're just kind of catching it. I think by employing things like gaming and some of these thought workshops, like what would you do in this case? It's interesting. That's why I really like teaching the class I'm teaching this semester, which is computing, ethics, and global society. Because some things that seem simple, are not. They're tough decisions. They may benefit some people. They're going to hurt some people. Is it the right thing to do? Even though you can do it, should you do it? It's-\"]",
          "['I met a guy in person, he used to help make movies. He\\'s like, \"yeah, my job used to take a team. And now it\\'s just one dude at one program.\" And so he\\'s changing careers into learning how to code, and so there\\'s that. Consumers might not be aware that they\\'re interacting with AI when they\\'re shopping online, except it might not be aware to the extent that they are and that can have harms to principles like consent, being aware enough of what\\'s going on to be able to fully make a decision and not doing that you\\'re interacting with AI or that visual. Invisible changes are happening on the screen behind the scenes. It might be bad depending on that context. And it\\'s fucking hard to regulate AI.']",
          "['Okay, here\\'s how I\\'m going to think about it. In the Pasco case, I\\'ll use that example for both because it\\'s good to ground myself in a fucking case so I don\\'t go crazy. Looking at that case and thinking about how I speak to my students about AI ethics, which I speak to them the way I do because of my interviews of the 10 people I had. And they all had different views on things. It\\'s like, well, how can I do this in a way that\\'s kind of in agreement with all these different views, knowing that again, the course is going to be kind of pigeonholed the way that Allan did it. How can I introduce to things? And one of the things that people talk about a lot in the ethics, when you\\'re giving people who aren\\'t ethicists, they\\'re just a fucking programmer. You want them to think through things, is the stake stakeholders model. We just ask what\\'s at stake? But remember week one, I\\'m doing a pre-assignment where they don\\'t have any understanding what I\\'m talking about. So I can\\'t use the word stakes because they might take that in a too narrow sense. And so I say to them is, \"In this case, what are the things that we want to get right? Yes, there\\'s this really shitty thing that happened in Florida. The article ends on a very sad note, but there are things that we\\'re trying to get as humans in this. All the bad things that happened, we wanted those not to go that way, but we don\\'t want to just burn out everything to happen because again, we\\'re trying to get at things. So I have the students focus on this shared human endeavor of things we try to get right. And so in that case, the police department did a shitty thing, but they\\'re trying to get right reducing crime. They\\'re trying to right serving their community\\'s interest. They\\'re trying to get right distributing their resources well. Whether they actually got those things right in a way that still abides by fair terms is also preparation is a whole nother story, and we\\'ll let the Department of Justice... we\\'ll see what their investigation finds. But it has a lot of promise. And I try not to focus on promise because coming into viewing AI from the point of view of promise and predictions and it\\'s... you mention advancements. My first instinct there is no, let\\'s not talk about advancements first. There\\'s a huge history of technology here. There\\'s a huge history of institutions. Policing hasn\\'t always been the best and still isn\\'t always the best. And so if something\\'s being embedded there, we better really look at that history. We can\\'t look to advancement and promises of AI as a way to ignore how we got here. So I tried to avoid that framing. But we are trying to use it to get at good things. So I can\\'t say what is actually done well, but I can say we\\'re generally trying to get at good values with it. So let\\'s get that right, kind of be my call to action. And that involves also looking at the history of how we get to places. Anna Hoffman, AI ethics researcher from Northwest, I can\\'t remember where she\\'s from, but she\\'s awesome. You should look up her work. I have the idea of let\\'s not talk about entry points from promises, or futures, or potential of AI because of a talk she had. She\\'s like I\\'m in this pandemic and I\\'m tired and I\\'m sad. I\\'m a tired, stressed professor and I\\'m tired of looking at AI. So I\\'m going to come at this in a fun way, which is what does AI look like if we start from the position of infinite love for trans people? Let\\'s just take a fundamentally different approach to how we might think about AI. And it was a really fucking fun talk. But one of the things is looking at the medias around promises of AI and what gets left out when we focus too much on promises. So I\\'m doubly rambling at this point, but the things we\\'re trying to get right with AI, it\\'s good to enumerate those. And we kind of reground ourselves in the value of what we\\'re actually trying to do. And a lot of harms come out when we lose track of that and focus too much on it can just solve a lot of these things. When it\\'s put into context that it ought not be in, harms can come up. When it\\'s put into context too quickly, harms can come up. So I\\'m going to stop rambling and drink water.\\n']",
          "['Oh yeah. I think that\\'s more important, but it\\'s mostly because I think it\\'s more important because you as an individual interact with society through those platforms and through that data and through that AI, even if you don\\'t realize you are, so me watching a video on YouTube, if I watch a video that has ethical issues, if I watch a video that it just is a very mean spirited video that can harm someone else by watching it I\\'m promoting it. And then while it might be, \"well, I didn\\'t tell anyone about it.\" It\\'s still me actually watching it needs an algorithm system that then promotes it to especially people in my geographic region or people with somewhat profiles to me. So it\\'s this idea that, I think it\\'s important because it also important and ultimately I think it\\'s the fault of the company that is the case, but I don\\'t trust them to fix that. So I think it\\'s more important to educate the individual on how that\\'s going to work. And then I think that\\'s just, once again your data literacy, understanding your digital footprint and also understanding your worth, humans are worth more as data to people than they are as humans. So I think understanding just how much your data is worth contextualize, you should just be giving it out for free, it is something fairly important and also the other adage of nothing is free in life where it\\'s, \"yeah, all these things are free.\" TikTok is free, but you\\'re getting a lot of data and you\\'re getting advertised to a lot.']",
          "['So the data ethics course is a combi credit. There\\'s a combi version and a non-combi version. When I taught it over summer, it was eight weeks online, a hundred-ish students, some combi, some non-combi. I had two TAs and also I was the TA for it twice after. So we designed it. I was a TA, I was a TA, and then I taught it and I\\'m praying to teach it again this summer. But it\\'s a lot of writing because it\\'s a big combi component. And Alan Ruble was the instructor who made it, who\\'s my advisor. And when teachers design courses, they use the pedagogy they were taught with is the most number one thing I learned from talking with all these AI ethics teachers and all these different backgrounds is they might have similar motivation, similar goals, similar big picture aims where all we give a shit about AI ethics. But then what they actually do in the classroom is just what they were taught with. And I\\'m just like, \"Okay, that\\'s not very critical of you, but good.\" But it\\'s good to see all these things and these different views besides my own too because again, I teach the way that I\\'ve been teaching. I teach drawing on my English background, not my computer science background, even though I teach computer science style courses because in English, they have you take pedagogy classes. So I\\'m drawing on that. I have weekly journals that I make my students do because I found that useful in a class I took. It\\'s like, \"I like that so I\\'m going to make my students do it.\" But I\\'ve adapted to make sense for coding. So in that class, it\\'s a lot of writing and reading because that\\'s what the instructor who designed did a lot because he\\'s a philosopher. And in philosophy you do a lot of reading and writing. I added in because I like them the studio discussions. The students read an article or watch... do some kind of prep for the discussion. They meet with their group on Microsoft Teams or similar tool. They record it. They post the recording to the discussion board and then they watch X number of recordings and respond to them. I did a pre-post set up for that for the course where they read the first and second house at the time of the Pasco PD case that is happening in Florida. Pasco police department made an algorithm to predict who would commit crime or would be likely offenders and then went out and targeted people based on it. It\\'s currently going under investigation by the Department of Justice, which means you\\'re going to have a lot of fact finding about this case. So that\\'s a good case to know when you\\'re teaching data ethics.']",
          "['I met a guy in person, he used to help make movies. He\\'s like, \"yeah, my job used to take a team. And now it\\'s just one dude at one program.\" And so he\\'s changing careers into learning how to code, and so there\\'s that. Consumers might not be aware that they\\'re interacting with AI when they\\'re shopping online, except it might not be aware to the extent that they are and that can have harms to principles like consent, being aware enough of what\\'s going on to be able to fully make a decision and not doing that you\\'re interacting with AI or that visual. Invisible changes are happening on the screen behind the scenes. It might be bad depending on that context. And it\\'s fucking hard to regulate AI. | Yes, exactly. There\\'s a lot of other things that go into it. So we\\'re going to pivot a little bit more. I know you don\\'t necessarily always work with youth, but what are your thoughts about youth learning about machine learning or learning about algorithms and AI?\\n']",
          "['Yeah. That makes sense. I am still learning how to code. So I\\'ll get there one day. | Oh yeah. I think that\\'s more important, but it\\'s mostly because I think it\\'s more important because you as an individual interact with society through those platforms and through that data and through that AI, even if you don\\'t realize you are, so me watching a video on YouTube, if I watch a video that has ethical issues, if I watch a video that it just is a very mean spirited video that can harm someone else by watching it I\\'m promoting it. And then while it might be, \"well, I didn\\'t tell anyone about it.\" It\\'s still me actually watching it needs an algorithm system that then promotes it to especially people in my geographic region or people with somewhat profiles to me. So it\\'s this idea that, I think it\\'s important because it also important and ultimately I think it\\'s the fault of the company that is the case, but I don\\'t trust them to fix that. So I think it\\'s more important to educate the individual on how that\\'s going to work. And then I think that\\'s just, once again your data literacy, understanding your digital footprint and also understanding your worth, humans are worth more as data to people than they are as humans. So I think understanding just how much your data is worth contextualize, you should just be giving it out for free, it is something fairly important and also the other adage of nothing is free in life where it\\'s, \"yeah, all these things are free.\" TikTok is free, but you\\'re getting a lot of data and you\\'re getting advertised to a lot.']",
          "['I use AI in a broad sense because people tend to use it in a broad sense. They don\\'t know what the fuck it actually means. It also doesn\\'t need to have... The reason why I define it the way I do, I\\'ll start here and I\\'ll go back to how I actually define it. There can be an AI moral case that has no AI actually in it, as long as the people in the case think that the system exists and that it has AI. And that\\'s all that matters is they attribute morally relevant qualities to the system. And they attribute an AI label to the system, whether it exists or not. So they\\'d be like, \"Yeah, we\\'re going to do this because it\\'s faster.\" Well, that\\'s a morally relevant quality they\\'re attributing to it, that they value speed and stuff. So they attribute that to it and that\\'s what they reason about it. My advisor has this awesome paper called agency laundry, which is about the moment when... there\\'s a train going by in the background. The moment when people go, \"No, it\\'s not my fault. I didn\\'t make the decision. The program made the decision.\" He\\'s like well no, you\\'re laundering your agency into the system. And that\\'s bad for X, X, X reason. And he walks through a few cases and those cases are all about algorithms it turns out. But the moral issue at heart is not about algorithms. It\\'s the same moral issues we\\'ve had for a fucking long time. Nothing is new under the sun. One of the people I interviewed talked about how in her class, she has to talk to the students about how we have these questions about AI. It\\'s new. We\\'re not trying to regulate it. We\\'re not trying to deal with it. We had the same fucking questions when the internet came up. We had the same questions when VHS tapes came up. And now those things kind of seem settled in retrospect. AI is just the thing that\\'s new in question raising right now. It\\'s really this longer huge tradition of technology raising questions is actually what I think is important. And so I use it in a broad sense. I just might just say it\\'s an intelligent system and leave it at that. Whether it actually exists or not, whether it\\'s actually humans on the other end or not, to me, it doesn\\'t matter. It matters how people think about the system and whether they call it AI, fuck, sure, that\\'s AI. ']",
          "['Technology in general.\\n | I met a guy in person, he used to help make movies. He\\'s like, \"yeah, my job used to take a team. And now it\\'s just one dude at one program.\" And so he\\'s changing careers into learning how to code, and so there\\'s that. Consumers might not be aware that they\\'re interacting with AI when they\\'re shopping online, except it might not be aware to the extent that they are and that can have harms to principles like consent, being aware enough of what\\'s going on to be able to fully make a decision and not doing that you\\'re interacting with AI or that visual. Invisible changes are happening on the screen behind the scenes. It might be bad depending on that context. And it\\'s fucking hard to regulate AI.']",
          "['I\\'m going to bring up... I had this already. Nope, that\\'s my writing sample. I have just been writing all this down for applications to places. And I\\'m at the point where if I write something down, I\\'m going to forget it immediately, which is the opposite problem because now I\\'m not constantly thinking about it. So AI or AI ethics is a bundle of ethical issues. And so thinking about populations depends on your entry point to thinking what AI ethics is. And so I\\'m just reading my notes, honestly. We can\\'t violate public trust, is kind of one thing. That comes up a lot in the discussions around autonomous vehicles, that we want to have autonomous vehicles for X, Y, Z reason. But in order for that to happen, the public kind of has to trust in the system. We have to agree. We have to know that it\\'s not going to run over black people more than white people. If it gets down to that moment, we have to know that it isn\\'t going to confuse us, not know that a cyclist is there or the actual case of where Uber hit a woman. I don\\'t know if it was Uber or not, but there was an autonomous vehicle that hit a woman because she jaywalked. Well, jaywalking should result in a fine, if that, not in death from a car that didn\\'t see you. The idea that the road is owned by cars is a relatively new one in human history. Roads were owned by the people walking on them. And so that one, public needs trust in it and that we don\\'t want to violate that trust. So in a way we can harm everyone when we release things too early in that sense or have things that create harms. Also, the people who are literally hit by the car, I think get harmed the most. Let\\'s see, there\\'s a lot of talk around misinformation being amplified on social media. And there\\'s also talk on certain things when you have newsfeeds, like Facebook, that put the things at the top that they think you want to see or that they want you to see. That\\'s in their benefit for you to see. Or TikTok, as fun as it is to find your own extremely niche set of friends on TikTok, and ridiculously fast, what\\'s getting left out? Who\\'s getting pushed down? And so there\\'s a lot of harms that come up when misinformation gets brought up to the top and there\\'s a lot of harms that get caused when certain communities are just completely pushed down and systematically given lower scores in the algorithm. And so there\\'s a lot of talk on TikTok around trans communities and people of color, people who don\\'t look pretty getting rated lower by the algorithm, so they\\'re not going to have as much of a viewership. And so when your money is tied to being a content creator, it sucks. There\\'s also the whole thing that if you\\'re queer and online, you are subject to harassment. You always have to have a few backup accounts because one of them is going to get blocked out because people are going to mass... people who don\\'t like you are going to mass report everything you do until you get flagged by the algorithm as bad and systematically have your account removed, even though you didn\\'t do anything wrong. That happens all the fucking times with trans content creators. There\\'s like all these, \"Hey, my thing got deleted again.\" And it\\'s just happens. So there\\'s a lot of algorithms that play a part in that. But also the algorithm isn\\'t separate from the system of humans interacting with it. So fuck, I talked a long list, when it\\'s applied in places like policing, compulsory education, medicine, et cetera. Those already have a lot of scrutiny on them, legal and public scrutiny on them. So it\\'s very important that we get AI right in those cases because we\\'re constantly looking at them for whatever reason. Reason might be that we have a lot of public scrutiny on policing and education because we want a fair and just society. And we see those institutions that are very important to the function of a fair and just society. So if we fuck up there at all, everyone pays attention. Work, AI changes the nature of work, and when the nature of work changes, some people are benefited and some people are completely displaced.']",
          "['I know that there\\'s a K12 curriculum that someone\\'s made for AI ethics. And so I just try to defer to that because I don\\'t want to collect K12 kids. It looks legit. They break it down in a way that\\'s not just bias. So that passes my muster of a lot of people, a lot of well meaning allies who are sometimes the most dangerous people, go jump to bias and go no further, which is why I tried to bring up all of these examples that weren\\'t bias. So there\\'s this K12 AI ethics curriculum somewhere. I think it\\'s AI K12 or something. It does a good job of helping students see things like it\\'s a computer. It has sensors. What\\'s the biology of the machine in a sense. It has these parts to it, something that students might not have ever thought about. One of my first activities I did when I taught CIT105, which was Intro to Computers for people who had never touched a computer in their lives, which was a good percentage of my students, the first thing I did is I would have a video of cats, just like a kitten livestream playing on the projector as they walked in, nice calming activity. And then once we got started, I said, \"Okay, tell me every piece of technology between these kittens and us.\" And it\\'s just illuminating seeing there\\'s this, there\\'s this part, and having them just name all the parts, just giving them permission to sit there and name the parts, I think, is important. Just kind of peel back that curtain and take things less. They start with letting students know there\\'re sensors. It makes decisions in these ways. Data\\'s collected in this way. It\\'s reasoned about in this way. Problems come up in this way, and blah, blah, blah. They spell it out very well and they have a nice breakdown of curriculum. I\\'m like this looks fine to me and I have to trust them because I don\\'t know a fuck at all about K12. | AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "2_to_ai_we",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "2_to_ai_we"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          7.676558971405029,
          8.18880558013916,
          7.452094078063965,
          7.8538665771484375,
          8.155828475952148,
          8.140889167785645,
          8.201452255249023,
          7.4270243644714355,
          8.160741806030273,
          7.399999141693115,
          7.4250712394714355,
          8.082064628601074,
          8.895576477050781,
          7.736592769622803,
          7.8159966468811035,
          8.076407432556152,
          7.5267863273620605,
          8.163052558898926,
          7.835085868835449,
          8.15701961517334,
          7.90126371383667,
          8.154326438903809,
          8.162938117980957,
          8.030558586120605,
          8.09398078918457,
          7.924742221832275,
          7.761706829071045,
          7.940756320953369
         ],
         "y": [
          5.317660808563232,
          6.616098403930664,
          7.273979663848877,
          5.450689792633057,
          6.397545337677002,
          5.9192609786987305,
          6.643109321594238,
          7.266821384429932,
          6.649801731109619,
          7.277495384216309,
          7.269623279571533,
          6.402036190032959,
          7.517966270446777,
          5.414622783660889,
          7.319760799407959,
          6.514247417449951,
          7.157155513763428,
          6.000519752502441,
          7.266615390777588,
          5.950023651123047,
          7.314194679260254,
          5.916865348815918,
          5.971324443817139,
          6.412082195281982,
          6.033017635345459,
          6.307502746582031,
          7.28983736038208,
          6.5507354736328125
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah. As, so as someone who works so closely with AI, can you imagine an elementary school student or a middle schooler or even younger than that, what sort of, how could you break that down? Like AI or machine learning to get them kind of exposed to that? | I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "['Yeah. It\\'ll definitely change the whole algorithm. Switching gears a little bit, we\\'re going to start talking about youth. What are your thoughts about youth learning about AI or machine learning? | I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic.']",
          "[\"That's awesome. I love that. What are your thoughts about youth learning about AI or machine learning? | I think it's wildly important. I don't think that programming should be something you go to college for. I think this is something that everybody should learn. It literally should just be like typing class. And I think AI should be... I don't know. Because some people, I don't know, I don't want to make people go that deep into something that they might not be that interested in. You know, I think everybody should learn programming because I think it'll become somewhat of a basic skill too. But actually, you know what, we got AI that are starting to program now and you got the discussion that manual programming is going to become obsolete because AI can program everything. So, that just kind of speaks to the complexity of what AI represents. I think it should be included more in the curriculum. I'll meet in the middle. I think it should be included more in the curriculum. I might not put it as an entire class. However, I do think that children should be given many more opportunities to learn about these technical things than they currently are. I think we should give them more to challenge them with. I think kids can do a lot more than we think they can.\"]",
          "['I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic. | I think to not be fearful of it, but to understand at its core what it is and how it works, so that there\\'s no fear, for one thing. And then, when they\\'re learning that, that they can also, I think, open up their imaginations to what it could be used for, for the benefit of global society because I think what\\'s been happening with technology is, it used to be inaccessible to people who had great ideas, and now with things like cloud computing and tons of open source software, some very specialized around AI, if you have a good idea, you can try some things out. If you get everybody\\'s brain in the game, we have a lot of serious problems that we need to address, and I think young people need to understand that no one\\'s going to solve those problems, and you need to think about... I should say, other people are not always going to solve the problem. You have the ability to solve some of these problems or to contribute to the knowledge. We have to start addressing these difficult global challenges, and we have to stop, I think, focusing so much on things like TikTok. Just think about all the technology that went into building TikTok. It\\'s like, I\\'m sure there are valid and great use cases for TikTok, but I would say by and large, from my small sample, this is kind of ridiculous. It\\'s entertaining, and you get that dopamine hit. I understand that, but I just think, of these technologists that develop things like TikTok or Facebook, I\\'m like, we probably could have ended food distribution problems or just poverty, environmental concerns, but instead, we have TikTok. It\\'s kind of depressing a little bit. I\\'m hoping that you get to the youth and have them understand this. It can only be a good thing, I think.']",
          "['Yeah. I think at a high level, my generation, it was always, for your education, you really need to understand reading and writing and arithmetic, math. I think to add to that, now you have to understand computational thinking. You have to understand just some basics of how computers work, things like that, just basic stuff, because you just have to. You probably grew up in a world that already had the internet, that already had things like GPS, that already had smartphone, or something close to it. I didn\\'t. My kids, they think I [inaudible 00:19:33] prehistoric times or something. It\\'s like cave man, how did you get anything done? But now, everybody that\\'s being born is born into a world where computing is everywhere. It\\'s not just reading and writing, and arithmetic. It\\'s reading, writing, arithmetic, and computational thinking, understanding how stuff works. In the spirit of that, you have to understand, I think at least conceptually, what AI is, and what it is not presently. We\\'re in no danger of robots taking over the world anytime soon. | I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic.']",
          "[\"I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "[\"That is essential, yes. You kind of touched on this, but what do you think is important for youth to know about AI or machine learning? | I think to not be fearful of it, but to understand at its core what it is and how it works, so that there's no fear, for one thing. And then, when they're learning that, that they can also, I think, open up their imaginations to what it could be used for, for the benefit of global society because I think what's been happening with technology is, it used to be inaccessible to people who had great ideas, and now with things like cloud computing and tons of open source software, some very specialized around AI, if you have a good idea, you can try some things out. If you get everybody's brain in the game, we have a lot of serious problems that we need to address, and I think young people need to understand that no one's going to solve those problems, and you need to think about... I should say, other people are not always going to solve the problem. You have the ability to solve some of these problems or to contribute to the knowledge. We have to start addressing these difficult global challenges, and we have to stop, I think, focusing so much on things like TikTok. Just think about all the technology that went into building TikTok. It's like, I'm sure there are valid and great use cases for TikTok, but I would say by and large, from my small sample, this is kind of ridiculous. It's entertaining, and you get that dopamine hit. I understand that, but I just think, of these technologists that develop things like TikTok or Facebook, I'm like, we probably could have ended food distribution problems or just poverty, environmental concerns, but instead, we have TikTok. It's kind of depressing a little bit. I'm hoping that you get to the youth and have them understand this. It can only be a good thing, I think.\"]",
          "[\"I think you can introduce it pretty young depending on how you do it. I taught a fourth and fifth grade after school class on computer science and programming and we were just kind of learning about how to do programming and scratch and they were making little games, but it was more about kind of how do you... It's about computational thinking, right? It's how do you break a problem down and think of a solution step by step and those are pretty young kids. And I think the idea of extending that to how can you think about machines or computers that are intelligent and what does it mean to kind of have a system learned from examples rather than telling it instructions step by step? I think you can start to kind of explain those concepts at a pretty young age, at a high level. You don't need to get into the weeds of a deep learning algorithm or anything even at the high school level. I don't think it's important to kind of get into the details of how any particular ML algorithm is built. But I think it's important to think about just what does it even mean to have a machine learning model and what does it mean to learn from data and learning about the importance of knowing where the data comes from and blah, blah, blah. So I think it could be pretty early. | I mean, I've seen some online little tools that looked fun where you learn about the concept by having... But it's very interactive and hands-on. And you'll start with, how do you develop a system that learns the difference between cats and dogs? Well here, let's find some pictures of cats and let's find some pictures of dogs and let's click on them and label these as cats and let's click on these and label them as dogs. And then here's this, for now with this seemingly kind of magic algorithm that I'll start to learn the difference between cats and dogs. And then you can start to go into it from there. Okay, why do you need labels? What are labels? Why do you need to kind of separate, train your model and this, and then test it on some data that's over here that's been held out. What does it mean? How do you know how well it's doing? What does it mean to evaluate? So I would just start with some simple problems like that and then try to build into just the concepts around the whole pipeline. Not the particular algorithm or back propagation or anything like that.\"]",
          "[\"I think to not be fearful of it, but to understand at its core what it is and how it works, so that there's no fear, for one thing. And then, when they're learning that, that they can also, I think, open up their imaginations to what it could be used for, for the benefit of global society because I think what's been happening with technology is, it used to be inaccessible to people who had great ideas, and now with things like cloud computing and tons of open source software, some very specialized around AI, if you have a good idea, you can try some things out. If you get everybody's brain in the game, we have a lot of serious problems that we need to address, and I think young people need to understand that no one's going to solve those problems, and you need to think about... I should say, other people are not always going to solve the problem. You have the ability to solve some of these problems or to contribute to the knowledge. We have to start addressing these difficult global challenges, and we have to stop, I think, focusing so much on things like TikTok. Just think about all the technology that went into building TikTok. It's like, I'm sure there are valid and great use cases for TikTok, but I would say by and large, from my small sample, this is kind of ridiculous. It's entertaining, and you get that dopamine hit. I understand that, but I just think, of these technologists that develop things like TikTok or Facebook, I'm like, we probably could have ended food distribution problems or just poverty, environmental concerns, but instead, we have TikTok. It's kind of depressing a little bit. I'm hoping that you get to the youth and have them understand this. It can only be a good thing, I think.\"]",
          "['I think it\\'s important, but I think it\\'s important from two perspectives. The thing I think it\\'s more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it\\'s very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society\\'s going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there\\'s it\\'s good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it\\'s a two front benefit from it. | Yeah. I think my full pathâ€¦ My mother\\'s an educational therapist who works at a high school/middle school. So I\\'ve talked about this a fair bit. I always think if I start with general coding, it starts in second to third grade and I think you can start introducing levels of machine intelligence as early as fifth grade, because machine intelligence doesn\\'t have to be demonstrated from a coding perspective. It can be demonstrated from just what are these things? What do they do? What do they look like? And the benefit is it\\'s an interesting part about it. There\\'s so many really good abstractions of coding principles. I said earlier, I said, \"oh a big coding thing for machine intelligence is something called a random forest. And a random forest is called that because it\\'s a bunch of binary decision trees grouped together and so they call it a forest.\" Computing and AI and all those things are already filled with a very large amount of abstract representations of what they are. And so translating that and it\\'s because a lot of us aren\\'t very smart and so it makes it easier to teach us, but there are also things that make it easier to lower the barrier of entry for a lot of these concepts, like talking about a decision tree from the perspective of an actual tree makes it easier for people to understand. So I think you can at least get that high level interest in there at fifth grade, or maybe even slightly earlier with fourth grade, just from a concept perspective under the assumption that there\\'s already some computing knowledge before that. And then, I think from a coding perspective, getting in there at late middle school is always my ideal because then it\\'s, \"oh this is something that you can pursue further in high school, but also you are smart enough right now to understand that.\" And that\\'s because I also think that as someone who did a lot of coding, a big issue with learning to code complex things is, for me it was a lot tied to my math maturity. So I found that as I gained greater math maturity, I also gained a lot better ability just to understand coding quickly. And so in terms of hitting the hardcore coding concepts of it, I wouldn\\'t want to do it before that math maturity, is at least somewhere. I felt like I started to get that in late middle school. So I think that\\'s the time to do that.']",
          "[\"Definitely. Yeah. Okay. So just shifting a little bit. This is more directly related to some of the work that we do in my lab. What are your thoughts about youth learning about artificial intelligence or machine learning even as young as elementary or middle school? | I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "[\"I think it's wildly important. I don't think that programming should be something you go to college for. I think this is something that everybody should learn. It literally should just be like typing class. And I think AI should be... I don't know. Because some people, I don't know, I don't want to make people go that deep into something that they might not be that interested in. You know, I think everybody should learn programming because I think it'll become somewhat of a basic skill too. But actually, you know what, we got AI that are starting to program now and you got the discussion that manual programming is going to become obsolete because AI can program everything. So, that just kind of speaks to the complexity of what AI represents. I think it should be included more in the curriculum. I'll meet in the middle. I think it should be included more in the curriculum. I might not put it as an entire class. However, I do think that children should be given many more opportunities to learn about these technical things than they currently are. I think we should give them more to challenge them with. I think kids can do a lot more than we think they can.\"]",
          "[\"I think it's important, but I think it's important from two perspectives. The thing I think it's more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it's very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society's going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there's it's good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it's a two front benefit from it. | Yeah. That makes sense. I am still learning how to code. So I'll get there one day.\"]",
          "[\"I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids. | I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it.\"]",
          "[\"I think it's important, but I think it's important from two perspectives. The thing I think it's more important is the computing at a very young age and then older youth I think is perfectly fine to start with machine learning from the perspective of it's very easy to learn if you are put on the right path. So I found machine learning easier to learn because I started doing coding at a much younger age. And so I think ultimately given how our society's going, things like data and things like computing are going to become another form of literacy. And so that idea of having youth learn is another form of just technological literacy, especially in terms of data computing and things like that. And then given that now as a person who have a digital footprint, helping humans under helping, especially youth understand just how powerful their personal identifying data is to a machine system is pretty important. And I think the best way to understand that is, this is what a machine could do with it. So there's it's good because you could get a lot of societal education to people because of its integration, but you could also get youth involved in stem and youth involved in AI development, which is going to be a big part of the workforce. So it's a two front benefit from it.\"]",
          "[\"I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids. | Yeah. As, so as someone who works so closely with AI, can you imagine an elementary school student or a middle schooler or even younger than that, what sort of, how could you break that down? Like AI or machine learning to get them kind of exposed to that?\"]",
          "[\"I think it can start with simple games like when we start teaching kids to code, there's MIT developed scratch and very visual, game way of doing it. And I think you'd definitely start in a similar way by teaching them, just make a game to teach what you want it to do through logical representations. And they can build something that looks and dresses how they want and you name it and they make a connection to it. | Yeah. I think you start very simply with the state logic that defines most AI systems because that not only school teach them about how the AI system functions, but also just a lot about very basic decision and logic trees, which is important for kids to understand. So I think really starting from that, if this, then this or this kind of breakdown.\"]",
          "[\"I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids.\"]",
          "[\"Definitely. Yeah. Okay. So just shifting a little bit. This is more directly related to some of the work that we do in my lab. What are your thoughts about youth learning about artificial intelligence or machine learning even as young as elementary or middle school? | I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids.\"]",
          "['I tell people all the time, \"Robots are not going to take over the world. They\\'re not going to kill all the humans,\" because what we have now really is, AI is mostly about pattern recognition. Honestly, just doing it faster, but for somebody who is just a young person learning, I think they just have to understand what kind of impact AI has on society and what it will have on their lives, especially when they are in a world with autonomous driving, things like that. They have to understand that everything is not magic. | That is essential, yes. You kind of touched on this, but what do you think is important for youth to know about AI or machine learning?']",
          "['Yes, venturing indicates well is a different story. So what age do you think students or young people should start learning about AI and machine learning? | AI ain\\'t the thing. Episode one of Halt and Catch Fire, which I haven\\'t able to keep watching because there\\'s too much drama for my tired, stressed out graduate self to keep watching, first episode of Halt and Catch Fire one of the main characters goes, \"Computers aren\\'t the thing, the thing that get us to the thing.\" I think the more important aim to get isn\\'t AI or machine learning in itself, but that longer tradition of humans having a history of the technology that raises questions. Here\\'s the one we have right now that raises questions. And in 10 years it will be a different thing or go by a different name. NFTs, the idea for that is at least as old as the 1980s. All this news about that, the idea of cryptocurrencies, this is an old thing of using computers and global networks to undermine how money itself works just for the fun of undermining how money itself works. Ellen Ullman\\'s getting close to the machine. I was reading it again recently. I\\'m like, \"God, this sounds like all these fucking NFT bros now. It just sounds exactly like them, but it\\'s talking about the same idea from the 1980s going by a different name. These things are always going to come up. They\\'re always going to be around. There\\'s always going to be question raising technology. So what do we do about it? How do we think about it? And I don\\'t have good meat answers for that other than it\\'s not just AI. So AI is the thing, but it\\'s not the thing. It\\'s not the thing. It\\'s the thing that gets us to the thing if we open up. But it\\'s good fun present activities. It\\'s good to have students work with stuff they\\'re familiar with, least as an entry point. They understand livestream of cats and they don\\'t think about all the pieces in between it. So I use it as an entry point. Having kids draw Alexa takes something they\\'re familiar with in a sense, but opens it up. So I\\'m fine starting with it. But it can\\'t be the end all, be all, whatever the thing is that we\\'re getting at. ']",
          "[\"No. I think what you're doing is important. I think that AI, machine learning, reinforcement learning, neural networks aren't going anywhere. They're going to become a bigger entity and play a bigger role in society as time goes by. So the people that are really going to matter in terms of making sure that this works for other people, our youth, their kids, they're the ones that are going to actually have the big impact on being able to change this and understand it at a better level then people that are our age it. It starts with the education of the things that we're talking about, understanding how this can go wrong, understanding the errors so you're educated on that and you can avoid that. In many ways, I think what people like me and a lot of people that are similar to me, we just try to put bandaids on this because we're too far along on the road. And youth have the ability to not just put bandaids on things, they can actually fix things, I think. | Yeah. Yeah. I think so too. And they're so creative, the kids that ... All kids really, but the kids we're working with are just phenomenal. And once they get into it, they design such amazing things to help people. They're really interested in designing robots to help and robots for social good. So they're really understanding this stuff. And even if the goal is not for them to go be computer scientists or to go build these, but to have that fundamental understanding so they can be critical consumers, so they stop and say, wait, this is wrong. I need to say something. That's what we're hoping, obviously. I mean, we're not going to follow them, but we're hoping that what we do has a little bit of that impact anyway.\"]",
          "[\"I think it's wildly important. I don't think that programming should be something you go to college for. I think this is something that everybody should learn. It literally should just be like typing class. And I think AI should be... I don't know. Because some people, I don't know, I don't want to make people go that deep into something that they might not be that interested in. You know, I think everybody should learn programming because I think it'll become somewhat of a basic skill too. But actually, you know what, we got AI that are starting to program now and you got the discussion that manual programming is going to become obsolete because AI can program everything. So, that just kind of speaks to the complexity of what AI represents. I think it should be included more in the curriculum. I'll meet in the middle. I think it should be included more in the curriculum. I might not put it as an entire class. However, I do think that children should be given many more opportunities to learn about these technical things than they currently are. I think we should give them more to challenge them with. I think kids can do a lot more than we think they can. | You know there's different ways to like introduce these things like you can introduce you know the concepts of AI to elementary school or elementary aged children, and then you know, just as they grow in the middle school and high school.  Those concepts can be expanded upon and elaborated, and you can get into those details.  And the specifics like when I substitute teacher who there I can't remember her name, I think it was very she has shared like technology class and she was a you know it's like physical programming, where you have the little robot that you guys along the line you know so like you know, an elementary school, you can-Yeah, you can talk to him about what.  You know what these.  Like how basically how these devices function, you know, probably not get super in depth with it, but like you know, once you get a middle school, you can start talking to them about  more advanced aspects, like the you know physical programming that my name is Barry was trying to show them, and you know, in the in the high school, you can really kind of get down to the nitty gritty.  So I think I think it should be taught at all ages, but obviously it should just follow the curve of development\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "3_think_it_start",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "3_think_it_start"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          6.326077938079834,
          5.93809700012207,
          6.282326698303223,
          6.039855003356934,
          6.130116939544678,
          6.445659637451172,
          6.2079548835754395,
          6.419421672821045,
          6.1097869873046875,
          6.242280006408691,
          6.498251438140869,
          6.299612998962402,
          6.260859489440918,
          6.414463996887207,
          6.268711090087891,
          6.388531684875488,
          6.467848777770996,
          6.374245643615723,
          6.507309436798096,
          5.945034503936768,
          6.672111988067627,
          6.314817905426025,
          6.334620475769043,
          6.299478054046631
         ],
         "y": [
          4.107004642486572,
          5.04662561416626,
          4.762088298797607,
          5.058380603790283,
          5.074801445007324,
          4.1188178062438965,
          5.20679235458374,
          4.3613200187683105,
          5.052605152130127,
          5.443116188049316,
          4.146294116973877,
          4.7558088302612305,
          5.408196449279785,
          4.259961128234863,
          5.401583671569824,
          4.241575241088867,
          4.106009006500244,
          4.314288139343262,
          4.468279838562012,
          4.9332194328308105,
          5.556126594543457,
          4.8681745529174805,
          4.714755058288574,
          4.75677490234375
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah, I mean, I understand that so going off of what you said, what do you think it would be important for you to know about AI and machine learning like what are some key things that you think they should take or learn about not necessarily going into the ethical issues if you don't want to. | I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids.\"]",
          "[\"Yeah, I mean, I understand that so going off of what you said, what do you think it would be important for you to know about AI and machine learning like what are some key things that you think they should take or learn about not necessarily going into the ethical issues if you don't want to. | I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"Thank you for those responses. Can you think of any tools, activities, or resources that could be used to help young people to start thinking about AI and machine learning and kind of the algorithms behind them? | I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids.\"]",
          "[\"I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "['Yeah. We have thought of that, integrating Snapchat filters or something like that. Something that they would be interested in. | Are you talking about just learning AI and ML in general or the ethical issues?']",
          "[\"Yeah, that's fascinating. And I think that's a really good example of the power of AI and how it can be so helpful, especially, like you're taking the person's strengths, you're taking the AI strengths and bringing them together to make this really functional system even more functional. So what are some examples then kind of on the opposite end of the spectrum of how potentially AI or machine learning can harm us? And who in particular do you think it harms? | Yeah, absolutely. One of the things, we have run with kids in the past, getting them involved in these kinds of discussions. So understanding that algorithms and these technologies can perpetuate some of these systemic inequities. So we posed the question to them, well, what should Google do? What should these companies do? Should they mess with the data and create kind of fake data that doesn't actually reproduce these inequities? But what's the consequence of that then? So kids have really interesting ideas when it comes to these-\"]",
          "[\"I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids. | I mean, I understand that. So going off of what you said, what do you think it would be important for youth to know about AI and machine learning? What are some key things that you think they should take or learn about not necessarily going into the ethical issues, if you don't want to think about that?\"]",
          "[\"I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids. | I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"I mean, I know privacy is always a big issue, especially when you're talking about collecting major data, in order for something to apply or use machine learning. It's got to be collecting a lot of data about environment and people it's working with. So you have people who are comfortable sharing different levels of information and different levels of being information collected about them. And then also if you have an agent like that constantly collecting data, wherever it's working or interacting, there's the concept of like, okay, at what point do you require people to be like, oh, where this is going on and happening and require some sort of consent versus like it's just, it's so ubiquitous that everybody just knows it's going on. There's probably a tipping point somewhere there, but I think that are long ways off from that. So I think the privacy concerns are going to be pretty, pretty important. | Definitely. Yeah. Okay. So just shifting a little bit. This is more directly related to some of the work that we do in my lab. What are your thoughts about youth learning about artificial intelligence or machine learning even as young as elementary or middle school?\"]",
          "[\"I mean, I understand that. So going off of what you said, what do you think it would be important for youth to know about AI and machine learning? What are some key things that you think they should take or learn about not necessarily going into the ethical issues, if you don't want to think about that? | I think one thing I'd like them to know is machine learning or artificial intelligence is not perfect. And although the goal may be to keep your, for instance, data private, but they may leak information too. They have the risk of doing that. It's different from a human who can kind of control that. Although we also have that risk, but artificial intelligence, it could be hacked too. So that could be something kids need to know. That if someone hacked through the system, they can go get all of her information and they can also make bad decisions, and accurate result or so. Inaccurate. So that type of thing I think will be good.\"]",
          "[\"I mean, I know privacy is always a big issue, especially when you're talking about collecting major data, in order for something to apply or use machine learning. It's got to be collecting a lot of data about environment and people it's working with. So you have people who are comfortable sharing different levels of information and different levels of being information collected about them. And then also if you have an agent like that constantly collecting data, wherever it's working or interacting, there's the concept of like, okay, at what point do you require people to be like, oh, where this is going on and happening and require some sort of consent versus like it's just, it's so ubiquitous that everybody just knows it's going on. There's probably a tipping point somewhere there, but I think that are long ways off from that. So I think the privacy concerns are going to be pretty, pretty important. | I think it's super important because it teaches you a lot about just logic thinking. So, I mean, if you start looking at how AI is designed or learned, it's very, there's a lot of logical flows and a little bit more about coding and it's almost its own language. And actually the younger, we can start that the better. Kids pick up. It's like what we discovered about foreign languages way too late was that we were starting teaching people way too late. I think it's the same when we start talking about coding and machine learning, the earlier we can start the more natural those language processes come to kids.\"]",
          "[\"I mean, I know privacy is always a big issue, especially when you're talking about collecting major data, in order for something to apply or use machine learning. It's got to be collecting a lot of data about environment and people it's working with. So you have people who are comfortable sharing different levels of information and different levels of being information collected about them. And then also if you have an agent like that constantly collecting data, wherever it's working or interacting, there's the concept of like, okay, at what point do you require people to be like, oh, where this is going on and happening and require some sort of consent versus like it's just, it's so ubiquitous that everybody just knows it's going on. There's probably a tipping point somewhere there, but I think that are long ways off from that. So I think the privacy concerns are going to be pretty, pretty important. | Yeah. As, so as someone who works so closely with AI, can you imagine an elementary school student or a middle schooler or even younger than that, what sort of, how could you break that down? Like AI or machine learning to get them kind of exposed to that?\"]",
          "[\"I would say one way is to maybe if they have experienced interacting with an AI that does not make accurate or correct prediction result... It's hard to say to a kid that act unethically, because for kids it's also important to protect them. It's different from you telling an adult saying, okay, see, in this game that AI teammate, or agent could make a decision that kills all the civilians in the game. We can't do that to kids. So that is more difficult, like how much information you could share with the kids.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "4_that_information_kids",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "4_that_information_kids"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          7.8365864753723145,
          7.803975582122803,
          7.818288326263428,
          7.7723894119262695,
          7.930703639984131,
          8.300036430358887,
          7.714391231536865,
          7.821467876434326,
          8.075492858886719,
          7.80539608001709,
          8.097742080688477,
          7.995737075805664,
          7.8326239585876465,
          7.908063888549805
         ],
         "y": [
          4.737344741821289,
          4.572177410125732,
          4.735092639923096,
          4.342001438140869,
          3.5451419353485107,
          3.8746354579925537,
          4.872121810913086,
          4.742733001708984,
          4.486819744110107,
          4.5621161460876465,
          4.483782768249512,
          4.540495872497559,
          4.703122138977051,
          4.4767374992370605
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "['A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on.']",
          "['A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on. | Oh yeah. Sorry. Simultaneous Location and Mapping. So it will essentially... and the more advanced Roombas, they do this too, right, where they\\'ll just roam around and map out the space, and then the robot will be able to situate itself within that space so it can be intelligent about how it navigates around. But so far we haven\\'t had any locomotion for robots in homes, again because of ethical and privacy concerns where if something goes wrong, I really don\\'t want my robot to roll into the bathroom or just make people feel uncomfortable in their home. I think all the good things I\\'m trying to do will be totally undermined by a robot creeping people out. And it\\'s also I think ethically the right thing to do to not have an opportunity to freak people out that way. So the robots I work with have that capability, and it\\'s part of what makes them expensive, but I don\\'t take advantage of it at all, other than the one robot I use for demonstrations in the lab. It\\'s mapped out and does location sensing. And so it can follow me around and I have a few places around the lab that it knows to go. So just to impress people, I\\'d be like, \"Hey, Timmy, go to the table.\" And it\\'ll wander over to the table and people are like, \"Wow, that\\'s [inaudible 00:08:28].\" Not really, but okay.']",
          "[\"That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning? | Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "[\"Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say. | Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "['And what sort of technology or programming do you use for these robots or have you been using? Do you use artificial intelligence or machine learning? | A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on.']",
          "[\"That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way? | Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "['A bit. Yeah. So the primary ones that I\\'d say we use is... I mean, my mine is often fringe machine learning AI, and often dumb versions of what could be construed as AI, because my work a lot of times is really interested in the interactions and not so much the technological solutions to automating those things. So we do a lot of work that uses text-to-speech, which I would definitely put in the AI natural language processing camp. And in that, one of the real challenges is in balancing the automated portion of it. It\\'s nearly impossible to get the speech exactly right. And so we debate a lot about how much time and energy to put into adding in markup language, things that can edit the [profidy 00:02:19] and that sort of stuff of the output versus just taking what we get. And so what that means in our workflow is that if we preset everything, if we know everything exactly that the robot\\'s going to say, then we can prerecord all the audio, and then we can review it and make sure we massage and finesse it. But if we want the robot to be more spontaneous where it\\'s generating the audio in the field, we don\\'t have that opportunity to correct or even review what\\'s going on with these things. So you\\'d mentioned the ethical and social aspects of those things. That\\'s one of my big concerns is when we use these off-the-shelf technologies without any human in the loop, that bad things on a low level just might make it seem confusing or just inauthentic. But it could also be pretty problematic the way that it comes across or even if we\\'re auto-generating what\\'s to be said, if there\\'s even a syntax error in those things, then it comes through.I had one during my early grad work where we were trying to insert the student\\'s name into the speech that the robot said, and this was all automatically processed. I didn\\'t prerecord anything. And we screwed up the syntax on it. And so it ended up actually reading the markup code that we had in there. It\\'d be like, \"Hello, ate our open bracket, closed bracket,\" And had all these weird things that it said. And I was really happy that I caught that before we put it in the field because that\\'s the sort of thing that it\\'s nightmare fuel. Because again, I work with some youngish kids. They\\'re 9 and 10. And so the last thing I want to do is scar them with this berserk-seeming robot. So yeah. So I think the text-to-speech is probably the primary one that I use on a daily basis.We\\'re developing techniques then... Again, one of the things we want to do is try to automate some of what the robot says because the human part of it is so labor-intensive. So we\\'re working on some techniques to use the real modern advances in tech summary are really helpful and pretty good now where I\\'m working on the homework utilities so the teachers can make homework assignments for kids that a robot would read to them. I think that\\'s the one that you were going to be on the board for. So the idea there though, and when we\\'re talking to teachers, they\\'re like, \"This sounds great. I really love the idea, but I really need a lot of automation in here,\" that if they\\'re going to go through and hand-write out everything the robot\\'s going to say they just can\\'t. So we\\'re looking at doing some auto-tech summaries where they can bring in a PDF or a website of something that they want the kids to read at home for their class and then just say, \"Summarize this portion for the kids. And I want the robot to say that summary.\" So that\\'s the next level for us. And so again, we still have the text-to-speech problems in there where we\\'re not going to have much control over the profidy and the way that it\\'s delivered, but we\\'ll also not have too much control over the actual contents of what\\'s generated there. So we\\'re working on methods to try to do pre-review things. So in the pipeline where the teacher maybe selects, summarize this portion and submits it before it actually gets out to a student, that we\\'d be able to have some sort of human review over that just to make sure it makes sense. And there\\'s other automated reviewing techniques that are out there. I haven\\'t looked at them in detail, but they\\'re out there too.So that\\'s, I think the one that is currently working on. And then the last one that comes to mind is I\\'ve always used OpenCV for facial recognition. And then we use that for face tracking. So all of my robots have been able to track the speaker by taking that essentially, just putting an X, Y position on the center of a face and then aligning motors to be where the robots face appears to be looking at the person. Probably the biggest one we haven\\'t gotten into, the Misty robot that I have uses SLAM mapping. So it can do navigation in a home pretty simply. And that\\'s an option. We just haven\\'t chosen to do any mobile robot stuff in people\\'s homes. It seems like one of the last things I want to work on. | Well, okay. So you\\'ve already touched on some potentially ethical issues with the location sensors, going into more private areas, potentially if for some reason the algorithm does that. And then you also talked about it with the speech-to-text, just some errors there that can impact your work. With all the different technologies you just referred to, are there other social or ethical problems that you come across with your technologies, like the facial recognition or just the child interacting with the speech-to-text or anything?']",
          "[\"Yeah. Yeah. It's cool. And what's interesting is it's almost like a broad computer science department that just happens to be at Microsoft. So there's a lot of us doing AI stuff, but it's kind of the whole spectrum of computer science research. | Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that.\"]",
          "[\"Sure. Yeah. So, like I said, it was still on interaction, but it was much more focused on one particular cue, which was understanding social gaze behavior. So I was looking, and I actually was doing human robot interaction, but I was also doing stuff with virtual agents. And so there was a lot of reading social science and psychology literature on understanding what is known about how people use their gaze when they interact with each other, and when do they make eye contact? When do they look away? When do they use gaze to reference things that they're talking about? And what's the timing of that? And so there's lots of kind of patterns that are known from the literature of just human observational studies, but then they're not kind of often not really precise enough to sort of implement in an artificial system. If you need to design a robot or an agent that can follow those patterns, you need to know sort of frame by frame almost when should it look at the person, when should it look at an object if there's some kind of collaboration going on? When should it look away? So my methodology was kind of take what I can learn from literature, but then also occasionally bring people into the lab in pairs or in groups to do a task or to have a conversation with each other and kind of record their gaze behaviors and their head motions and their speech. And then doing a lot of, at that time, a lot of manual annotation to get more hard numbers and statistical distributions of, for example, when do they avert their gaze from each other, and then there'd be a process of, okay, can I turn that into kind of some kind of computational model that I can implement on a robot or an agent? And then there's a study where I bring people into the lab and at that time, it was basically just diadic, just one-on-one conversations. So it'd be one person talking with the robot. And maybe sometimes there is a task they had to do like a sorting task, or maybe sometimes it was more rapport building and just conversational. And I would measure if the robot that uses the distributions of gaze behavior that I learned from humans versus the robot that maybe just has static gaze or gazes randomly, which one results in better outcomes? Whether it's just how much did you like the robot, or task performance? How much did you learn if it was an educational scenario, or how quickly did you complete the task? So there was usually an array of different measures. But yeah, that was kind of generally the methodology that I followed. And so I've kind of studied three or four different mechanisms of gaze and did different studies like that. | That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way?\"]",
          "[\"Yeah. I consider myself a researcher that works on... I mean, AI, a lot of people have different definitions for it. I work broadly in AI and I would say I'm a user of machine learning because the kind of AI work is not necessarily developing new kinds of machine learning techniques or developing the next big, deep learning breakthrough, which a lot of research is, especially at Microsoft Research. But I'm kind of interested in having a goal of an interactive system that can do something and then thinking about what are the abilities it needs. Okay. It needs to speak, it needs to talk to people. It needs to understand speech. So there are machine learning models that can do that. It needs vision. It needs to be able to see people and it needs to be able to see objects. So, okay. What are the best computer vision models out there that I can use? So I'm constantly kind of on the lookout for what's the state of the art for a lot of different kinds of machine learning models. And then my research is on how do I bring them together and kind of integrate them into a larger system, an end to end system, an interactive intelligence system. Because a lot of machine learning is kind of, you focus on one sort of wedge or one sort of competency like speech or vision or whatever. And so I'm interested in... And that's great, but how do you actually bring those together into end to end systems, which is its own science I would say.\"]",
          "[\"Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say.\"]",
          "[\"That's interesting how that seems very applicable for AI, right. It could be implemented in a lot of different types of AI systems. Do you know any examples of AI systems that have been implemented? Or work areas like your work would be implemented? Wouldn't it be most AIs that have to work with humans or faces in a way? | Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say.\"]",
          "[\"Yeah, I mean, there isn't a lot though. I mean, obviously there's work that people are doing on human robot interaction, for example. If you want a robot to interact with people and it needs to detect where people are and it needs to have a conversation with them, it's very applicable to that. But I think a lot of work is more on... I mean, a lot of AI work isn't really interactive anyway. I mean, it's more about machine learning and developing better vision systems that can classify images or speech recognition systems that can classify speech, but not necessarily interactively. It's on a batch data set or all of these deep learning models for text generation, which is a little bit applied to text chat bots. So a lot of the interaction with AI is chat bots, I would say, which is written text, and there's no need to understand or no ability even to think about cues beyond what's coming in the text stream. The argument I make at Microsoft to why I think an input research is important is that those kinds of applications are coming and they're important and not enough people are working on them. And they've become really important in virtual reality, augmented reality, which everyone's very excited about these days. And of course in human robot interaction. If you want robots doing things with and alongside humans that they need to kind of not just gaze, but all sort of social cues, they need to be able to understand and participate in them. So I think there's a lot of applications that way, but it's not really what the field is focusing on as a whole, I would say. | That's interesting. So then would you tell me a little bit about your experience that you've done working with either AI or machine learning?\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "5_to_robot_we",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "5_to_robot_we"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          2.9465603828430176,
          2.984149694442749,
          2.9455862045288086,
          2.7653915882110596,
          2.9876937866210938,
          2.9360530376434326,
          2.954592227935791,
          7.026263236999512,
          7.072707653045654,
          2.9598073959350586,
          2.7803761959075928,
          2.819739580154419,
          2.787604808807373,
          3.5358870029449463
         ],
         "y": [
          1.609811782836914,
          1.6271992921829224,
          2.3954577445983887,
          1.8778024911880493,
          1.6039776802062988,
          2.372241973876953,
          1.6055341958999634,
          0.13463537395000458,
          0.18174906075000763,
          2.4235596656799316,
          1.8969630002975464,
          1.9794546365737915,
          1.9037742614746094,
          1.6624737977981567
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah, so I got a computer science degree, and then during my computer science degree, I worked at a few different jobs using it. So I worked, like, IT management in ceilings working IT, I then had two stints doing software development work, one for a manufacturing company and then another for a larger tech company. And then around the time of graduating, I decided to go into grad school instead. And now that I'm in grad school, most of what I do is research with AI systems and humans using AI systems.\"]",
          "[\"I mean, I got into computer science generally. I was interested in computer games, video games. I was kind of interested in the AI that goes into all the games and the enemies that you fight against or whatever. But I started out more interested in graphics and animation and my undergrad was more, I took more classes that are about computer graphics and animation with a little bit of AI, just kind of out of interest. But then in grad school, it was really just in talking with some of the potential professors that were there that I could work with. So my main advisor [inaudible 00:23:30] he kind of really started to introduce me to some of the ideas of, HCI and just thinking about interaction and thinking about how to... Because I was still interested in animation and animated characters, but thinking about how could you actually design animated characters that interact with people? And so I just got super interested in thinking about how does interaction work and then more and more interested in how does humans' social interaction work in the first place? It's so complex. So my passion has become understanding as much as I can about human social interaction and how to computationalize it enough that you can start to develop intelligence systems that participate in that.\"]",
          "[\"Yeah. I feel like there's so much you could talk about that's just here are all things that you use every day that are using some kind of AI or ML, like Snapchat filters, right? There are machine learning model. How is that possible? Taking the time to think about what enabled that? How is this working? And then with the ethical issues, I don't know. There's a news story every week that you could connect to. So I feel like connecting to things that are happening and big stories that are in the news I think would probably be what's most engaging, I would guess. | I mean, I got into computer science generally. I was interested in computer games, video games. I was kind of interested in the AI that goes into all the games and the enemies that you fight against or whatever. But I started out more interested in graphics and animation and my undergrad was more, I took more classes that are about computer graphics and animation with a little bit of AI, just kind of out of interest. But then in grad school, it was really just in talking with some of the potential professors that were there that I could work with. So my main advisor [inaudible 00:23:30] he kind of really started to introduce me to some of the ideas of, HCI and just thinking about interaction and thinking about how to... Because I was still interested in animation and animated characters, but thinking about how could you actually design animated characters that interact with people? And so I just got super interested in thinking about how does interaction work and then more and more interested in how does humans' social interaction work in the first place? It's so complex. So my passion has become understanding as much as I can about human social interaction and how to computationalize it enough that you can start to develop intelligence systems that participate in that.\"]",
          "['I pretty much always wanted to be more in the realm of technology and computer science, but also just really working with people. I was a little intimidated by computer science as a field at first. So, I went with psych. And then, I always knew I wanted to go to grad school. And so, I was originally thinking a good marriage between the two would be human factor psychology, because that is also very involved with technology and design and development. But I didn\\'t even know about the human-centered computing program until my current advisor, I found some papers by him that were about human-AI teaming and team cognition in those teams. And so, I reached out to him and the lab that I was working at the time, I did a collaboration with Nathan and he basically just reached out at me and was like, \"Hey, I\\'d like to have you in the lab. What do you think?\" And I\\'m thinking to myself, \"Oh, this is an opportunity to get to learn computer science and be a little bit more involved in that and still retain the usefulness of my undergrad.\" So, I jumped right at that opportunity. | It\\'s definitely different than undergrad. I got my undergrad at Clemson too, in genetics. So, I understand. I\\'m doing a similar thing as you. Would you tell me a little bit more about your AI teaming and cognition, your research, what you\\'re studying?']",
          "['That\\'s interesting. How did you actually... I know you said you had a psychology degree from Clemson and then you kind of moved into the human-centered computing program. What made you interested in that? How did you become interested in working with AI and machine learning? | I pretty much always wanted to be more in the realm of technology and computer science, but also just really working with people. I was a little intimidated by computer science as a field at first. So, I went with psych. And then, I always knew I wanted to go to grad school. And so, I was originally thinking a good marriage between the two would be human factor psychology, because that is also very involved with technology and design and development. But I didn\\'t even know about the human-centered computing program until my current advisor, I found some papers by him that were about human-AI teaming and team cognition in those teams. And so, I reached out to him and the lab that I was working at the time, I did a collaboration with Nathan and he basically just reached out at me and was like, \"Hey, I\\'d like to have you in the lab. What do you think?\" And I\\'m thinking to myself, \"Oh, this is an opportunity to get to learn computer science and be a little bit more involved in that and still retain the usefulness of my undergrad.\" So, I jumped right at that opportunity.']",
          "[\"Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at. | Yeah, I'm actually familiar with that one, because it's the CU-TLP program. We have some people in the learning sciences that are working on that. So I'm familiar with this one. Would you mind giving me a little bit more detail about your project that involves AI ethics? What were some of the results that came out of that?\"]",
          "[\"Yeah. So pretty much been an academic in training for a very long time. My dad was a dean at Penn State, so I grew up in academia. I knew about academia. I never wanted to be in academia, this is the funny part of the story. I remember telling both of my parents, why would I ever, ever want to do what my dad does? It sounds like the most boring thing in the world. And then I have ended up mimicking his career in many ways. So my professional career is very interdisciplinary. I have a degree in psychology, I have a degree in information science, and then a postdoc basically on computer science. And then I'm a professor of computer science, essentially. So I expand the spectrum from social and hard computational perspectives, and I think that's really important when you talk about AI. It can't just be one or the other. It can't just be a bunch of psychologists ruminating about what they think is important or sociologists thinking what they think is important about AI. You need real computer scientists in the room as well to understand the feasibility of how these things are going to happen. So both of these entities need to be talking to each other. I kind of planned my career that way, that I knew from a very early age that I was interested in the intersection of just people and technology. So I started off learning, I built my foundation with people, with the psychology, understanding that. And throughout my career, I've kept that foundation. It's what grounds me as a researcher and from my worldview of things. I've kept that humanistic foundation, but I built on top of that through additional degrees, more of a technology flare from information sciences and computer science. And now you kind of get whatever the heck I am nowadays. It's like a morphous blob of social and computer science. But I think it's important. And that's how I train my students to think about things is, if we're going to make sure that AI is beneficial for people and inclusive to many people, you better be taking into account the human side of things. But you also need equally know, from my perspective, as people that build AI, you need to know how to do that as well. So it's a big ask, but I think it's what the next generation of people studying, and implementing and developing AI need. They need both of these perspectives. Collaboration amongst both of those is great.But if we can start training people for this early on, to your point in some of your studies that you look at, if we can start instilling this mindset in kids from an early age of AI is not just computer science, it's human science as well, it's both of these things together, instilling that from an early age and having actual interdisciplinary training and degrees for that is going to be really important. So I went all over the place. I'll do this throughout the interview, I'll ramble- | Can you give us an example of a real world situation where that research could be applied, where AI and humans are working together?\"]",
          "[\"Yeah. So pretty much been an academic in training for a very long time. My dad was a dean at Penn State, so I grew up in academia. I knew about academia. I never wanted to be in academia, this is the funny part of the story. I remember telling both of my parents, why would I ever, ever want to do what my dad does? It sounds like the most boring thing in the world. And then I have ended up mimicking his career in many ways. So my professional career is very interdisciplinary. I have a degree in psychology, I have a degree in information science, and then a postdoc basically on computer science. And then I'm a professor of computer science, essentially. So I expand the spectrum from social and hard computational perspectives, and I think that's really important when you talk about AI. It can't just be one or the other. It can't just be a bunch of psychologists ruminating about what they think is important or sociologists thinking what they think is important about AI. You need real computer scientists in the room as well to understand the feasibility of how these things are going to happen. So both of these entities need to be talking to each other. I kind of planned my career that way, that I knew from a very early age that I was interested in the intersection of just people and technology. So I started off learning, I built my foundation with people, with the psychology, understanding that. And throughout my career, I've kept that foundation. It's what grounds me as a researcher and from my worldview of things. I've kept that humanistic foundation, but I built on top of that through additional degrees, more of a technology flare from information sciences and computer science. And now you kind of get whatever the heck I am nowadays. It's like a morphous blob of social and computer science. But I think it's important. And that's how I train my students to think about things is, if we're going to make sure that AI is beneficial for people and inclusive to many people, you better be taking into account the human side of things. But you also need equally know, from my perspective, as people that build AI, you need to know how to do that as well. So it's a big ask, but I think it's what the next generation of people studying, and implementing and developing AI need. They need both of these perspectives. Collaboration amongst both of those is great.But if we can start training people for this early on, to your point in some of your studies that you look at, if we can start instilling this mindset in kids from an early age of AI is not just computer science, it's human science as well, it's both of these things together, instilling that from an early age and having actual interdisciplinary training and degrees for that is going to be really important. So I went all over the place. I'll do this throughout the interview, I'll ramble- | Yeah. So what we study in my research group, almost everything we study nowadays is on this concept known as human AI teaming or human autonomy teaming. So it's the idea of humans teaming with AIs or autonomous teammates for the completion of a shared goal or a shared task. So everything we study nowadays is related to this concept, and there's a lot of different perspectives and paradigms and ways to look at that concept. And we really try to run the gamut on this. So the thing I talk about all the time when we talk about human AI teaming is that there's a bidirectional quality to it. You have to check off the boxes for the human. You have to check off the boxes for the AI. So the human needs to know how to interact with the AI. It needs to have an expectation of what the AI is. But also the AI needs to know what the heck humans are. It needs to know what teaming is, what matters. And this is where we have to get better. We have to develop autonomous teammates that actually know how to interact with humans, because it can't just be a one sided paradigm. And that's what it is right now. We stick people into a human AI team and we say, go work with this AI. But the AI has no clue how to work with you as a human. So what happens is that the human has to take on this brunt of dealing with basically a bad teammate. So what we're trying to do is number one, understand perceptions that humans have of AI as teammates, and reverse engineer those perceptions so we can build more effective, good AI teammates. So like I was talking about before, you can clearly see how there's the psychology point of view with the perceptions of humans and AI as teammates, but then on the other side, it's the computer science side of things. How do we actually build AI's that understand communication, coordination, awareness, things like team cognition, really critical aspects of teaming that have to be built into human AI teams?\"]",
          "[\"Yeah. So pretty much been an academic in training for a very long time. My dad was a dean at Penn State, so I grew up in academia. I knew about academia. I never wanted to be in academia, this is the funny part of the story. I remember telling both of my parents, why would I ever, ever want to do what my dad does? It sounds like the most boring thing in the world. And then I have ended up mimicking his career in many ways. So my professional career is very interdisciplinary. I have a degree in psychology, I have a degree in information science, and then a postdoc basically on computer science. And then I'm a professor of computer science, essentially. So I expand the spectrum from social and hard computational perspectives, and I think that's really important when you talk about AI. It can't just be one or the other. It can't just be a bunch of psychologists ruminating about what they think is important or sociologists thinking what they think is important about AI. You need real computer scientists in the room as well to understand the feasibility of how these things are going to happen. So both of these entities need to be talking to each other. I kind of planned my career that way, that I knew from a very early age that I was interested in the intersection of just people and technology. So I started off learning, I built my foundation with people, with the psychology, understanding that. And throughout my career, I've kept that foundation. It's what grounds me as a researcher and from my worldview of things. I've kept that humanistic foundation, but I built on top of that through additional degrees, more of a technology flare from information sciences and computer science. And now you kind of get whatever the heck I am nowadays. It's like a morphous blob of social and computer science. But I think it's important. And that's how I train my students to think about things is, if we're going to make sure that AI is beneficial for people and inclusive to many people, you better be taking into account the human side of things. But you also need equally know, from my perspective, as people that build AI, you need to know how to do that as well. So it's a big ask, but I think it's what the next generation of people studying, and implementing and developing AI need. They need both of these perspectives. Collaboration amongst both of those is great.But if we can start training people for this early on, to your point in some of your studies that you look at, if we can start instilling this mindset in kids from an early age of AI is not just computer science, it's human science as well, it's both of these things together, instilling that from an early age and having actual interdisciplinary training and degrees for that is going to be really important. So I went all over the place. I'll do this throughout the interview, I'll ramble-\"]",
          "['I pretty much always wanted to be more in the realm of technology and computer science, but also just really working with people. I was a little intimidated by computer science as a field at first. So, I went with psych. And then, I always knew I wanted to go to grad school. And so, I was originally thinking a good marriage between the two would be human factor psychology, because that is also very involved with technology and design and development. But I didn\\'t even know about the human-centered computing program until my current advisor, I found some papers by him that were about human-AI teaming and team cognition in those teams. And so, I reached out to him and the lab that I was working at the time, I did a collaboration with Nathan and he basically just reached out at me and was like, \"Hey, I\\'d like to have you in the lab. What do you think?\" And I\\'m thinking to myself, \"Oh, this is an opportunity to get to learn computer science and be a little bit more involved in that and still retain the usefulness of my undergrad.\" So, I jumped right at that opportunity.']",
          "[\"Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at. | Yeah, so work that I've done has been outlining how there are different camps I've worked in. Number one is outlining how humans and AI interact with each other, specifically my dissertation work is on how humans and AI systems can impact and influence each other in a task. So how a human could be susceptible to letting a robot or an AI system tell it what to do and take commands from it or vice versa. So looking at what comprises that, how humans should lead AI or how an AI should lead a human, things like that. So that trade off and then I've also done work looking at how ethics in AI systems can be implemented, created and how it interacts with humans. So, it's fairly big part is the ethical implications of AI systems and how those ethical implications ultimately impact the utility of AI. And then the last, there are other smaller things that I've had to do. But then the last main one I worked on is AI. There's a grant that I work on fairly often that is a grant that looks at using machine intelligence and recommender systems to provide recommendations to teachers for professional development. So it's a lot of providing recommendations for their professional development and I work on the side of that where I work on building this system and outline in that aspect.\"]",
          "[\"Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at.\"]",
          "[\"Yeah, so I got a computer science degree, and then during my computer science degree, I worked at a few different jobs using it. So I worked, like, IT management in ceilings working IT, I then had two stints doing software development work, one for a manufacturing company and then another for a larger tech company. And then around the time of graduating, I decided to go into grad school instead. And now that I'm in grad school, most of what I do is research with AI systems and humans using AI systems. | Yeah, so I think there's two small separations there, which is like my initial interest in computer science is when I was a lot younger. I actually went to a fair amount of coding camps as a kid. I just always was around tech and then my mom knew I liked it a lot. So we would go do local camps that would look at programming and things like that, were just in the area and targeting younger kids. Then when I got out of high school, the guy who gave me my first job as an IT professional decided to teach a coding course at my high school. So I took that a few times just so that way I could keep redoing it and I found it really fun. And also it was just like really interesting, because it was vastly different than all the other material we were covering in high school. And then obviously I went into getting the CS degree in four years of that. So I just stuck with it. And then when I got into grad school, I had to pick a domain and I wanted to work on tech that was more like 10, 15, 20 years out as opposed to looking at tech that already existed. So then I was like, AI is a future tech. That's really interesting. It's not very formed yet. There's a little structure to it. So I thought it'd be fun to look at.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "6_computer_so_science",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "6_computer_so_science"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          1.111106514930725,
          1.5991697311401367,
          1.634647011756897,
          1.4360334873199463,
          1.501905083656311,
          1.1118297576904297,
          1.6068532466888428,
          1.5874817371368408,
          1.6165984869003296,
          1.4464517831802368,
          1.161421537399292,
          1.1461915969848633,
          1.1797903776168823,
          1.395344614982605
         ],
         "y": [
          8.294069290161133,
          7.769954681396484,
          7.753460884094238,
          7.940978527069092,
          7.882920742034912,
          8.293251991271973,
          7.783701419830322,
          7.801530361175537,
          7.773162841796875,
          7.931577205657959,
          8.242681503295898,
          8.257650375366211,
          8.225105285644531,
          7.996157646179199
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important. | Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.\"]",
          "[\"Yeah, I learned that a little bit the hard way, but just one mistake and then we fixed it. Okay, great. So imagining it sounds like you think it is important to teach kids about both those things like AI concepts, but also maybe some of the ethics around privacy and other issues and maybe getting more important as the years go on. Do you know of or can you think of any specific tools or activities or ways to engage youth in both of those things? | I think it's going to be a lot easier than we think just because they're so ingrained in the technology itself. My daughter is just about to turn one and she can already swipe on a tablet because she's watches her brother swipe on a tablet. And I think just those simple movement to think, they're going to be so more used to it than we are getting used to it. I don't think it'll be that much of a stretch just relating it to the devices they already use or have , I mean, even Alexa is a type of AI, because it does learn things about you, it learns to pick up your voice and your inflections and specific words and what you like. So, I mean just being able to talk about the everyday devices that they use is going to be easy for them.\"]",
          "[\"Oh, demographically. I think I did actually, the first thought was people who are pretty good at programming and math probably, and who are interested in creating really advanced technology that's very smart. I can think about humans and demographic. I don't actually really have a thought of a specific group who design those. I would say just people who are good at programming and math or either one of those. | Actually, I was surprised. I remember when I learned programming, I think I started learning programming after I went to college. The first year freshman, I feel like currently kids, they learn. They actually have the channels to interact with different technologies really early. They probably have a Google home or Siri on their mom or dad's phone or like their iPad or so. So I feel they're exposed to those advanced technologies, including machine learning or artificial intelligence a lot. And I feel like, I'm not sure, I know high school kids definitely learn programming. I'm not totally sure about middle school kids, but I would say even if it's not in school, they still have a lot of chance to interact with technologies. Not necessarily to learn how it works, but more just to get to know it. And I currently don't see, at least I haven't really thought about the harm of they learning machine learning at a really young age. There might be [inaudible 00:17:12] issues. I'm not sure, but I think they might trust more than we do. I don't really trust the Google, not trust, it's more like, I just don't feel very comfortable having Google home listening to my voice, but they probably got the young generation because they interacted with technologies much earlier than we did. So that might have built the trust in machine learning or artificial intelligence.\"]",
          "[\"Starting with logic trees and then some state machine logic, things like that. | Yeah, absolutely. Kids are getting Chromebooks in second grade now. The schools are distributing. We just talked about our children, interacting with Alexa that's a intelligent system, so.\"]",
          "[\"Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas? | So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful.\"]",
          "['Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.']",
          "[\"So the biggest thing that comes to mind in terms of just learning the logic patterns that are behind these is, I had a class, it was my only EE class. I absolutely hated it. And I would not have made it through if my husband didn't have to be an electrical engineer, where we had to program a Roomba so that it would go and park somewhere. And we had to develop this state machine logic that would do that and then program everything. But I think, yeah, hands on activities like that you're going to program a Roomba and watch it do it or a remote control car or something like that. Something what we can physically see the effects of what they've created is really helpful. | Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important.\"]",
          "[\"Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas? | Yeah and it gives them, I mean, little bit older geeks might get a good jump of satisfaction out of their code compiling, but for kids, they need to see something more. So, oh my code made it do this and it can go there now is a lot better ever win.\"]",
          "[\"Yeah, yeah. Pretty close. Yeah, so when you're talking about her swiping, she does similar things, very happy with the technology. She barely talks. She's about maybe 10 words and one of them is Alexa. She walks around screaming, Alexa, Alexa. Okay, great. What about if you've had experiences, the last question really, experiences teaching or been in situations where you're the student, have your teachers or have you used any learning techniques that you feel were helpful to learn some of these ideas? | Yeah. I think that's absolutely right. We sort of, we run a couple of pilots with some kids and we learn that very quickly that they love robots. They want to take what they're doing on the screen and see it in real life and see that connection. And so I think that's absolutely right. Especially with young kids. That tactile piece that physical computing I think is really important.\"]",
          "[\"For kids, especially younger, the more tactile they can get on anything the better and the more able they are to really pay attention. So just if we can make physical representations of those trees and the logic and things that move to the better. So one of my sons teach preschool teacher, they have were teaching them shapes and they have like shaped man who gets on the floor and they put them all the pieces together to make shapes and patterns and things. You can do the same thing with logic trees and decision making and then even understanding things like privacy, where you know how far something spreads or what you're taking. So just the more tactile you can really make something for kids at that age the better. | I think it's going to be a lot easier than we think just because they're so ingrained in the technology itself. My daughter is just about to turn one and she can already swipe on a tablet because she's watches her brother swipe on a tablet. And I think just those simple movement to think, they're going to be so more used to it than we are getting used to it. I don't think it'll be that much of a stretch just relating it to the devices they already use or have , I mean, even Alexa is a type of AI, because it does learn things about you, it learns to pick up your voice and your inflections and specific words and what you like. So, I mean just being able to talk about the everyday devices that they use is going to be easy for them.\"]",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "7_think_alexa_kids",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "7_think_alexa_kids"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          6.423547267913818,
          6.854139804840088,
          6.738757133483887,
          6.822705268859863,
          6.711336135864258,
          6.450076580047607,
          6.720575332641602,
          6.599125862121582,
          6.639172554016113,
          6.82025146484375,
          6.677968502044678
         ],
         "y": [
          2.7788171768188477,
          2.6905276775360107,
          3.6482431888580322,
          2.8205907344818115,
          2.891125202178955,
          2.9203107357025146,
          3.1127545833587646,
          2.6356353759765625,
          2.6259679794311523,
          3.2097818851470947,
          2.933375597000122
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "[\"If you sit around in a room and you hear a bunch of people just talking, you learn a lot. They're not teaching you, you're just learning a lot. You're just kind of catching it. I think by employing things like gaming and some of these thought workshops, like what would you do in this case? It's interesting. That's why I really like teaching the class I'm teaching this semester, which is computing, ethics, and global society. Because some things that seem simple, are not. They're tough decisions. They may benefit some people. They're going to hurt some people. Is it the right thing to do? Even though you can do it, should you do it? It's-\"]",
          "['Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences. | Yeah, so from two perspectives, in terms of full formalized teaching, I\\'ve taught the second one is about stuff I do with my mother, but I\\'ve taught coding courses at the collegiate level, like lab courses at collegiate style. And so there is less of a gap there in terms of age. I was pretty much two years older than everyone I was teaching. And so yeah, you do get interesting. You do have to work down in terms of their understanding, but also you can still stay at a very high level, but even when I was doing that, the best instructors I\\'ve ever had are the ones that pulled up their code and did it with you. So, that\\'s just how I did it. Like I said, I think what changes is the modality you do it through. So for college kids, you don\\'t pull up a YouTube and talk about how their data is produced there. You just pull up data and you work with them through a data set. So, I think the teaching principle, the pedagogy principle, [inaudible 00:38:56] whatever the word is, principle still stays the same, which that demonstration aspect, the difference is how abstract that demonstration gets at the collegiate level. We don\\'t need that. But then my mother works with students who have gotten interested in stem and I\\'m like, \"yeah, I can talk to them about this.\" And it\\'s more about like once again, the education side for that younger side is, I don\\'t want to just sit up there and tell them about those things. I want them to go find what they find interesting. So I give them the website of, \"here\\'s is all, here\\'s scratch.\" Just play around with it for 10 minutes. I was like, \"I don\\'t want to touch it.\" I\\'m not going to talk to you about it. You just go play with it for 10 minutes because that\\'s like how I not only learned it, but I got interested in it. Because I think it\\'s less interesting for a student to be told what to do. And it\\'s way more interesting if they were, \"here\\'s a fun thing. Here\\'s 10 minutes.\" Instead of doing homework in class, go do this for 10 minutes. And that was always my favorite part of doing computer stuff when I was younger.']",
          "['Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it. | Yeah. One that just comes to mind that didn\\'t come up is self-driving car technology. And that is dangerous in a much more salient way, right? And I think getting people to understand, when I see a car and I look over, I evaluate what the person behind that wheel is attending to, right? Before I cross the street, I like to make sure that I see that person look at me so that I know that they\\'re sensing me, right? So if and when it comes to be that I look over and there\\'s nobody at that wheel, there really needs to be an acclimation to understanding, \"Okay, the situation is that there\\'s a machine driving that car. How does it make mistakes?\" I know how people make mistakes, right? I know what it is. If they didn\\'t look at me and I start to walk out and they just roll even at a red light, that could catch me. So I pay attention to that. What are the types of errors that machines make in that scenario are important, that we know that sometimes, it might not recognize a stop sign at all. People are typically better at it. Maybe not typically, but we learn to understand the types of errors that can be common in these systems, or even uncommon, if they\\'re going to be catastrophic, and are aware enough in a way that we can respond in reasonable actions to it. I mean, right now, if I saw a driverless car, I would go nowhere near the street because they\\'re prone to weird accidents that I don\\'t understand right now. So I can\\'t figure them out well enough to do it. But eventually, if they become commonplace, we\\'ll have to know how that works.']",
          "['Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences.']",
          "['Yeah I mean that is you just gave us a lot of good information Thank you so, can you think of any ways to help us connect these topics like thinking about data machine learning AI to their everyday lives and make it meaningful for them. | Oh yeah. Like I said, I think for me, actually I talked about with a log talking about, I think reverse engineering, looking at, instead of being like, \"oh, how does data impact your daily life?\" Be more like, \"oh, how does your daily life impact your data?\" Taking it event by event is great. And then looking at the basic tasks you could do as an individual or pieces of entertainment they like. Relating it back to some they derive pleasure from a just consumer standpoint. For instance, I\\'m sure a lot of children watch Netflix or other things like that, Netflix and you could even take this as an interesting one. You open up Netflix and it\\'s, \"here are recommendations for you.\" Talking to them about, \"well, what does that recommendation come from?\" And then having them throw out those ideas of, oh, they could see a show they watched and they\\'re like, \"oh, I watched the show already.\" I think that\\'s why [inaudible 00:36:12] and answer is yes you did. That\\'s why they recommended it to you. Because Netflix did a bunch of research and found the number one thing that determined whether or not someone wanted to watch a show or not is whether or not they already finished it because people just like to do the same thing over and over again. That was an interesting one too. I met with Home Depot one time and they\\'re like, \"yeah, our algorithm is designed to show you the same items over and over again because eventually you\\'ll like it, you\\'ll lower your standards and like it eventually.\" So tiny things like that. But yeah, I think with something like Netflix, it will be interesting or media richness, talking about this is a piece of media you work with, this is a platform to interact with every day and extracting data from it. Because I think that\\'s the key is, looking at the data they already have. So that way it means something to them. And then like that way, next time I go on Netflix, that\\'s how I was as a kid. I know I might have been a weird kid, but if I learned that in school, the next thing I would\\'ve done is gone home and talked to my mom about all the cool things Netflix has with data and then watch and I would pull up our Netflix, this is what it\\'s doing right now. So I think that\\'s for me, what I would find interesting or applicable to those younger audiences.']",
          "['Yeah. Absolutely. You answered everything. I\\'m looking at my follow-ups. I\\'m like, \"Oh, you got that. You got that.\" Yeah. I mean, if you want to expand a little bit, if you were to take Jules for example, right, your daughter, and she\\'s six, and what would you want her to know about machine learning or about at that age, anything or about how harmful it can be? Would you talk to her about privacy? Would you talk to her about misrepresentation or discrimination? Where would you go? | Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "['Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it. | Yeah. And that interaction between human and the self-driving car, the human and the technology, is really interesting. If you can\\'t look at them in the eye, how do you interact with them to ensure to minimize risk, right? That\\'s a whole other dimension of designing these technologies and testing them. And also-']",
          "['Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          "['Yeah. I love that, Joe. And I can see that in your work, the way you\\'ve described it. That\\'s great. Okay. So let\\'s switch. So talking a little bit about why we\\'re doing this interview, right? We\\'re taking what people are saying and trying to apply it for learning experiences for young people. So what are your thoughts just generally about elementary school, middle school-age kids learning about either AI machine learning and the social and ethical impacts or both? [crosstalk 00:29:56] ideas around that, what they should learn, what\\'s important for them to know? Can they [crosstalk 00:30:00] in those issues | Yeah. It makes think of what we already do a lot with advertisement, that we call it out. We point it out for what it is. I see us doing that a little bit too. She doesn\\'t encounter it that much, but we like to shine the light on these sort of things so she sees the impact of them. So when there\\'s a commercial that catches her attention, we always say, \"That\\'s an ad.\" And she\\'s so accustomed, she\\'ll turn to us and like, \"What are they trying to sell us?\" And that\\'s the approach, I think, again, on that output part is to really, I think, if you\\'re teaching kids about it in ways that they\\'re going to use it, that\\'s a little bit different. But there\\'s also the consumer side of it that I haven\\'t really thought much about to really interrogate this thing, right? I\\'m on Twitter. Why am I seeing this thing? Right? And understanding at least some of the details behind what goes in there, right? So if you\\'re completely not thoughtful about it, it might seem like, \"Because this is just the right thing for me to see,\" or, \"This is what all my friends are seeing,\" or, \"This is just the most important news.\" But if you\\'re actually taught that it\\'s there because there\\'s this system built to put it there in front of you, and that system has a goal that is maybe antithetical to your actual goals, those are the types of things that I teach Jules about. And then the same thing, right? There\\'s little goofy online things that they\\'ll use facial recognition and they put ears and stuff on people. We\\'ve talked about that too, where it goes wrong, and we talk about how and why that goes wrong. So if the ears end up on your eyes, we talk about like, \"Well, there\\'s a system in there that there is an error, and that\\'s how it screwed this up. It\\'s not magic. It\\'s a system.\" So I think again, on the consumer side of it, I\\'m not trying to teach her to use these things yet, but I want her to be really aware of how they are. And again, it comes back to the way we treated science classrooms, that there\\'s this social good, societal good for science literacy, that you can understand when this CDC says something and makes a recommendation, you understand at least the basics about the process of coming to those conclusions. You should understand at least the basics about how what you are seeing or who and what you\\'re interacting with that come from machine learning and AI kind of thing, you should understand the role that those things play and their propensity for error. Because that\\'s the other thing that comes up in my work. People assume the robots know what they\\'re doing. People assume [crosstalk 00:36:26] smarter than they are because we have been culturally trained to believe that about them. My robots are often pretty dumb [crosstalk 00:36:33] really don\\'t believe that about them at all. So that\\'s the other side is to really be aware that they are limited, that the Jetsons\\' all-purpose robots or chat bots even are nowhere near close. So to treat them as if they are going to make a mistake all the time, because they are. You might not even notice it.']",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "8_to_about_like",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "8_to_about_like"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          7.578370571136475,
          9.907402992248535,
          9.316093444824219,
          9.885623931884766,
          9.895676612854004,
          9.252779006958008,
          9.303382873535156,
          9.295660018920898,
          9.257917404174805,
          9.299212455749512
         ],
         "y": [
          7.049922943115234,
          4.880551815032959,
          5.3948540687561035,
          4.885363578796387,
          4.8283209800720215,
          5.360877513885498,
          5.368912696838379,
          5.376779079437256,
          5.427334308624268,
          5.396990776062012
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "D1",
          "x": 0.9444405376911164,
          "y": 0.6284505605697639,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "D2",
          "x": 6.8732639223337175,
          "xshift": 10,
          "y": 9.538179683685303
         }
        ],
        "height": 750,
        "shapes": [
         {
          "line": {
           "color": "#CFD8DC",
           "width": 2
          },
          "type": "line",
          "x0": 6.8732639223337175,
          "x1": 6.8732639223337175,
          "y0": -8.281278562545776,
          "y1": 9.538179683685303
         },
         {
          "line": {
           "color": "#9E9E9E",
           "width": 2
          },
          "type": "line",
          "x0": 0.9444405376911164,
          "x1": 12.802087306976318,
          "y0": 0.6284505605697639,
          "y1": 0.6284505605697639
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Documents and Topics</b>",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1200,
        "xaxis": {
         "visible": false
        },
        "yaxis": {
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mmr_topic_model.visualize_documents(docs_clo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
